\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amssymb}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{stmaryrd}
\usepackage{subfig}
\usepackage{mathtools}
\usepackage{xcolor}
\usepackage{floatrow}
\usepackage{minted}
\usepackage{color}
\usepackage{multicol}
\usepackage{stackengine}
\usepackage{scalerel}
\usepackage{xcolor}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Custom definitions
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclareMathOperator*{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\simtext}[1]{\ensuremath{\stackrel{\text{#1}}{\sim}}}
\newcommand\myeq{\stackrel{\mathclap{\normalfont\mbox{D}}}{=}}
\newtheorem{thm}{Hypothesis}
\newtheorem{lem}[thm]{Result}

\definecolor{bg}{rgb}{0.98,0.98,0.98}
\floatsetup[listing]{capposition=bottom}    

\def\boxitem#1{\setbox0=\vbox{#1}{\centering\makebox[0pt]{%
  \fboxrule=2pt\color{mLightBrown}\fbox{\hspace{\leftmargini}\color{black}\box0}}\par}}

\newenvironment{longlisting}{\captionsetup{type=listing}}{}

\setlength{\columnsep}{0.2cm}

\newcommand{\RomNum}[1]
    {\MakeUppercase{\romannumeral #1}}

\def\boxitem#1{\setbox0=\vbox{#1}{\centering\makebox[0pt]{%
  \fboxrule=2pt\color{mLightBrown}\fbox{\hspace{\leftmargini}\color{black}\box0}}\par}}

\newcommand\dangersign[1][2ex]{%
  \renewcommand\stacktype{L}%
  \scaleto{\stackon[1.3pt]{\color{red}$\triangle$}{\tiny !}}{#1}%
}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\linespread{1.5}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{%
  \large Wiederholung, Ergänzung, Erklärung \& Intuition:\\
  \Large Statistik I \& II für Studierende der Wirtschaftswissenschaften\\
  \large (Ludwig-Maximilians-Universität München)
}
\date{\textbf{Stand:} \today}
\author{\textbf{Autoren:}\\ 
Matthias Aßenmacher\thanks{Institut für Statistik, LMU München; Kontakt bei Fragen \& Anregungen: \url{matthias@stat.uni-muenchen.de}},\\
Ann-Kathrin Köpple\thanks{Studentische Hilfskraft (SoSe20), Institut für Statistik, LMU München},\\
Christoph Luther\thanks{Studentische Hilfskraft (WiSe 18/19 - SoSe 20), Institut für Statistik, LMU München},\\
Patricia Haro\thanks{Studentische Hilfskraft (WiSe 18/19 - WiSe 19/20), Institut für Statistik, LMU München},\\
Maximilian Mandl\thanks{Institut für medizinische Informationsverarbeitung, Biometrie und Epidemiologie, LMU München\vspace{.25cm}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle

\blfootnote{
\hspace{-0.75cm} 
\textit{\small 
This work is licensed under a Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)\\          
License details: \url{https://creativecommons.org/licenses/by-nc/4.0/}}
}

\noindent Dieses  Dokument wurde aus verschiedenen Quellen erstellt. Es soll als kleine Verständnishilfe für Studierende angesehen werden, wobei auf mathematische Genauigkeit und Vollständigkeit explizit verzichtet wird. Außerdem wird jedes Thema durch einen Block an Multiple-Choice Aufgaben und Hinweisen auf die passenden \texttt{R}-Funktionen für die behandelten Methoden ergänzt. Für Fehler wird keine Haftung übernommen.\\

\noindent Der erste Teil dieses Dokuments ($\widehat \approx$ Statistik I) wurde von Ann-Kathrin in Zusammenarbeit mit Matthias im Sommer 2020 verfasst. Ann-Kathrin verantwortete das Schreiben des Erstentwurfs, Matthias war verantwortlich für intensives Korrekturlesen, Anpassungen und Erweiterungen.\\
Der zweite Teil ($\widehat \approx$ Statistik II) basiert auf Vorlesungszusammenfassungen von Max aus dem Sommer 2019, welche in Zusammenarbeit mit Christoph, Patricia \& Matthias in dieses Format gegossen und detaillierter ausgearbeitet wurden. Besonderer Dank für diesen zweiten Teil gilt Herrn Dr. Alexander Engelhardt, der freundlicherweise einen Teil seines Materials zur Verfügung gestellt hat (siehe auch: \url{https://www.crashkurs-statistik.de}), sodass wir uns hiervon inspirieren lassen konnten. Aufgrund des Fehlens der Themenbereich Kombinatorik und Wahrscheinlichkeitsrechnung wurden dieser im Sommer 2020 von Ann-Kathrin \& Matthias ergänzt.

\clearpage

\tableofcontents

\clearpage

\hspace{0pt}
\vfill
\begin{center}
    {\Huge Statistik I}
\end{center}
\vfill
\hspace{0pt}

\clearpage

\section{Grundbegriffe, Skalenniveaus, Datenerhebung}

\subsection{Was ist die Statistik?}

Die Statistik kann man in drei verschiedene Grundaufgaben einteilen. Die deskriptive, explorative und induktive Statistik. In Statistik I wird hauptsächlich die deskriptive und die explorative Statistik thematisiert.
\begin{itemize}
     \item Deskriptive Statistik: Das Ziel der deskriptiven Statistik ist es, umfangreiches Datenmaterial in Tabellen, Graphiken und Kennzahlen übersichtlich darzustellen. Viele dieser Methoden sind bereits aus der Schule bekannt (z.B. Kreis- und Balkendiagramm oder arithmetisches Mittel a.k.a. der Durchschnitt) und werden in dieser Veranstaltung durch weitere Ma\ss{}zahlen und Darstellungsweisen erg\"anzt.
     \item Explorative Statistik: Hierbei wird das aufbereitete Datenmaterial auf Strukturen und Muster untersucht um mögliche Hypothesen aufzustellen.
\end{itemize}

\noindent Der Begriff \textit{Daten} mag f\"ur den ein oder anderen etwas neu sein, bezeichnet jedoch im Grunde genommen nichts anderes als eine Messung oder Erhebung von Werten. Vereinfachend kann man sich einen \textit{Datensatz} z.B. einfach als die Messung der Gr\"o\ss{}e aller Personen im H\"orsaal vorstellen.

\subsection{Grundbegriffe}
\subsubsection{Untersuchungseinheit und Grundgesamtheit}
Die \textbf{Untersuchungseinheit} ist ein einzelnes zu untersuchendes Objekt, welches durch das Symbol $\omega$ dargestellt wird.
\\Die \textbf{Grundgesamtheit} ist die Menge an Objekten, über die man etwas sagen möchte. Das Symbol der Grundgesamtheit ist $\Omega$. Somit sind alle Untersuchungseinheiten zusammen die Grundgesamtheit. Diese Beziehung lässt sich wie folgt umschreiben: $\omega \in \Omega$
\\ \textbf{Beispiel:} Die gesamten Studenten im H\"orsaal sind die Grundgesamtheit. Ein einzelner Student ist die Untersuchungseinheit.

\subsubsection{Bestandsmasse und Bewegungsmasse}
 Wenn man die Grundgesamtheit $\Omega$ zu einem bestimmten Zeitpunkt einmal misst, dann spricht man von einer \textbf{Bestandsmasse}.\\
 Beispiel hierfür ist die Messung, bei der festgestellt wird wie viele Studenten am Semesteranfang (10. Oktober) immatrikuliert sind.\\
Im Gegensatz dazu spricht man von einer \textbf{Bewegungsmasse} wenn Ereignisse gemessen werden, die über einen bestimmten Zeitraum eintreten können. Das wären zum Beispiel die Studenten, die während des Wintersemesters das Studium abbrechen.

\subsubsection{Merkmale und Merkmalsausprägungen}
Wenn wir von einer bestimmten Eigenschaft oder einem Aspekt der Untersuchungseinheit sprechen, nennt man dies \textbf{Merkmal} oder \textbf{statistische Variable}.
\\Wenn man sich nun für einen gemessenen/konkreten Wert eines Merkmals interessiert, dann nennt man das \textbf{Merkmalsausprägung}.
In unserem \textbf{Beispiel} könnte man sich für die Leistung in Statistik I der Studenten interessieren, somit sind die \textbf{Merkmalstr\"ager} \textit{Studenten, die Statistik I belegt haben}.
Eine konkrete \textbf{Merkmalsausprägung} mit dem \textbf{Merkmal}: "\textit{Leistung in Statistik I}"  wäre dann auf der Notenskala 1,0 bis 5,0 beispielsweise die Note 2,3.\\
Es gibt zwei Arten von Merkmalsausprägungen:
\begin{itemize}
     \item \textbf{Qualitative:}  Merkmalsausprägungen, sind Ausprägungen, die keinen mathematischen Wert annehmen, also nicht aus Zahlen bestehen. In unserem Beispiel wäre das z.B. die Einteilung der Leistungen in \textit{bestanden} und \textit{nicht bestanden}.
    \item \textbf{Quantitativ:} Merkmalsausprägungen sind \textit{messbar} und werden somit mit Zahlen angegeben. Beispielsweise in der Klausur 40 von 60 Punkten erreicht.
    Quantitative Merkmalsausprägungen kann man weiter unterscheiden in diskret, stetig und quasistetig.
    \begin{itemize}
        \item \textbf {Diskrete} Merkmale haben abz\"ahlbar viele m\"ogliche Merkmalsauspr\"agungen, das heißt nicht quasi unendlich viele Ausprägungen wie z.B. Sandkörner am Meer (mehr dazu s.u. bei \textit{quasistetig}), sondern man kann die möglichen Merkmalsausprägungen mit nicht allzu großem Aufwand abzählen.
        (Bsp.: Geschlecht (da gibt es nur männlich oder weiblich) oder die Platzierung beim Schönheitswettbewerb (bei einer Teilnahme von 5 Personen, kann ich nur Platz 1, 2, 3, 4 oder 5 bekommen), Studiendauer in Semestern)
        \item\textbf{Stetige} Merkmale können unendlich viele verschiedene Merkmalsausprägungen haben. (Das Alter, da zwischen bspw. 18 und 19 unendlich viele Nachkommastellen vorhanden sein können, oder aber auch Anteilswerte sind hierfür ein gutes Beispiel) 
        \item \textbf{Quasistetigen} Daten sind theoretisch stetig (bspw. Körpergröße, Gewicht, o.ä.) Daten, werden aber nur auf einer diskreten Skala (in sehr kleinen Einheiten) gemessen. Da solche Daten \textit{praktisch} in den meisten Fällen auch wie stetige Daten behandelt werden, spricht man hier von \textit{quasistetigen} Daten.
        \item Werden (quasi)stetige Daten in Klassen eingeteilt (bspw. Abfrage von Gehalt in Fragebögen), so spricht man von \textbf{klassierten} oder \textbf{klassiert-stetigen} Daten. Diese Klassenbildung hat weitreichende Implikationen für die anwendbaren Methoden (vgl. folgende Kapitel).
    \end{itemize}
\end{itemize}
Der \textbf{Merkmalsraum} oder \textbf{Zustandsraum} ist die Menge aller möglichen Merkmalsausprägungen. Hier in unserem Beispiel hat der Merkmalsraum des Merkmals Notenleistung gerundet auf ganze Noten eine M\"achtigkeit von 5 (Die Noten: 1, 2, 3, 4, 5)

\subsubsection{Skalentypen}\label{sec:Skalentypen}
Merkmale besitzen aufgrund der Eigenschaften ihrer m\"oglichen Merkmalsausprägungen bestimmte Skalierungen. Diese richtig zuordnen zu können ist sehr wichtig, denn je nach Skalentyp kann man in den folgenden Kapiteln unterschiedliche Maßzahlen bestimmen. 

\paragraph{Nominalskala:} Die Ausprägungen bei einer Nominalskala können nur voneinander unterschieden werden, jedoch nicht geordnet oder ins Verhältnis gesetzt werden. Daher kann man die Merkmalsausprägungen nicht werten, in dem man bspw. sagt, blaue Autos seien besser als (oder doppelt so gut wie) rote Auto. 
Die einzige Aussage die getroffen werden kann ist, ob die Ausprägungen \textit{gleich oder ungleich} sind.

\paragraph{Ordinalskala:} Bei der Ordinalskala können wir nicht nur eine Aussage über gleich/ungleich (wie bei der Nominalskala) treffen, sondern zusätzlich auch die Ordnung, d.h. über \textit{kleiner und größer}. Somit können ordinale Merkmale in eine natürliche Rangfolge/Ordnung gebracht werden. Diese Ordnung kann interpretiert werden, jedoch nicht die Abstände. Beispiele hierfür wären die Schweregrade eines Computerspiels oder die Güteklassen eines Hotels, da man zwar sagen kann, dass das 4-Sterne-Hotel besser als das 2-Sterne-Hotel ist, nicht jedoch, dass es doppelt so gut ist o.\"a. 

\paragraph{Metrische Skala:} Die metrische Skala besitzt den höchsten Informationsgehalt. Denn hier kann man zusätzlich zu den Aussagen gleich/ungleich und größer/kleiner auch Aussagen über Abstände zwischen den Merkmalsausprägungen treffen. Somit kann man die Merkmalsausprägungen in eine Ordnung bringen und die Abstände zwischen den Merkmalsausprägungen messen und interpretieren. Die metrische Skala kann man weiter auf splitten in Intervall- und Verhältnisskala:\\
\noindent Bei der \textbf{Intervallskala} können Differenzen gebildet werden um eine Aussage über den Abstand zu machen, jedoch keine Quotienten, da es kein (natürlichen) Nullpunkt gibt um zwei Werte in Relation zu einander zu setzen. (\textbf{Beispiel} Temperatur: Man kann sagen, dass es heute 10 Grad kälter als gestern ist, jedoch nicht , dass es heute halb so warm ist wie gestern.\\
Bei der \textbf{Verhältnisskala} gibt es diesen natürlichen Nullpunkt. Deshalb kann man Quotienten bilden und Verhältnisse sinnvoll interpretieren. (\textbf{Beispiel} Größe: Man kann sagen, dass Person A doppelt so gro\ss{} ist wie Person B.)
Ein Spezialfall der Verhältnisskala ist die Absolutskala, da nur natürliche Einheiten vorkommen (keine physikalischen Größen). \textit{Nat\"urliche Einheiten} sind bspw. Anzahlen, im Sinne von 10 Äpfel oder 5 Blumen.

\subsection{Datenerhebung}
Um mit Daten arbeiten zu können müssen diese erst einmal "entstehen oder hergestellt" werden. Dafür gibt es die Datenerhebung. Diese \textit{beschafft} Informationen bzw. \textit{gewinnt} Daten.

\subsubsection{Erhebungsarten}

\textbf{Primärerhebung:} Wenn ich selbst eine Erhebung (Befragung, Beobachtung, Experiment) starte ohne auf vorhandenes Material zurück zu greifen, dann wird es als Primärerhebung bezeichnet.\\
\textbf{Sekundärerhebung}: Wenn ich auf bereits vorhandenes Material zurückgreife (z.B. Daten/Statistiken aus dem Internet) dann handelt e sich um eine Sekundärerhebung. Das Material existierte schon vor meiner Recherche.

\subsubsection{Umfang}
Bei Erhebungen, kann man entweder alle Untersuchungseinheiten einer Grundgesamtheit miteinbeziehen, dann spricht man von einer \textbf{Voll-/Totalerhebung} oder nur eine Teilmenge der Grundgesamtheit miteinbeziehen. Dann spricht man von einer \textbf{Teilerhebung (Stichprobe)}.
Ein Beispiel für die Totalerhebung wäre eine Volkszählung oder eine Evaluation, bei der alle Studenten befragt werden.\\
Teilerhebungen sind zum Beispiel Qualitätsprüfungen von Produkten, bei denen einzelne Produkte überprüft werden oder die Sonntagsumfrage bei der einzelne, zufällig ausgewählte Bürger aus der Bevölkerung abstimmen können, wen sie wählen würden wenn aktuell Bundestagswahl wäre.

\subsubsection{Datenform}
Je nachdem wie oft und über welchen Zeitraum eine Erhebung gemacht wird, gibt es bestimmte Datenformen.
Bei \textbf{Querschnittsdaten} wird der Ist-Zustand zu einem bestimmten Zeitpunkt aufgenommen. An mehreren Untersuchungseinheiten werden ein oder mehrere Merkmale nur \textit{einmal} erhoben. \textbf{Beispiele} hierfür sind Lehrerevaluationen oder der Mietspiegel.\\
Eine Erweiterung hiervon sind die \textbf{Longitudinal-, Längsschnitt,- oder Paneldaten}. Hierbei werden ein oder mehrere Merkmale an mehreren Untersuchungseinheiten zu \textit{verschiedenen} Zeitpunkten wiederholt erhoben. Somit interessiert uns hier u.a. auch die Entwicklung der Merkmale im Zeitverlauf.\\
\textbf{Beispiele:} SOEP (Wiederholungsbefragung von privaten Haushalten in Deutschland), Deutsches Mobilitätspanel (Befragung von Haushalten nach ihrem Mobilitätsverhalten und ihrer PKW-Nutzung).
Bei einer \textbf{Zeitreihe} wird \textit{ein Merkmal} an aufeinander folgenden Zeitpunkten beobachtet. Hierbei wird die Entwicklung eines Merkmals im Zeitverlauf beobachtet. \textbf{Beispiele} hierfür sind Aktienkurse oder (Preis-/Mengen-)Indizes.

\subsubsection{Erhebungsmethode}

Man unterscheidet bei den Erhebungsmethoden grundsätzlich die \textbf{Beobachtung}, die \textbf{Befragung} und das \textbf{Experiment}. Im Gegensatz zur Befragung ist die Beobachtung, wenn sie verdeckt ausgeführt werden kann, weitgehend unverfälscht, da die Merkmalstr\"ager nicht mit in die Erhebung einbezogen werden. Bei Befragungen sind Personen direkt mit einbezogen und können teils eher schwerlich unverfälschte Aussagen über das eigene Verhalten machen. Jedoch kann bei der Beobachtung nur das äußerliche Verhalten ermittelt werden, wohingegen man bei einer Befragung auch innere Einstellungen und gedankliche Prozesse durch gezielte Fragen messen kann. Das Experiment ermöglicht Ursachenforschung, ein Nachteil ist jedoch (genauso wie bei der Beobachtung), dass es sehr zeit- und kostenaufwändig sein kann.

\subsection{Datenaufbereitung}
Nachdem Daten erhoben wurden, müssen diese nun aufbereitet werden, in einem sinnvollen Format abgespeichert und ausgewertet werden können.

\subsubsection{Datenstruktur}\label{sec:struct}
Um Daten abspeichern zu können, werden diese üblicherweise in Datenmatrix dargestellt.
Dies ist wie eine Tabelle, in der 
\begin{itemize}
    \item jede Zeile die Information über eine \textbf{Untersuchungseinheit} enthält
    \item jede Spalte einem \textbf{Merkmal} entspricht
    \item jedes Element der Matrix einer \textbf{Merkmalsausprägung} entspricht
\end{itemize}

\begin{table}[H]
\centering 
\begin{tabular}{ccccccc}
   \hline
    Nr         & nm & nmqm & wfl & rooms & bj & bez\\
     \hline
    1   & 608.40 & 12.67 & 48 & 2 &1957 & Untergiesing\\      
    2   &780.00 & 13.00 & 60 & 2 & 1983 & Bogenhausen\\ 
    
    3  & 822.60 & 7.48 & 110 & 5 & 1957 & Obergiesing\\
    
    4  & 500.00  & 8.62 & 58 & 2 & 1957 & Schwanthh \\
    5  & 595.00  & 8.50 & 70 & 3 & 1972 & Aubing \\
    6  &960.00   & 11.85& 81 & 3 & 2006 & Schwanthh\\
     \hline
             
\end{tabular}
    \caption{\textit{Mietspiegel Beispiel für eine Datenmatrix}}
    \label{tab:Datenmatrix}
\end{table}

\noindent \framebox[\textwidth]{\texttt{R-Befehl für die Erstellung von Datensätzen: > data.frame()} \hfill \href{https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/data.frame}{Dokumentation}}

\noindent \dangersign[3ex] Meist werden Datensätze in der Praxis nicht in \texttt{R} erstellt, sondern aus einer externen Quelle (z.B. einer .csv-Datei oder einer .txt-Datei) importiert. 

\noindent \framebox[\textwidth]{\texttt{R-Befehl für den Import von Datensätzen: > read.table()} \hfill \href{https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/read.table}{Dokumentation}}

\subsubsection{Kodierung}
Da man mit Zeichenketten (Wörtern) nicht rechnen kann, müssen diese aufbereitet werden.
Der Vorgang, bei dem Zeichenkette/Merkmalsausprägungen Zahlen zugeordnet werden nennt man Kodierung (Vergleiche Tabelle \ref{tab:Datenmatrix}: Hier würden die Verschiedenen Stadtteile mit Nummern kodiert).


\subsubsection{Transformation}
Bei der Daten Aufbereitung kann es sinnvoll sein Daten zu transformieren, also mathematisch verändern.
Je nach Skalentyp sind verschiedene Arten von Transformationen zulässig.\\
\textbf{Nominalskala:} Alle \textit{eindeutigen} Transformationen (z.B. vgl. Tabelle \ref{tab:Datenmatrix}: Transformation von Stadtteilbezeichnungen in Zahlencodes)\\
\textbf{Ordinalskala:} Alle Transformationen, welche die \textit{vorliegende Ordnung erhalten}\\
\textbf{Intervallskala:} Alle Transformationen der Form $g(x)=a+bx,  b>0$ (z.B. Temperaturumrechnung von °F in °C)\\
\textbf{Verhältnisskala:} Alle Transformationen der Form $g(x)=bx, b>0$ (z.B. Umrechnung von Minuten in Stunden)
\\

\subsubsection{Statistik-Software}
Es gibt verschiedene Software-Programme um statistische Analysen durchzuführen. In Statistik-Veranstaltungen oder studentischen Projekten vereinfacht es die Analyse enorm, wenn man nicht alles per Hand rechnen muss, sondern die Software das für einen mit ein paar Befehlen macht. In der praktischen Arbeit mit Daten ist dies ebenfalls der Standard, auch aus Gründen der Reproduzierbarkeit, etc. Hier in der Vorlesung behandeln wir hauptsächlich die Programmiersprache \texttt{R}. Am Ende jeweils von Statistik I und II gibt es in der Veranstaltung eine genauere Einführung dazu, jedoch stolpert man im Verlauf des Skripts auch immer wieder über Befehle und Outputs aus \texttt{R}.
Mit \texttt{R} kann man so gut wie alles aus der Vorlesung berechnen. Hat man einmal einen Datensatz importiert, kann man die verschiedensten Sachen damit berechnen ohne den Original-Datensatz selbst zu verändern. Dies ist einer der vielen Vorteile gegenüber Programmen wie bspw. Excel.\\

\noindent Dieses Skript liefert, ergänzend zu den intuitiven und eher nicht-technischen Erklärungen der verschiedenen Themen, zu jeder Methode/Maßzahl eine kurze Info darüber, wie diese in \texttt{R} anwend- bzw. berechenbar ist. Dies war bereits weiter oben in Kapitel \ref{sec:struct} zu sehen und wird sich wie ein roter Faden durch dieses Manuskript ziehen.\\

\noindent \texttt{R} selbst ist kostenlos beziehbar unter \hfill \href{http://www.r-project.org}{http://www.r-project.org}.\\
\noindent Der Editor R-Studio kann hier heruntergeladen werden: \hfill \href{https://www.rstudio.com/}{https://www.rstudio.com/}\\
\noindent Es finden sich auch zahlreiche Hilfe-Seiten, wie z.B. \hfill \href{https://www.rdocumentation.org/}{https://www.rdocumentation.org/}\\
\noindent oder \hfill \href{https://stat.ethz.ch/R-manual/}{https://stat.ethz.ch/R-manual/}\\



\newpage


\section{Häufigkeitsverteilungen, (univariate) grafische Darstellung}
Häufigkeiten, deren Berechnung und Darstellung in Diagrammen, sind sicher schon weitgehend aus der Schule bekannt. Im Folgenden wird dies wiederholt und weiter ergänzt.
\subsection{Berechnung von Häufigkeiten}
\subsubsection{absolute Häufigkeit}\label{sec:abs.Häuf}
Um die Anzahl der Untersuchungseinheiten zu erfassen, welche eine bestimmte Merkmalsausprägung aufweisen, verwendet man die absolute Häufigkeit.\\
Diese kann man bei Nominal-, Ordinal-/ und der metrischen Skala anwenden.
Die \textbf{allgemeine Formel für die absolute Wahrscheinlichkeit} lautet:
\begin{align*}
    n_j=\sum_{i=1}^n I_{a_j} (x),\qquad  j=1,...,k
\end{align*}
Hierbei muss die folgende Bedingung erfüllt sein:
\begin{align*}
    I_{a_j}=\begin{cases}
			1 & \text{falls } x_i = a_j \\
            0 & \text{sonst}
		 \end{cases}
\end{align*}
Wenn eine Untersuchungseinheit $I_{a_j}$ die entsprechende Merkmalsausprägung hat, dann bekommt sie den Wert 1 und wird in die Summe mit einbezogen. Fällt eine Untersuchungseinheit nicht in die zu erfassende Menge, also hat eine Untersuchungseinheit nicht die entsprechende Merkmalsausprägung, dann tritt der 2. Fall ein, nämlich diese erhält den Wert 0. Zum Schluss werden alle Untersuchungseinheiten, die diese gewünschte Ausprägung aufweisen aufsummiert.\\
Die Berechnung der absoluten Häufigkeit macht vor allem für Merkmale Sinn, bei denen nicht allzu viele verschiedene Merkmalsausprägungen beobachtet werden (z.B. Noten, Lieblingsfarbe, o.ä.).
\\
\\
\textbf{Klassenbildung} \label{sec:Klassenbildung}\\
Um die Übersicht bei stetigen und diskreten Merkmalen mit vielen Ausprägungen (=quasistetig) zu behalten, macht man sich die Klassenbildung zu Nutze.
Um eine sinnvolle und brauchbare Verteilung bei der Klassifizierung zu bekommen, bietet es sich an, die Grundgesamtheit in $\sqrt{n}$ Klassen zu teilen (grobe, sehr allgemeine Faustregel!)\\
Allgemein gibt es zwei Möglichkeiten zur Wahl der Klassen
\begin{enumerate}
    \item nach sachologischen Gegebenheiten
    \item nach willkürlichen Kriterien
\end{enumerate}
da man jedoch mit den willkürlichen Kriterien Strukturen verfälschen kann, sollten diese eher vermieden werden.

\clearpage

\noindent \textbf{Einschub: Mathematische Notation bei Klassenbildung}\\
\begin{center}
    \begin{tabular}{cl}
         k & Anzahl der Klassen\\
$e_{j-1}$&untere Klassengrenze der j-ten Klasse\\
$e_j$&obere Klassengrenze der j-ten Klasse\\
$d_j=e_j-e_{j-1}$& Klassenbreite der j-ten Klasse\\
$a_j=\frac{1}{2}(e_j+e-{j-1})$& Klassenmitte der j-ten Klasse\\
$n_j$ &Anzahl der Beobachtungen in der j-ten Klasse\\
    \end{tabular}
\end{center}

\subsubsection{relative Häufigkeit}
Die absolute Häufigkeit ist bei unterschiedlichen Stichprobenumfängen nicht vergleichbar.
Um dieses Problem zu umgehen kann man die relative Häufigkeitsverteilung verwenden, die die gesamte Verteilung auf 1 normiert. Somit sind (relative) Häufigkeiten nun auch für unterschiedliche Stichprobenumfänge vergleichbar.\\
Die relativen Häufigkeiten $f_j$ sind \textit{die Anteile} die auf jede Ausprägung entfallen. Man berechnet sie durch den Quotienten aus der absoluten Häufigkeit und dem Stichprobenumfang.
$$\frac {\mbox {absolute Häufigkeit}}{\mbox{Stichprobenumfang}} = \frac{n_j}{n}=f_j$$

\subsubsection{Häufigkeitstabelle}
Die Häufigkeitstabelle umfasst alle möglichen Ausprägungen eines Merkmals (bzw. alle gebildeten Klassen) und deren (relative \& absolute) Häufigkeiten.
Man kann sie bei \textit{diskreten} und bei \textit{gruppierten stetigen} Merkmalen (vgl. Tabelle \ref{tab:stetige Merkmale}) verwenden, jedoch nicht bei stetigen, da jede Beobachtung einen anderen Wert hat und somit die Tabelle "unendlich lang" werden würde. Für gruppierte stetige Merkmale kommen zusätzlich zu den Spalten \textit{Merkmalsausprägung} $a_j$, \textit{absolute Häufigkeit} $n_j$, und \textit{relative Häufigkeit} $f_j$, noch die Spalten \textit{Klassengrenzen} [$e_{j-1};e_j$[ und die \textit{Klassenbreite} $d_j$ hinzu.

\begin{table}[h!]
\centering
 \begin{tabular}{|c c c c c|} 
 \hline
j   & [$e_{j-1};e_j$[& $d_j$ & $n_j$ &$f_j$ \\ [0.5ex] 
 \hline\hline
 1 & [$e_0;e_1$[& $d_1$&$n_1$ & $f_1$ \\ 
 : & : & : & : & :\\
 : & : & : & : & : \\
 k & [$e_{k-1};e_k$[ & $d_k$ & $n_k$ & $f_k$ \\ [0,5ex] 
     \hline\hline
    $\sum$   &          &        & n     & 1 \\
 \hline
 \end{tabular}
  \caption{\textit{Allgemeine Form bei gruppierten (quasi-)stetigen Merkmalen}}
    \label{tab:stetige Merkmale}
\end{table}

\subsection{Graphische Darstellung von Häufigkeiten}
Da graphische Darstellungen leichter verständlich und übersichtlicher sind, werden die Daten meist ergänzend zu den Häufigkeitstabellen auf diese Art und Weise dargestellt.

\subsubsection{Balken- und Säulendiagramm}

\textbf{Gestaltung von Säulendiagrammen}
\begin{itemize}
    \item Auf der x-Achse (Abszisse) sind die verschiedenen Merkmalsausprägungen abgetragen, darüber entstehen die Säulen. Jede Säule entspricht einer Merkmalsausprägung
    \item Auf der y-Achse (Ordinate) wird die Skala abgetragen, um ablesen zu können, wie groß die Anzahl oder der Anteil einer Merkmalsausprägung ist.
    \item Die Höhe der Säule kann die absoluten oder die relative Häufigkeit darstellen. 
\end{itemize} 

\noindent\textbf{Gestaltung von Balkendiagrammen}\\
Das Balkendiagramm ist identisch zum Säulendiagramm, jedoch um 90 Grad gedreht.
\begin{itemize}
    \item Auf der x-Achse (Abszisse) ist die Skala abgetragen
    \item Auf der y-Achse (Ordinate) sind die Merkmalsausprägungen abgetragen.
\end{itemize} 

\noindent\textbf{Gestapeltes Balkendiagramm}\\
Bei einem gestapelten Balkendiagramm nutzt man die Tatsache, dass sich die relativen Häufigkeiten zu 1 aufsummieren. Hat man beispielsweise in verschiedenen Jahren unterschiedliche Zusammensetzungen der relativen Anteile, kann man diese in einem gestapelten Säulendiagramm gut vergleichen.\\

\noindent \framebox[\textwidth]{\texttt{R-Befehl für Balken-/Säulendiagramme: > barplot(data)} \hfill \href{https://www.rdocumentation.org/packages/graphics/versions/3.6.2/topics/barplot}{Dokumentation}}

\noindent \dangersign[3ex] Für gestapelte Balkendiagramme muss der Funktion eine Tabelle übergeben werden.

\subsubsection{Kreisdiagramm}
\textbf{Gestaltung von Kreisdiagrammen}\\
Jede Merkmalsausprägung erhält einen Sektor des Kreises. Man berechnet den Winkel durch die Multiplikation der relativen Häufigkeit mit 360°.\\
Das Kreisdiagramm kann bei allen Skalen verwendet werden, jedoch kann die Ordnung von Ausprägungen nicht wiedergegeben werden. Somit würde bei Verwendung einer Ordinalskala der Informationsgehalt über die Ordnung verloren gehen.\\

\noindent \framebox[\textwidth]{\texttt{R-Befehl für Kreisdiagramme: > pie(data)} \hfill \href{https://www.rdocumentation.org/packages/graphics/versions/3.6.2/topics/pie}{Dokumentation}}

\subsubsection{Histogramme}
Das Histogramm ist, im Gegensatz zu den bisher vorgestellten Diagramm-Typen, nur bei metrischer Skala anwendbar.\\
\textbf{Gestaltung von Histogrammen}
\begin{itemize}
    \item Da hier ein metrisches Merkmal vorliegt, muss dieses zunächst einmal in Klassen eingeteilt werden.
    \item Auf der x-Achse (Abszisse) ist die Skala des Merkmals abgebildet.
    \item Da die Fläche der einzelnen Balken den relativen Häufigkeiten entspricht (bzw. Histogrammfläche und relative Häufigkeit sind proportional zueinander), lässt sich die Höhe eines Balkens (Ordinate) wie folgt berechnen:
    \begin{align*} 
    &h_j = \frac{f_j}{d_j} \qquad (<=> h_j\cdot d_j = f_j) 
    \end{align*}
    \item Werden Klassen gleicher Breite verwendet, so wird des Öfteren auch die relative/absolute Häufigkeit auf der Ordinate abgetragen, da auch hierdurch die Forderung der Proportionalität (siehe oben) gewahrt wird.
\end{itemize}

\noindent \textbf{Probleme}\\
Da das Aussehen von der gewählten Klassengröße abhängt, sollte man, wie schon in (vgl. \textit{Klassenbildung} Kapitel \ref{sec:abs.Häuf})  erwähnt, sachlogische Gegebenheiten bei der Wahl der Klassenbreiten heranziehen.\\
Offene Klassen, die gegen unendlich gehen, sind nicht abbildbar. Eine Möglichkeit ist, die Klasse so zu wählen, dass darin schon die Mehrheit der Merkmalsausprägungen enthalten sind. (vgl. Induktive Statistik im zweiten Semester)\\

\noindent \framebox[\textwidth]{\texttt{R-Befehl für Histogramme: > hist(data)} \hfill \href{https://www.rdocumentation.org/packages/graphics/versions/3.6.2/topics/hist}{Dokumentation}}

\noindent \dangersign[3ex] Per default nutzt \texttt{R} gleiche Klassenbreiten, deren Anzahl mit Hilfe der \href{https://en.wikipedia.org/wiki/Histogram#Sturges'_formula}{Formel von Sturges} berechnet wird. Selbst gewählte Klassen (auch ungleicher Breite) können mit dem \texttt{breaks}-Argument übergeben werden.

\subsection{Ordnungsstatistik}

Für die Berechnungen einige Maße ist es wichtig, dass die Merkmalsausprägungen geordnet sind. Die \textit{Ordnungsstatistik} ist nur bei bei ordinaler und metrischer Skala verwendbar, da man nominale Merkmale nicht ordnen kann (vgl. Kapitel \ref{sec:Skalentypen}). Dabei werden die Ausprägungen der Urliste in eine aufsteigende Ordnung gebracht. Damit man erkennt, ob eine Urliste oder eine Ordnungsstatistik vorliegt, werden die tiefgestellten Indizes bei der Ordnungsstatistik in Klammern $x_{(i)}$ gesetzt.\\
Die tiefgestellte Zahl in Klammern gibt den Rang an. Gibt es zwei Merkmalsausprägungen mit der gleichen Ausprägung, dann nennt man das \textit{Bindung} (Tie). Wenn man Bindungen in einer Ordnungsstatistik berücksichtigt, dann erhalten diese den gemittelten Wert ihrer bisherigen Position in der Ordnungsstatistik.\\
Beispiel: Urliste: $x_1=8;\quad x_2=4;\quad x_3=5;\quad x_4=1;\quad x_5=4$\\
Ordnungsstatistik (mit Bindungen): $x_{(1)}=1;\quad x_{(2,5)}=4;\quad x_{(2,5)}=4;\quad x_{(4)}=5;\quad x_{(5)}=8$\\
Rangabfrage: $Rg(4)=2,5$\\

\noindent \framebox[\textwidth]{\texttt{R-Befehl für die Ordnungsstatistik: > sort(data)} \hfill \href{https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/sort}{Dokumentation}}


\subsection{Empirische Verteilungsfunktion}\label{sec:emp-vert}
Bei der empirischen Verteilungsfunktion benötigt man die Ordnungsstatistik, daher ist sie nur für Merkmale mit ordinaler und metrischer Skala anwendbar. Hier werden nicht die einzelnen Merkmalsausprägungen in ihrer Häufigkeit einzeln dargestellt, sondern die Häufigkeiten werden \textit{kumuliert}, d.h. aufsummiert.

\begin{align*} 
&F(x) = \sum_{a_j\leq x} f(a_j) 
\end{align*}
    
\noindent F(x) ist die kumulierte relative Häufigkeit an der Stelle x, das bedeutet, dass alle Wahrscheinlichkeiten $f(a_j)$ der Merkmalsausprägungen $a_j$ kleiner gleich x aufsummiert werden.\\
Die relativen Häufigkeiten werden immer weiter aufsummiert, weshalb die Funktion monoton wachsend ist und dann schließlich bei 1 stagniert, da die kumulierte relative Häufigkeit nicht höher als 1 sein kann. Somit ist der Wertebereich von F(x) von 0 und 1.\\

\noindent Es gibt verschiedene Vorgehensweisen bei diskreten und stetigen Merkmalen, deshalb betrachten wir im Folgenden die Verteilungsfunktion für die beiden Skalen separat.

\subsubsection{Vorgehensweise bei ordinalen und diskreten Merkmalen}
\begin{enumerate}
    \item Ordnungsstatistik bilden
    \item Relativen Häufigkeiten berechnen
    \item Kumulierte Häufigkeiten $F(x)$ für jede \textit{unterschiedliche} Merkmalsausprägung berechnen 
    \item Graph: Trage die kumulierten Häufigkeiten als ($x_i$; $F(x_i$)) in ein Diagramm ein und verlängere die Punkte mit einem \textit{horizontalen} Strich, bis zum Abszissenwert der nächsten Merkmalsausprägung. Somit entsteht eine Treppenfunktion, welche von 0 bis 1 geht.
\end{enumerate}
Die Rechenregeln für ordinale und diskrete Merkmale: Skript (vgl. Slide 2.39)

\subsubsection{Vorgehensweise bei stetigen Merkmalen}
\begin{enumerate}
    \item (Geordnete) Klassen bilden (vgl. Kapitel \ref{sec:Klassenbildung})
    \item Relativen Häufigkeiten der Klassen berechnen
    \item Kumulierten Häufigkeiten $F(e_j)$ für jede Klasse berechnen
    \item Graph: Trage die kumulierten Häufigkeiten als ($e_j$; $F(e_j)$) in das Diagramm ein und verbinde die Punkte (da man eine Gleichverteilung innerhalb der Klassen annimmt).
\end{enumerate}
\textbf{Berechnung der empirischen Verteilungsfunktion von klassierten Daten:}\\
Die Folgende Formel verwendet man, um eine kumulierte Häufigkeit einer bestimmten Merkmalsausprägung zu bekommen.
\begin{align*}
    F(x)=\begin{cases}
			0      & x < e_0        \\
            F(e_{j-1}+\frac{f_j}{d_j}(x-e_{j-1}) & x \in[e_{j-1}]\\
            1      &x>e_k
		 \end{cases}
\end{align*}
Die Schwäche ist, dass man hierfür von einer Gleichverteilung innerhalb der Klassen ausgehen muss, was eine sehr starke Annahme ist und nicht immer unbedingt realistisch ist.\\

\noindent \framebox[\textwidth]{\texttt{R-Befehl für die emp. Verteilungsfunktion: > ecdf(data)} \hfill \href{https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/ecdf}{Dokumentation}}


\clearpage


\section{Lagemaße}
Grob gesagt beschreiben (zentrale) Lageparameter, wo sich der Schwerpunkt der Daten auf einer Skala befindet. Manche Lageparameter machen nur bei bestimmten Skalen Sinn. In der folgenden Tabelle ist markiert, bei welchen Skalen die einzelnen Lageparameter jeweils Sinn machen.

\begin{center}
 \begin{tabular}{|l c c c|} 
 \hline
 & Nominalskala & Ordinalskala& metrische Skala\\
 \hline\hline
 Modus &x & x & x \\ 
 \hline
 Median &  & x & x \\
 \hline
 Quantile &  & x & x \\
 \hline
 Box-Plots &  & x & x \\
 \hline
Mittelungen &  &  & x \\
 \hline
\end{tabular}
\end{center}

\subsection{Modus}
Die Merkmalsausprägung, die am häufigsten auftritt, nennt man Modus. Diese Maßzahl macht sowohl bei diskreten oder bei (quasi)stetigen Merkmalen Sinn, solange es eine überschaubare Anzahl an verschiedenen Merkmalsausprägungen gibt. Dies ist bei diskreten Merkmalen logischerweise öfter der Fall als bei (quasi)stetigen Merkmalen. Bei klassiert-stetigen Merkmalen nimmt man oft die Klassenmitte der Klasse mit der höchsten absoluten Häufigkeit.\\

\noindent \framebox[\textwidth]{\texttt{R-Befehl für den Modus: > Mod(data)} \hfill \href{https://www.rdocumentation.org/packages/DescTools/versions/0.99.37/topics/OddsRatio}{Dokumentation}}

\noindent \dangersign[3ex] Die R-Funktion ist \textbf{nicht} Teil von \texttt{base-R} sondern Teil des Paketes \texttt{DescTools}. Dieses muss zunächst installiert (\texttt{install.packages("DescTools")}) und anschließend importiert werden (\texttt{library(DescTools)}).

\subsection{Median/Zentralwert}\label{sec:median}
Um den Median bei diskreten Merkmalen anwenden zu können, bringt man die Urliste zuerst einmal in eine Ordnungsstatistik.
Sind die Merkmalsausprägungen aufsteigend geordnet, teilt der Median die Grundgesamtheit in zwei gleich große Teile. Unterhalb des Medians liegen die Hälfte (50\%) der Werte die \textit{kleiner oder gleichen} dem Median sind und im zweiten Teil befinden sich die andere Hälfte der Merkmalsausprägungen die \textit{größer oder gleich} dem Median sind. Der Median wird mit $\tilde{x}_{0,5}$ bezeichnet.
Alternativ kann man den Median auch mit der Verteilungsfunktion berechnen, indem man den Wert bestimmt, bei dem die kumulierte relative Häufigkeit 0,5 beträgt. Dies würde dann so aussehen: $F(\tilde{x}_{0,5})=0,5$.\\
Die Stärke des Medians ist, dass dieser relativ unempfindlich gegenüber Ausreißern und Extremwerten ist. Das heißt, wenn bspw. 10 Merkmalsausprägungen einer Grundgesamtheit im Bereich zwischen 0 und 10 haben, macht es keinen Unterschied, ob der größte Wert auch innerhalb dieses Bereichs liegt (also z.B. den Wert 10 hat) oder weit drüber hinaus geht (z.B. 60), da für den Median lediglich die \textit{Anzahl} der Beobachtungen über-/unterhalb herangezogen werden, nicht jedoch deren konkreter Wert.

\noindent Für den Median gibt es eine Fallunterscheidung bzgl. der Berechnung zwischen gerader und ungerader Anzahl der Beobachtungen. Für die ungerade Anzahl an Beobachtungen $n$ ist es einfach der mittlere Wert der Ordnungsstatistik. Da es jedoch bei gerade Anzahl von Beobachtungen keine Mitte gibt, ist es hier ein bisschen aufwändiger. Man bildet das arithmetische Mittel der beiden Beobachtungen, zwischen denen die Mitte der Ordnungsstatistik wäre.
\begin{align*}
   \tilde{x}_{0,5}=\begin{cases}
			x_{\frac{(n+1)}{2}}     & \text{falls n ungerade}       \\
            \frac{1}{2}(x_{(\frac{n}{2})}+x_{(\frac{n}{2}+1)})  & \text{falls n gerade}\\
		 \end{cases}
\end{align*}
Bei klassierten metrischen Merkmalen kann man nicht so wie oben beschrieben vorgehen, da man die exakten Merkmalsausprägungen nicht kennt.\\
Deshalb greift man hier wieder auf die Annahme der Gleichverteilung innerhalb der Klassen zurück. Man bestimmt die Klasse, in der der Median liegt, d.h. man schaut in welcher Klasse die kumulierte relative Häufigkeit 0,5 liegt. Unter Verwendung der Formel für klassierten Daten (siehe Formelsammlung) kann man nun den Median berechnen.\\
Man subtrahiert von 0,5 die kumulierte relative Häufigkeit der unteren Klassengrenze der Klasse, die den Median enthält. Das Ergebnis dividiert man durch die relative Häufigkeit der Klasse in der der Median enthalten ist. Den Quotienten multipliziert man mit der Klassenbreite. Zum Schluss addiert man die untere Klassengrenze, der Klasse, in der der Median liegt dazu.\\

\noindent \framebox[\textwidth]{\texttt{R-Befehl für Median: > median(data, ...)} \hfill \href{https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/median}{Dokumentation}}

\subsection{Quantile}\label{sec:Quantile}
Die Quantile sind wie der Median bei ordinalen und metrischen Daten anwendbar (Um genau zu sein ist der Median lediglich ein bestimmtes Quantil). Uns interessiert möglicherweise nicht nur der Median, der uns durch seine Lage sagt, an welcher Stelle 50\% der Werte kleiner oder gleich diesem Wert sind, sondern potentiell die gleiche Aussage auch für andere Prozentwerte (bspw. 25\%). Somit können wir die Grundgesamtheit durch ein Quantil in zwei beliebig großen Teilbereich aufteilen, welche sich zu 100\% aufsummieren. Man wählt einen Wert $\alpha$ zwischen 0 und 1 auswählen und kann somit die Lage des Quantils zu bestimmen. Um die Realisation des Quantils zu bestimmen, multipliziert man die Anzahl der Beobachtungen $n$ mit $\alpha$.
\begin{itemize}
    \item Wenn das Produkt n$\alpha$ keine ganze Zahl ist, dann ist die nächst größere ganze Zahl die Realisation des Quantils.
    \item Wenn das Produkt n$\alpha$ eine ganze Zahl ist, dann addiert man diese mit dem nächst größeren Wert und bildet daraus den Mittelwert.
\end{itemize}
Wie der Median sind auch die Quantile (relativ) unempfindlich gegenüber Ausreißern, da sie in erster Linie nur auf der Lage der Ausprägungen basiert und nicht auf den konkreten Werten.\\

\noindent \framebox[\textwidth]{\texttt{R-Befehl für Quantile: > quantile(data, probs, ...)} \hfill \href{https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/quantile}{Dokumentation}}

\paragraph{Besondere Quantile}
Zusätzlich zum Median, der dem 50\%-Quantil entspricht, gibt es noch zwei weitere besondere Quantile. Zum einen das \textbf{untere Quartil}, das dem 25\%-Quantil entspricht, und zum anderen das \textbf{oberen Quartil}, welches dem 75\%-Quantil entspricht. Diese beiden sind u.a. im folgenden Kapitel \ref{sec:Box-Plot} für die Box-Plots relevant.

\subsection{Boxplots} \label{sec:Box-Plot}
Ein Boxplot ist eine grafische Darstellung, mit der man sich schnell einen Überblick über die Verteilung der Ausprägungen eines Merkmals schaffen kann und sehr schnell die Unterschiede zwischen Verteilungen von verschiedenen Merkmalen erkennen. 
Um den \textit{einfachen} Box-Plot zu zeichnen, benötigt man den Median und das obere und untere Quartil, sowie das Minimum und das Maximum.
\paragraph{Vorgehensweise:}
\begin{enumerate}
    \item Zuerst zeichnet man die Box, die durch das untere und obere Quartil begrenzt ist.
    \item Danach zeichnet man in die Box den Median als dicke Linie ein.
    \item Die Striche die von der Box weggehen sind die sog. \textit{Whiskers}. Sie gehen bis zum Minimum und Maximum.
\end{enumerate}
Zeichnet man den modifizierten Boxplot, so kommen drei Schritte hinzu. Die Modifikation liegt hierbei in der Kennzeichnung von Ausreißern. Ausreißer sind hierbei (für gewöhnlich) definiert als Werte, die \textit{"mehr als 1,5-mal die Boxlänge von einem der beiden Quartile entfernt sind"}.
\begin{enumerate}
    \item Man berechnet die Länge der Box, indem man das untere vom oberen Quartil abzieht. Anschließend bestimmt man damit die "Grenze", ab wo ein Wert ein Ausreißer nach oben oder unten wäre.
    \item Die Whiskers gehen nun nicht mehr bis zum Minimum/Maximum, sondern lediglich bis zum kleinsten/größten Wert innerhalb der berechneten Grenzen.
    \item Alle Werte, außerhalb dieser Grenzen zeichnet man als Kreis ein. Dies sind Ausreißer.
    \item \textit{Anmerkung:} Von Extremwerten spricht man, wenn ein Wert mehr als 3 Boxlängen entfernt von dem oberen bzw. unteren Boxenrand entfernt liegt. Diese werden manchmal separat mit einem Sternchen eingezeichnet.
\end{enumerate}

\noindent \framebox[\textwidth]{\texttt{R-Befehl für den Boxplot: > boxplot(data, ...)} \hfill \href{https://www.rdocumentation.org/packages/graphics/versions/3.6.2/topics/boxplot}{Dokumentation}}

\subsection{Mittelungen}
Im Folgenden werden drei verschiedene Mittelungen aufgeführt. Je nach dem ob die Daten in gleicher/unterschiedlicher Gewichtung in einen Mittelwert einfließen sollen oder ob es sich um eine multiplikative Verknüpfung zwischen den Werten handelt werden arithmetisches, harmonisches oder geometrisches Mittel verwendet.

\subsubsection{arithmetisches Mittel}\label{sec:arithm}
Das arithmetische Mittel ist den meisten sicherlich als \textit{"Durchschnitt"} bekannt. Hierbei gehen alle Daten mit \textit{gleicher} Gewichtung in die Berechnung ein. Diese erfolgt durch Aufsummieren aller Merkmalsausprägungen und Teilen durch die Anzahl $n$ der Merkmalsausprägungen.\\
Liegen klassierte/gruppierte Daten mit unterschiedlichen Klassengrößen $n_j$ vor, so macht man Gebrauch vom gewichteten arithmetischen Mittel. Hierbei werden die Merkmalsausprägungen beim Aufsummieren mit ihrer Klassengröße $n_j$ gewichtet und diese Summe anschließend durch $n$ geteilt.\\
Eine Schwäche des Mittelwertes ist die Empfindlichkeit gegenüber Ausreißern/Extremwerten, welche dadurch zustande kommt, dass die Abweichungen aller Ausprägungen in Summe Null ergeben. Somit verschiebt sich der Mittelwert durch extreme Werte sehr schnell in deren Richtung.\\

\noindent \framebox[\textwidth]{\texttt{R-Befehl für das arithm. Mittel: > mean(data, ...)} \hfill \href{https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/mean}{Dokumentation}}

\subsubsection{harmonisches Mittel}
Das harmonische Mittel wird im Gegensatz zum arithmetischen Mittel verwendet, wenn die Merkmalsausprägungen unterschiedlich gewichtet werden sollen. Ein Hinweis darauf, dass man das harmonische Mittel verwenden muss, sind Verhältniszahlen, bspw. $\frac{km}{h}$ oder $\frac{EUR}{h}$.\\
Man berechnet das harmonische Mittel durch den Quotienten aus den aufsummierten Anteilen und der Summe der Quotienten aus der Gewichtung und der Merkmalsausprägung $x_i$.
Wenn man das harmonische Mittel aus einer Häufigkeitstabelle berechnet, ergibt sich ein Sonderfall. Denn der Zähler (die aufsummierten Anteile), ist $n$ (bzw. 1 bei relativen Häufigkeiten) und der Nenner ergibt sich aus dem Quotient der absoluten Häufigkeit (bzw. relativen Häufigkeit) der Merkmalsausprägung durch die Merkmalsausprägung selbst.\\

\noindent \framebox[\textwidth]{\texttt{\textbf{Kein} R-Befehl für das harmonische Mittel in base-R verfügbar.\hfill \mbox{}}}

\subsubsection{geometrische Mittel}
Das geometrische Mittel verwendet man bei relativen Merkmalsausprägungen (z.B. Wachstumsfaktoren), die sich auf einen bestimmten Ausgangswert beziehen. Bei solchen Werten kann man Aussagen treffen, wie sich Werte zwischen zwei Zeitpunkten verändert haben. Mit dem geometrischen Mittel berechnet man dann die durchschnittliche Veränderung eines Wertes im Zeitverlauf. Da es sich um eine multiplikative Verknüpfung handelt, multipliziert man alle relativen Veränderungen und zieht dann die n-te Wurzel aus der Anzahl der $n$ relativen Veränderungen. Alternativ kann man auch anstelle des Aufmultiplizierens auch einfach den Quotient des n-ten Ausgangswert durch den ersten Ausgangswert teilen und anschließend die n-te Wurzel ziehen.\\

\noindent \framebox[\textwidth]{\texttt{\textbf{Kein} R-Befehl für das geometrische Mittel in base-R verfügbar. \hfill \mbox{}}}

\subsubsection{Vergleich von Mittelwert \& Median}
Ein Unterschied zwischen arithmetischem Mittel und Median ist die Empfindlichkeit bzw. Robustheit gegenüber den Ausreißern. Der Median ist relativ robust ggü. Ausreißern, während das arithmetische Mittel eher empfindlich ist. Basierend auf diesen Eigenschaften können durch deren Vergleich Rückschlüsse auf die Verteilung der Daten gezogen werden.

\paragraph {Symmetrische Verteilung} Fallen Median und arithmetisches Mittel zusammen (d.h. sind in etwa gleich), dann spricht man von einer symmetrischen Verteilung, da hierdurch der Schluss gezogen werden kann, dass entweder (i) keine Ausreißer vorliegen oder (ii) sich die Ausreißer auf beiden Seiten (d.h. nach oben und unten) die Waage halten. 

\paragraph{Asymmetrische Verteilung} Fallen Median und arithmetisches Mittel auseinander, dann spricht man von einer asymmetrischen Verteilung. Wenn das arithmetische Mittel größer ist als der Median, kann man daraus schließen, dass es tendenziell eher Ausreißer nach oben gibt. Eine solche Verteilung wird als linkssteil bzw. rechtsschief bezeichnet. Linkssteil, da sich die untere Hälfte der Daten (links vom Median) eher nah am Median befindet und das Histogramm somit eher steil ansteigend aussieht. Rechtssteil, da das arithmetische Mittel durch die potenziellen Ausreißer weiter nach "rechts gezogen" (rechtsschief) wird und das Histogramm eher flach abfallend aussieht. Deshalb ist das arithmetische Mittel in diesem, Fall größer als der Median.\\
Die Verteilung heißt im Gegensatz dazu rechtssteil bzw. linksschief, wenn der Median größer ist als das arithmetische Mittel. D.h. es kann genau dieselbe Intuition wiederverwendet werden, nur diesmal in die andere Richtung.

\clearpage

\subsection{Aufgaben}

\paragraph{1. Unterschiede zwischen Mittelwert und Median?}

\begin{itemize}
    \item[a)] Mittelwert ist robuster ggü. Ausreißern \hfill $\square$
    \item[b)] Median ist robuster ggü. Ausreißern   \hfill $\square$
    \item[c)] Keine \hfill $\square$
\end{itemize}

\paragraph{2. Der Median ..}

\begin{itemize}
    \item[a)] .. liegt immer genau in der Mitte der Box. \hfill $\square$
    \item[b)] .. entspricht dem 50\%-Quantil. \hfill $\square$
    \item[c)] .. entspricht dem 2. Quartil. \hfill $\square$
    \item[d)] .. ist wichtig dafür, zu berechnen wann ein Wert ein Ausreißer ist. \hfill $\square$
\end{itemize}

\paragraph{3. Welche Mittelung ist geeignet, um den durchschnittlichen Anstieg der Transferausgaben in der Fußballbundesliga zu ermitteln?}

\begin{itemize}
    \item[a)] Arithmetisches Mittel \hfill $\square$
    \item[b)] Geometrisches Mittel \hfill $\square$
    \item[c)] Harmonisches Mittel \hfill $\square$
    \item[d)] Alle drei machen Sinn \hfill $\square$
\end{itemize}


\newpage


\section{Streuungsmaße}
Bis jetzt haben wir uns Lagemaße angeschaut, welche nur etwas über die (zentrale) Lage der Daten aussagen. Um zu quantifizieren, wie stark die Daten schwanken/streuen (Hier: Wie stark die Daten um einen Mittelwert schwanken). Weil man zur Berechnung von Streuungsmaßen Differenzen benötigt, ist auch hier wieder nur für bestimmte Skalenniveaus die Berechnung der vorgestellten Streuungsmaße möglich.

\begin{center}
 \begin{tabular}{|l c c c|} 
 \hline
 & Nominalskala & Ordinalskala& metrische Skala\\
 \hline\hline
 Spannweite & & x & x \\ 
 \hline
Quartilsabstand &  & x & x \\
 \hline
 Mittlere absolute Abweichung (MAD) &  &  & x \\
 \hline
Varianz &  &  & x \\
 \hline
Standardabweichung &  &  & x \\
\hline
Variationskoeffizient & & & x\\
\hline  
\end{tabular}
\end{center}

\subsection{Spannweite}
Als \textbf{Streubereich} bezeichnet man den Bereich in dem die gesamten Merkmalsausprägungen liegen. Dessen Breite bezeichnet man als \textbf{Spannweite}. Dabei subtrahiert man den kleinsten Wert (\textit{Minimum}) vom größten Wert (\textit{Maximum}). Da es nur auf diesen beiden Werten basiert, ist es anfällig gegenüber Ausreißern, welche zu sehr großen Spannweiten führen können.\\

\noindent \framebox[\textwidth]{\texttt{R-Befehl für die Spannweite: > max(data) – min(data)} \hfill \href{https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/Extremes}{Dokumentation}}

\subsection{Quartilsabstand}
Im Gegensatz zur Spannweite ist der (Inter-)Quartilsabstand \textit{robust} gegenüber Ausreißern (d.h. er wird nicht von ihnen beeinflusst). Im vorigen Kapitel wurden bei den Quantilen (vgl. Kap. \ref{sec:Quantile}) die beiden besonderen Quantile, \textit{oberes und unteres Quartil}, vorgestellt. Aus deren Abstand ergibt sich der (Inter-)Quartilsabstand. Grafisch kann man sich dies als die Länge der Box im Boxplot veranschaulichen (vgl. Kap. \ref{sec:Box-Plot}).
Im (Inter-)Quartilsabstand liegen somit die mittleren/zentralen 50\% der Werte, darunter logischerweise auch der Median (vgl. Kap. \ref{sec:median}).
\begin{align*}
    d_Q = \tilde{x}_{0,75}-\tilde{x}_{0,25}
\end{align*}

\noindent \framebox[\textwidth]{\texttt{R-Befehl für den Qurtilsabstand: > IQR(data)} \hfill \href{https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/IQR}{Dokumentation}}

\subsection{Mittlere absolute Abweichung (MAD)}
Die \textit{mittlere absolute Abweichung} (Englisch: mean absolute deviation) gibt die durchschnittliche Abweichung der Merkmalsausprägungen um einen bestimmten (zentralen) Wert $A$ an. $A$ kann beispielsweise der Median oder der Mittelwert sein. Bei der Berechnung werden die betragsmäßigen Differenzen aus den einzelnen Datenpunkten und $A$ aufsummiert und durch die Beobachtungszahl $n$ dividiert.

\begin{align*}
    D_A = \frac{1}{n} \sum_{i=1}^n |x_i-A| 
\end{align*}

\noindent Die Betragsstriche verhindern dabei, dass sich die positiven und negativen Abweichungen "aufheben". Eine weitere Möglichkeit dies zu vermeiden, wäre die Differenzen zu quadrieren (vgl. Kap. \ref{sec:Varianz}).\\

\noindent \framebox[\textwidth]{\texttt{R-Befehl für den MAD: > mad(data, center = median(data), ...)} \hfill \href{https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/mad}{Dokumentation}}

\subsection{Varianz}\label{sec:Varianz}
Die Varianz $s^2$ ist die mittlere \textit{quadratische} Abweichung zum arithmetischen Mittel. Die Varianz ist dabei das gängigste Maß für die Streuung von Merkmalsausprägungen um das arithmetische Mittel. Wie oben bereits erwähnt, wird durch die Quadrierung verhindert, dass sich die positiven \& negativen Abweichungen "aufheben" können.

\begin{align*}
    s^2 = \frac{1}{n} \sum_{i=1}^n (x_i-\bar{x})^2
\end{align*}

\paragraph{Verschiebungssatz} Jede einzelne Abweichung auszurechnen und zu quadrieren kann bei großer Anzahl $n$ von Datenpunkten sehr umständlich sein. Deshalb kann durch die Umformung mittels des Verschiebungssatzes eine handrechnerisch leichtere Form erreich werden:

\begin{align*}
    s^2 = \frac{1}{n} (\sum_{i=1}^n x^2_i)-\bar{x}^2 = \overline{x^2} - \bar{x}^2
\end{align*}

\noindent \textit{Anmerkung:} Neben dem arithm. Mittel (welches auch für die "normale" Varianz-Formel benötigt wird) muss hier nur noch das arithm. Mittel der \textit{quadrierten} Daten berechnet werden, um schließlich die Varianz berechnen zu können.

\paragraph{Varianz aus klassierten/gruppierten Daten} Liegen nun keine Einzeldaten vor sondern gruppierte Daten von denen die Varianz bestimmt werden soll, geht man folgendermaßen vor:
Die Streuung in zwei Teile zerlegt ($s^2_{zwischen}$ und $s^2_{innerhalb}$), diese separat berechnet und anschließend addiert werden.\\
\textit{Anmerkung:} Dies mag auf den ersten Blick etwas kontra-intuitiv erscheinen, jedoch ist diese Berechnung auch ohne Kenntnis der Einzeldaten (d.h. mit de facto weniger Information) möglich. Aufgrund dessen ist diese Zerlegung in manchen Fällen hilfreich.\\

\noindent $s^2_{zwischen}$: Bei der Streuung zwischen den Klassen wird die durchschnittliche quadratische Abweichung der Mittelwerte der Klassen $(\bar{x_j})$ vom Mittelwert aller Daten $(\bar{x})$ berechnet.

\begin{align*}
    s^2_{zwischen} = \frac{1}{n} \sum_{i=1}^k n_j (\bar{x_j}-\bar{x})^2
\end{align*}

\noindent $s^2_{innerhalb}$: Bei der Streuung innerhalb der Klassen wird zuerst die Streuung jeder einzelnen Gruppe ($s^2_j=\frac{1}{n} \sum_{i=1}^n (x_i-\bar{x})^2$) berechnet. Von diesen gruppenspezifischen Varianzen wird anschließend das gewichtete arithmetische Mittel (vgl. Kap. \ref{sec:arithm}) gebildet:

\begin{align*}
    s^2_{innerhalb} = \frac{1}{n} \sum_{i=1}^k n_j s^2_j
\end{align*}

\noindent Somit kann man die gesamte Varianz durch die Summe aus $s^2_{zwischen}$ und $s^2_{innerhalb}$ berechnen.\\

\noindent \framebox[\textwidth]{\texttt{R-Befehl für die Varianz: > var(data)} \hfill \href{https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/cor}{Dokumentation}}

\noindent \dangersign[3ex] Die R-Funktion teilt bei der Berechnung der Varianz nicht durch $n$ sondern durch $n-1$. Die Hintergründe dafür werden in Statistik II (vgl. Kap. \ref{sec:ML}) erläutert. Um dies zu umgehen und die (empirische) Varianz zu berechnen, sollte das Ergebnis dieses R-Befehls mit \texttt{(n - 1) / n} multipliziert werden.

\subsection{Standardabweichung}
Die Standardabweichung erhält man, indem man die positive Wurzel der Varianz (vgl. Kap. \ref{sec:Varianz} zieht. Der Vorteil der Standardabweichung ist, dass diese wieder in der gleichen Einheit wie die Beobachtungswerte vorliegt, da wir das Quadrieren aus der Formel für die Varianz durch das Wurzelziehen wieder auflösen.\\

\noindent \framebox[\textwidth]{\texttt{R-Befehl für die Standardabweichung: > sd(data)} \hfill  \href{https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/sd}{Dokumentation}}

\noindent \dangersign[3ex] Wie bei der Varianz wird auch beim R-Befehl für die Standardabweichung nicht durch $n$ sondern durch $n-1$ geteilt.

\subsection{Variationskoeffizient}
Beim Variationskoeffizient wird die Standardabweichung in Beziehung zum arithm. Mittel gesetzt, damit die Streuungen von Datensätzen mit unterschiedlichen Mittelwerten miteinander verglichen werden können. Die Berechnung erfolgt durch den Quotienten aus Standardabweichung und arithm. Mittel, dadurch wird der Variationskoeffizient dimensionslos. \\

\noindent \framebox[\textwidth]{\texttt{R-Befehl(e) für den Variationskoeffizient: > sd(data) / mean(data) \hfill \mbox{}}}

\clearpage

\subsection{Aufgaben}

\paragraph{1. Bei welcher Maßzahl werden hohe Abweichungen vom Mittelwert stärker gewichtet?}

\begin{itemize}
    \item[a)] MAD \hfill $\square$
    \item[b)] Varianz   \hfill $\square$
    \item[c)] Bei beiden gleich stark \hfill $\square$
\end{itemize}

\paragraph{2. Welche Aussagen zur Streuungszerlegung sind wahr?}

\begin{itemize}
    \item[a)] Die Varianz innerhalb der Gruppen ist immer größer als zwischen den Gruppen. \hfill $\square$
    \item[b)] Man kann die Varianz innerhalb und zwischen den Gruppen einfach addieren\\um die Gesamtvarianz zu erhalten. \hfill $\square$
    \item[c)] Es gibt Sonderfälle, bei denen die Streuung zwischen den Gruppen der\\Gesamtstreuung entspricht. \hfill $\square$
    \item[d)] Es muss immer eine Streuung innerhalb der Gruppen vorliegen. \hfill $\square$
\end{itemize}

\paragraph{3. Der Verschiebungssatz ..}

\begin{itemize}
    \item[a)] .. erleichtert die Berechnung des arithmetischen Mittels. \hfill $\square$
    \item[b)] .. kann auch bei gruppierten Daten verwendet werden. \hfill $\square$
    \item[c)] .. dient zur Berechnung des arithmetischen Mittels der quadrierten Daten. \hfill $\square$
    \item[d)] .. benötigt das arithmetische Mittel der quadrierten Daten. \hfill $\square$
\end{itemize}

\paragraph{4. Welche der folgenden Aussagen zum Variationskoeffizienten sind wahr?}

\begin{itemize}
    \item[a)] Der Variationskoeffizient ermöglicht den Vergleich von Streuungen von Merkmalen,\\die in verschiedenen Einheiten gemessen werden. \hfill $\square$
    \item[b)] Der Variationskoeffizient ermöglicht den Vergleich von Streuungen von Merkmalen,\\die in verschiedenen Größenordnungen liegen. \hfill $\square$
    \item[c)] Für die Berechnung des Variationskoeffizienten müssen beide Merkmale in der gleichen\\Einheit vorliegen. \hfill $\square$
    \item[d)] Zur Berechnung des Variationskoeffizienten benötigt man den Median. \hfill $\square$
\end{itemize}


\newpage


\section{Konzentrationsmaße}\label{sec:konz}
Bis jetzt können wir bei einem Datensatz Aussagen über die (zentrale) Lage der Daten und das Ausmaß der Streuung treffen. Im Folgenden werden auch Aussagen über die \textit{Konzentration} der Daten von Interesse sein, sowie deren graphische Darstellung. Somit kann bspw. ausgesagt werden, ob eine eher gleiche ($\widehat =$ faire) Verteilung oder möglicherweise ein Monopol vorliegt.\\
Da man die Daten ins Verhältnis zueinander setzt, sind diese Maßzahlen nur noch Merkmale mit  metrischem Skalenniveau möglich. Konzentrationsmaße werden im weiteren Verlauf in absolute und relative Konzentrationsmaße geteilt.

\subsection{Absolute Konzentrationsmaße}\label{sec:abs-konz}

\subsubsection{Konzentrationsrate} \label{sec:konz-rate}
Die Konzentrationsrate ist ein eher simples Maß, mit dem man Aussagen à la "\textit{Die drei größten Marktteilnehmer machen 60\% des Umsatzes.}" treffen kann. Hierfür addiert man einfach die (Markt-)Anteile der $g$ Merkmalsträger mit den größten Anteilen zusammen, wobei $g$ je nach Kontext/Interesse vorab gewählt werden muss:

\begin{align*}
    CR_g = \sum_{i=n-g+1}^n p_i = \sum_{i=n-g+1}^n (\frac{x_i}{\sum_{i=1}^n x_i})
\end{align*}

\noindent Würde ich beispielsweise bei 10 Merkmalsträgern die Konzentrationsrate der $g = 2$ größten Merkmalsträger berechnen wollen, dann würde ich die Summe der Anteile $p_i$ von $i=10-2+1=9$ bis $n=10$ berechnen. Somit also die Summe der Anteile des neunten und zehnten Merkmalsträgers.
Der Wertebereich der Konzentrationsrate kann von $\frac{g}{n}$, bei Gleichverteilung, wenn die Anzahl der g der größten Merkmalsträger auch dem Anteil an den gesamten Merkmalsträgern entspricht, bis hin zur 1 gehen. Bei einer Konzentrationsrate von 1 liegt ein Monopol (bei $g = 1$) oder ein Oligopol (bei $g \geq 2$) vor.\\

\noindent \textbf{Einschub: Praktische Relevanz // Wahl von $g$}\\
Das Gesetz gegen Wettbewerbsbeschränkung (\href{https://dejure.org/gesetze/GWB/18.html}{§18 GWB}) legt fest, wann man bei Unternehmen von marktbeherrschend sprechen kann. Um bei \textit{einem} Unternehmen von marktbeherrschend sprechen kann, benötigt dieses ein Marktanteil von mind. 40\%. Eine Gesamtheit von \textit{drei oder weniger} Unternehmen muss ein Marktanteil von mindestens 50\% erreichen und eine Gesamtheit von \textit{fünf oder weniger} Unternehmen muss einen Marktanteil von mindestens zwei Drittel erreichen, damit von Marktbeherrschung gesprochen werden kann.

\subsubsection{Konzentrationskurve}
Die Konzentrationsrate wird durch die Konzentrationskurve graphisch dargestellt. Bei der Konzentrationskurve werden zuerst die Merkmalsträger absteigend nach ihrer Größe abgetragen. Auf der x-Achse ist somit die kumulierte Anzahl der Merkmalsträger mit den größten Ausprägungen abgetragen (also $1, 2, \hdots, n$) und auf der y-Achse die kumulierten relativen Marktanteile. Je flacher der Graph ist, desto ähnlicher sind die Anteile verteilt. Entspricht der Graph einer Gerade, so liegt eine Gleichverteilung vor. 

\subsubsection{Herfindahl-Index}
Ein weiteres absolutes Konzentrationsmaß ist der Herfindahl-Index. Dieser bezieht sich nicht wie die Konzentrationsrate (vgl. Kap. \ref{sec:konz-rate}) nur auf die $g$ größten Merkmalsträger, sondern liefert somit eine allgemeinere Aussage über alle Merkmalsträger. Den Herfindahl-Index berechnet man als Quotienten aus der Summe der quadrierten Beobachtungen $\sum_{i=1}^n x_i^2$ und der quadrierten Summe aller Beobachtungen $(\sum_{i=1}^n x_i)^2$. Da im Zähler zuerst quadriert und anschließend aufsummiert wird, im Nenner hingegen zuerst aufsummiert und dann quadriert wird, ist klar, dass der Zähler stets kleiner oder gleich dem Nenner sein wird. Somit ergibt sich folgender Wertebereich:\\
Liegt ein Monopol vor (ein Merkmalsträger besitzt die gesamt Merkmalssumme), so sind Zähler \& Nenner identisch, was zu einem Wert von $H = 1$ führt. Bei einer Gleichverteilung (alle Merkmalsträger besitzen die gleiche Merkmalsausprägung $a$), erhalten wir den Wert $H = \frac{1}{n}$.\\

\noindent \textbf{Einschub I: Beweis für die untere Grenze:}

\begin{align*}
H =\frac{\sum_{i=1}^n x_i^2}{(\sum_{i=1}^n x_i)^2} = \frac{n \cdot a^2}{(n \cdot a)^2} = \frac{n \cdot a^2}{n^2 \cdot a^2} = \frac{1}{n}
\end{align*}

\noindent \textbf{Einschub II: Aussagen über Veränderungen}\\
Beim Herfindahl-Index kann man relativ einfach pauschale Aussagen über Veränderungen treffen, solange die Merkmalssumme gleich bleibt.
\begin{itemize}
    \item \textit{Beispiel 1: Fusion zweier Markteilnehmer}\\
    In diesem Fall würde sich der Nenner nicht ändern, da sich die gesamte Merkmalssumme nicht ändert. Der Zähler würde jedoch größer werden, da $(a + b)^2 > a^2 + b^2$. Wichtig ist aber hierbei auch, die damit einhergehende Veränderung des Wertebereichs zu beachten.
    \item \textit{Beispiel 2: Transfer von einem großen Merkmalsträger zu einem kleineren}\\
    Wird ein Teil der Merkmalssumme von einem größeren zu einem kleinerer Merkmalsträger transferiert (und bleibt der kleinere dadurch weiterhin kleiner), so wird der Herfindahl-Index ebenfalls sinken. Der Wertebereich ändert sich dabei nicht. Umgekehrt (Transfer von klein zu groß) gilt dieselbe Intuition.
\end{itemize}

\noindent \framebox[\textwidth]{\texttt{R-Befehl für den Herfindahl-Index: > Herfindahl(data)} \hfill \href{https://www.rdocumentation.org/packages/DescTools/versions/0.99.37/topics/Herfindahl}{Dokumentation}}

\noindent \dangersign[3ex] Die R-Funktion ist \textbf{nicht} Teil von \texttt{base-R} sondern Teil des Paketes \texttt{DescTools}. Dieses muss zunächst installiert (\texttt{install.packages("DescTools")}) und anschließend importiert werden (\texttt{library(DescTools)}).

\clearpage

\subsection{Relative Konzentrationsmaße}\label{sec:rel-konz}

\subsubsection{Lorenzkurve}\label{sec:lk}
Die Lorenzkurve ist eine grafische Methode um Konzentration auf eine \textit{relative} Art und Weise darzustellen. Sie ist bereits auf den ersten Blick von der Konzentrationskurve zu unterscheiden, da sowohl auf der x- als auch auf der y-Achse ausschließlich \textit{relative} Werte abgetragen sind. Somit gehen beide Achsen von 0 bis 1. Anhand der Lorenzkurve kann man Aussagen à la "\textit{Die ärmsten X\% der Merkmalsträger besitzen einen Anteil von Y\% der Merkmalssumme.}" treffen. Der Begriff "arm" und "besitzen" soll hierbei aber noch signalisieren, dass dieses Konzept lediglich auf Einkommen, o.ä. anwendbar ist, sondern dient hier einfach als plastische Beispielformulierung. Man kann diese Aussage auch invertieren und zu folgendem Schluss kommen: "\textit{Die reichsten (100 - X) \% der Merkmalsträger besitzen einen Anteil von (100 - Y) \% der Merkmalssumme.}"\\
All diese beispielhaften Formulierungen zeigen, dass es essentiell ist die Daten geordnet vorliegen zu haben, bevor man die Punkte zur Erstellung der Lorenzkurve berechnen kann. Bei der Berechnung werden im Folgenden zwei Fälle unterschieden: Individualdaten (der "normale" Fall) und gruppierte Daten (der etwas "kompliziertere" Fall).

\paragraph{Berechnung Lorenzkurve bei Individualdaten}
Da jeder Merkmalsträger jeweils einen Anteil von $\frac{1}{n}$ an der Gesamtheit der Merkmalsträger ausmacht, teilt man die x-Achse in $n$ gleichgroße Abschnitte. Liegen z.B. 5 Beobachtungen vor, so wird die x-Achse durch Markierungen bei $\frac{1}{5}$, $\frac{2}{5}$, $\frac{3}{5}$ \& $\frac{4}{5}$ unterteilt. Der zugehörige Wert der Ordinate gibt für jeden dieser x-Werte den kumulierten Anteil der Merkmalssumme dieses Anteils der Merkmalsträger an. Nehmen wir also an, unsere 5 Beobachtungen hätten folgende Ausprägungen:
$$x_1 = 4;\qquad x_2 = 1;\qquad x_3 = 7;\qquad x_4 = 5;\qquad x_5 = 3$$

\noindent Die geordneten Daten sähen in diesem Fall so aus:
$$x_{(1)} = 1;\qquad x_{(2)} = 3;\qquad x_{(3)} = 4;\qquad x_{(4)} = 5;\qquad x_{(5)} = 7$$

\noindent Da wir eine gesamte Merkmalssumme von 20 hätten, hätte die erste Beobachtung daran einen kumulierten Anteil von $\frac{1}{20}$, die ersten beiden hätten einen kumulierten Anteil von $\frac{1+3}{20} = \frac{4}{20}$, die ersten drei hätten einen kumulierten Anteil von $\frac{1+3+4}{20} = \frac{8}{20}$ und die ersten vier hätten einen kumulierten Anteil von $\frac{1+3+4+5}{20} = \frac{13}{20}$.\\
Somit ergeben sich folgende Punkte für die Lorenzkurve.
$$(\frac{1}{5}|\frac{1}{20});\qquad (\frac{2}{5}|\frac{4}{20});\qquad (\frac{3}{5}|\frac{8}{20});\qquad (\frac{4}{5}|\frac{13}{20})$$

\noindent Außerdem gehören zu \textbf{jeder} Lorenzkurve die Punkte (0|0) und (1|1), da logischerweise 0\% der Merkmalsträger auch 0\% der Merkmalsumme besitzen und 100\% der Merkmalsträger auch 100\% der Merkmalsumme besitzen.\\

\noindent \dangersign[3ex] \textit{Notation:} Die Werte Werte auf der x-Achse werden oft als $u_i$ und die Werte auf der y-Achse als $v_i$ bezeichnet. Dies ist für die Formeln in Kapitel \ref{sec:gini} wichtig.\\

\noindent Für die Interpretation ist es wichtig sich klar zu machen, wie eine perfekt Gleichverteilung der Merkmalssumme sich in der Lorenzkurve widerspiegeln würde. In unserem Beispiel wäre in diesem Fall jedes $x_i = 4$ (gesamte Merkmalssumme von 20 aufgeteilt auf 5 Merkmalsträger) und die Punkte für die Lorenzkurve wären:
$$(0|0);\qquad(\frac{1}{5}|\frac{1}{5});\qquad (\frac{2}{5}|\frac{2}{5});\qquad (\frac{3}{5}|\frac{3}{5});\qquad (\frac{4}{5}|\frac{4}{5});\qquad (1|1)$$

\noindent In diesem Fall würde die Lorenzkurve perfekt mit der Winkelhalbierenden übereinstimmen, was auch der Grund ist, warum die Winkelhalbierende oft mit in die Grafik eingezeichnet ist. Sie dient quasi als Referenz, wie es im Fall der Gleichverteilung aussähe um abschätzen zu können wie stark die Lorenzkurve davon abweicht.\\
Das andere Extrem, ein Monopol, läge vor falls die gesamte Merkmalssumme einem Merkmalsträger zugeordnet würde. In unserem Beispiel wären dann $x_{(1)} = \hdots = x_{(4)} = 0$ und $x_{(5)} = 20$, was zu folgenden Punkten für die Lorenzkurve führen würde:
$$(0|0);\qquad(\frac{1}{5}|0);\qquad (\frac{2}{5}|0);\qquad (\frac{3}{5}|0);\qquad (\frac{4}{5}|0);\qquad (1|1)$$

\noindent In Abbildung \ref{fig:lk1} sind die Lorenzkurven für die drei Beispielszenarien dargestellt.
\begin{figure}[htbp]
    \centering
\includegraphics[width = .9\textwidth]{figures/example_lc.pdf}
    \caption{Lorenzkurven zum Beispiel für Individualdaten}
    \label{fig:lk1}
\end{figure}

\noindent Zwischen diesen beiden Extrema gibt es sehr viele Abstufungen von (Un)Gleichverteilung.
Generell lässt sich festhalten: Je weiter der Graph der Lorenzkurve (nach links unten) von der Winkelhalbierende entfernt ist, also je größer die Fläche dazwischen ist, desto ungleicher verteilt, also desto konzentrierter ist ein Merkmal. Vice versa, je näher an der Winkelhalbierenden, desto gleichmäßiger verteilt, als desto weniger konzentriert ist ein Merkmal. Diese angesprochene Fläche spielt auch beim Gini-Koeffizienten (vgl. Kap. \ref{sec:gini}) eine entscheidende Rolle.\\
Weitere Eigenschaften der Lorenzkurve sind, dass sie \textbf{immer} unterhalb der Winkelhalbierenden und niemals darüber verlaufen muss. Da die Merkmalsausprägungen kumuliert (also aufsummiert) werden, kann der Graph nur monoton steigend sein. Zudem muss die Steigung eines Kurvensegments immer größer oder gleich dem vorigen Segments sein, da die Merkmalsausprägungen bei der Lorenzkurve nach Größe geordnet wurden.

\paragraph{Berechnung Lorenzkurve bei gruppierten Daten}
Hat man obige Erklärungen für Individuldaten verstanden, so wird auch das Verständnis für das Vorgehen bei gruppierten Daten nicht schwer fallen. Der erste wichtige, und visuell auffälligste, Unterschied besteht darin, dass die Abstände auf der x-Achse nicht mehr identisch ist. Ansonsten sind die Berechnungen weitestgehend ähnlich zum dem Fall für Individualdaten.\\
Nehmen wir an wir hätten im obigen Beispiel nun nicht mehr 5 Beobachtungen sondern 100. Dabei haben 10 Beobachtungen eine Merkmalsausprägung von 1, 40 haben eine Merkmalsausprägung von 5, 20 eine Merkmalsausprägung von 7 und 30 eine Merkmalsausprägung von 15. Insgesamt entspricht dies einer Merkmalssumme von 800. Die Gruppe mit der geringsten Merkmalsausprägung hätte damit einen Anteil von $\frac{10}{100} = 10\%$ an den Merkmalsträgern und einen Anteil von $\frac{10 \cdot 1}{800} = 0,0125$ an der Merkmalssumme, die beiden Gruppen mit den geringsten  Merkmalsausprägungen einen Anteil von $\frac{10 + 40}{100} = 50\%$ an den Merkmalsträgern und einen Anteil von $\frac{10 \cdot 1 + 40 \cdot 5}{800} = 0,2625$ an der Merkmalssumme, usw.\\Dies führt zu folgenden Punkten für die Lorenzkurve:
$$(0|0);\qquad(0,1|0,0125);\qquad (0,5|0,2625);\qquad (0,7|0,4375);\qquad (1|1)$$

\noindent Abbildung \ref{fig:lk2} zeigt die Lorenzkurve für dieses Beispielszenario.
\begin{figure}[htbp]
    \centering
\includegraphics[width = .7\textwidth]{figures/example_lc2.pdf}
    \caption{Lorenzkurve zum Beispiel für gruppierte Daten}
    \label{fig:lk2}
\end{figure}

\noindent \dangersign[3ex] \textit{Notation:} Um Formeln für gruppierte von Formeln für Individualdaten abzuheben wird eine Tilde verwendet, d.h. x-Werte als $\tilde u_i$ und y-Werte als $\tilde v_i$ bezeichnet.\\
\noindent \dangersign[3ex] \textit{Anmerkung I:} Die gleiche Lorenzkurve wie in Abbildung \ref{fig:lk2} hätte man auch für die Individualdaten zeichnen können, jedoch wäre dies ein um einiges höherer Aufwand gewesen. In diesem Fall wäre es jedoch möglich gewesen, da wir tatsächlich für jedes Individuum dessen genaue Merkmalsausprägung kennen.\\
\noindent \dangersign[3ex] \textit{Anmerkung II:} Kennen wir \textbf{nicht} für jedes Individuum dessen genaue Merkmalsausprägung, sondern lediglich einen Gruppenmittelwert, so wird jedem Inidividuum in einer Gruppe diese Gruppenmittelwert als Merkmalsausprägung zugeordnet. Dadurch kann man ganz normal, wie oben für gruppierte Daten gezeigt, vorgehen. Wichtig ist dabei jedoch im Hinterkopf zu behalten, dass damit implizit die Annahme einhergeht, dass innerhalb der Gruppen Gleichverteilung herrscht, da wir jedem Individuum einer Gruppe denselben Wert zuordnen. Diese Annahme muss nicht immer realistisch sein und sollte stets kritisch hinterfragt werden.\\

\noindent \framebox[\textwidth]{\texttt{R-Befehl für die Lorenzkurve: > Lc(data)} \hfill \href{https://www.rdocumentation.org/packages/ineq/versions/0.2-13/topics/Lc}{Dokumentation}}

\noindent \dangersign[3ex] Die R-Funktion ist \textbf{nicht} Teil von \texttt{base-R} sondern Teil des Paketes \texttt{ineq}. Um diese Funktion verwenden zu können, muss das Paket zunächst installiert werden (\texttt{install.packages("ineq")}) und anschließend importiert werden (\texttt{library(ineq)}).

\clearpage

\subsubsection{Gini-Koeffizient}\label{sec:gini}
Die im vorigen Kapitel erwähnte Fläche $F$, die zwischen dem Graph und der Winkelhalbierenden liegt, stellt die Basis für den Gini-Koeffizienten dar, welcher ein Maß für die relative Konzentration ist. Der Gini ist definiert als "\textit{Zweimal die Fläche zwischen Winkelhalbierender und Lorenzkurve.}"\\

\noindent Kennt man bereits die kumulierten Anteile an der Merkmalssumme (y-Werte der Punkte auf der Lorenzkurve), so ist der Gini recht einfach zu berechnen:
\begin{itemize}
    \item Znächst addiert man jeweils zu jedem Anteilswert den Anteilswert der vorherigen Punktes (angefangen bei 0 bis zur 1): $(v_{i-1}+v_i)$
    \item Diese Summen werden anschließend addiert: $\sum_{i=1}^n (v_{i-1}+v_i)$
    \item und mit $\frac{1}{n}$ multipliziert: $\frac{1}{n}\sum_{i=1}^n (v_{i-1}+v_i)$
    \item Dieses Produkt wird zum Schluss von 1 abgezogen.
\end{itemize}
\begin{align*}
    G=1-\frac{1}{n}\sum_{i=1}^n n_j \cdot (v_{i-1} + v_i)
\end{align*}
Für gruppierte Daten ändern sich Vorgehen und Formel nicht dramatisch. Der einzige Unterschied zur obigen Formel besteht darin, dass man mit den Gruppengrößen $n_j$ gewichten muss:

\begin{align*}
    G=1-\frac{1}{n}\sum_{i=1}^n (\tilde v_{i-1} + \tilde v_i)
\end{align*}

\noindent Man kann sich das ein bisschen wie bei gewichteten arithmetischen Mittel in Kapitel \ref{sec:arithm} vorstellen, da auch dort Unterschiedlichen Gruppengrößen auf ähnliche Art \& Weise Rechnung getragen wird.\\

\noindent Der Wertebereich des Gini-Koeffizient beginnt bei 0, was für eine absolute Gleichverteilung spricht. Dies macht intuitiv Sinn, da bei der absoluten Gleichverteilung die Fläche zwischen Winkelhalbierender und Lorenzkurve nicht existiert. Dass die obere Grenze nicht bei Eins, sondern bei ($\frac{n-1}{n}$, ist auf den ersten Blick vielleicht etwas weniger intuitiv. Bei einem Blick auf das Monopol-Szenario in Abbildung \ref{fig:lk1} sollte jedoch klar werden, dass die Fläche zwischen Winkelhalbierender und Lorenzkurve nicht den Wert 0,5 erreichen kann \& somit der Gini (also das Doppelte dieser Fläche) nicht 1 werden kann. Grund dafür ist das Dreieck, welches unten rechts stets per Konstruktion ausgespart wird. Die Größe dieses ausgesparten Dreieckes hängt von der Anzahl der Merkmalsträger $n$ ab und damit auch der Wertebereich.\\

\noindent Da mit einem variierendem Wertebereich (je nach Anzahl der Merkmalsträger) schwierig Konzentrationen für verschiedene Merkmale verglichen werden können, berechnet man den \textbf{normierten Gini-Koeffizienten $G^+$}.\\
Der normierte Gini-Koeffizient erhält man, indem man den berechneten Gini-Koeffizient mit $\frac{n}{n-1}$ multipliziert:
\begin{align*}
    G^+=\frac{n}{n-1}G
\end{align*}
Der Wertebereich von $G^+$ geht dann noch von 0 bis 1 und ist unabhängig von $n$. Die Konzentration 0 steht dabei für absolute Gleichverteilung, also dafür, dass es \textit{keine} Konzentration gibt, während 1 für eine \textit{vollständige} Konzentration, also für ein Monopol, steht. Somit ist es nun kein Problem mehr verschiedene Merkmale mit unterschiedlichem $n$ in Bezug auf ihre Konzentration zu vergleichen.\\

\noindent \dangersign[3ex] Bei der Normierung des Gini für gruppierte Daten entspricht $n$ weiterhin der Anzahl der Beobachtungen und \textbf{nicht} der Anzahl der Gruppen.\\

\noindent \textbf{Einschub: Aussagen über Veränderungen}\\
Beim Gini kann man relativ einfach pauschale Aussagen über Veränderungen treffen, solange die Anzahl der Merkmalträger gleich bleibt.
\begin{itemize}
    \item \textit{Beispiel 1: Alle Merkmalsträger erfahren dieselbe relative Steigerung}\\
    Alle Merkmalsträger steigern ihre Merkmalssumme um 10\%. In diesem Fall würde sich Gini nicht verändern, da sich an den Relationen nichts gändert hat.
    \item \textit{Beispiel 2: Alle Merkmalsträger erfahren dieselbe absolute Steigerung}\\
    Alle Merkmalsträger steigern ihre Merkmalssumme um 10 Einheiten. In diesem Fall würde Gini nicht sinken, da in Relationen zueinander nun alle etwas gleichere Anteile besitzen. Man kann sich das gut an einem Extremfall veranschaulichen: Angenommen jeder Merkmalsträger würde seine Merkmalssumme um das 100-fache der bisher größten Ausprägung steigern. Dadurch wurden alle bisher dagewesenen Unterschiede quasi irrelevant werden und jeder hätte nahezu gleich viel. 
\end{itemize}

\noindent \framebox[\textwidth]{\texttt{R-Befehl für den Gini: > Gini(data)} \hfill \href{https://www.rdocumentation.org/packages/ineq/versions/0.2-13/topics/ineq}{Dokumentation}}

\noindent \dangersign[3ex] Die R-Funktion ist \textbf{nicht} Teil von \texttt{base-R} sondern Teil des Paketes \texttt{ineq}. Um diese Funktion verwenden zu können, muss das Paket zunächst installiert werden (\texttt{install.packages("ineq")}) und anschließend importiert werden (\texttt{library(ineq)}).

\clearpage

\subsection{Aufgaben}

\paragraph{1. Welche Aussagen bzgl. Gini \& Lorenzkurve sind wahr?}

\begin{itemize}
    \item[a)] Die absolute Merkmalssumme ist unerheblich für den Gini. \hfill $\square$
    \item[b)] Höherer Gini bedeutet (global) steilere Lorenzkurve. \hfill $\square$
    \item[c)] Der Gini ist uneingeschränkt geeignet um die Konzentration in zwei Gruppen\\zu vergleichen. \hfill $\square$
    \item[d)] Erhalten alle Merkmalsträger dieselbe prozentuale Steigerung ihres (absoluten)\\Teils der Merkmalssumme, so verändert sich der Gini nicht. \hfill $\square$
\end{itemize}

\paragraph{2. Welche Aussagen bzgl. des Herfindahl-Index sind wahr?}

\begin{itemize}
    \item[a)] Der Herfindahl-Index ist uneingeschränkt geeignet um die Konzentration in zwei\\Gruppen zu vergleichen. \hfill $\square$
    \item[b)] Falls sich die Merkmalssumme ändert, können definitive Aussagen über der Änderung\\des Herfindahl-Index getroffen werden. \hfill $\square$
    \item[c)] Falls sich die Verteilung Merkmalssumme ändert, können definitive Aussagen über der\\Änderung des Herfindahl-Index getroffen werden. \hfill $\square$
    \item[d)] Höherer Herfindahl-Index bedeutet ungleichere Verteilung. \hfill $\square$
\end{itemize}

\paragraph{3. Der Gini für gruppierte Daten ist nur identisch zum "normalen" Gini, falls ..}

\begin{itemize}
    \item[a)] .. alle Gruppen gleich groß sind. \hfill $\square$
    \item[b)] .. absolute Gleichverteilung herrscht. \hfill $\square$
    \item[c)] .. Gleichverteilung innerhalb der Gruppen herrscht. \hfill $\square$
    \item[d)] .. die Anzahl der Gruppen kleiner als 10 ist. \hfill $\square$
\end{itemize}

\newpage

\section{Zusammenhangsmaße}
Bis jetzt haben wir stets lediglich eine Variable (bzw. Merkmal) $X$ und dessen Ausprägungen $x_1, x_2,.., x_i$ betrachtet. Auf diese Art und Weise war es uns möglich, Aussagen über dessen Lage, Streuung und Konzentration zu treffen.\\
Da in diesem Kapitel Aussagen über Zusammenhänge getroffen werden sollen bzw. Zusammenhänge quantifiziert werden sollen, werden nun stets \textbf{zwei} Variablen/Merkmale $X$ \& $Y$ gleichzeitig betrachtet. Dies führt dazu, dass die beobachteten Ausprägungen der Merkmale nun folgendermaßen vorliegen: $(x_1; y_1), (x_2; y_2), .., (x_n; y_n)$

\noindent \paragraph{Beispiel:} Wir betrachten nun bei Personen gleichzeitig ihre Körper- ($X$) und ihre Schuhgröße ($Y$). Dies könnte bspw. zu folgender Stichprobe führen: $(185cm;\; 44), (170cm;\; 39), .., (195cm;\; 47)$\\

\noindent Von zentraler Bedeutung für die Quantifizierung von Zusammenhängen ist es, sich im Vorhinein darüber klar zu werden, auf welchen Skalenniveaus die beobachteten Merkmale gemessen werden. Dies hat erhebliche Auswirkungen darauf, welche Zusammenhangsmaße berechnet werden können. Als Richtlinie gilt hier stets: Es kann lediglich ein Zusammenhangsmaß berechnet werden, welches für die Skala desjenigen Merkmals geeignet ist, welches auf der "niedrigeren" Skala gemessen wird.

\noindent \paragraph{Beispiel:} Betrachten wir bei Personen ihr Geschlecht ($X$) und ihr Einkommen ($Y$), so sind nur Zusammenhangsmaße anwendbar, die für nominale Merkmale geeignet sind, da das Merkmal Geschlecht lediglich nominal skaliert ist. Berechnet man stattdessen Betriebszugehörigkeit ($X$) und Einkommen ($Y$), so sind Zusammenhangsmaße für metrische Merkmale anwendbar.

\subsection{Die Kontingenztafel}
Eines der grundlegendsten Instrumente zur gemeinsamen Darstellung zweier Merkmale ist die Kontingenztafel.
Hierbei wird pro unterschiedliche Merkmalsausprägung von $X$ eine Zeile, für jede untersch. Merkmalsausprägung von $Y$ eine Spalte belegt. Dies führt bei der Betrachtung der Merkmale $X$: "Parteizugehörigkeit im Bundestag" (Ausprägungen: \textit{Union, SPD, Grüne, FDP, Linke, AfD}) und $Y$: Geschlecht (Ausprägungen: \textit{weiblich, männlich, divers}) zu einer Kontingenztafel mit 6 Zeilen und 3 Spalten, d.h. einer $(6\times3)$-Kontingenztafel.\\
Die Entscheidung, welche Variable in den Zeilen und welche Variable in den Spalten abgetragen wird, kann mehr oder minder frei getroffen werden. Oftmals wird die Auswahl jedoch nach zwei Kriterien getroffen:
\begin{itemize}
    \item Kann ein betrachtetes Merkmal als "Schichtungsmerkmal" betrachtet werden, so steht es für gewöhnlich in den Zeilen. Ein Schichtungsmerkmal ist eine Variable, die die Stichprobe in verschiedene Gruppen einteilt, wie z.B. "Geschlecht", "Bildungsabschluss" oder "Raucher/Nicht-Raucher".
    \item Vermutet man in den Daten eine kausale Wirkungsstruktur (d.h. ein Merkmal hat einen Einfluss auf das Andere), so steht meist dasjenige Merkmal in den Zeilen, welches als Ursache angesehen wird, z.B.
    bei der Betrachtung von "Raucher/Nicht-Raucher" und "Auftreten von Lungenkrebs ja/nein" würde man das Merkmal "Raucher/Nicht-Raucher" in den Zeilen abtragen.
\end{itemize}
Da in der Kontingenztafel für jede mögliche Ausprägung eines Merkmals eine zusätzliche Zeile bzw. Spalte in der Darstellung benötigt wird, ist sie lediglich für Merkmale mit nicht allzu vielen verschiedenen Ausprägungen sinnvoll. Für metrische Merkmale bedeutet dies, dass diese nur sinnvoll in Kontingenztafeln dargestellt werden können, falls sie vorher in Klassen eingeteilt wurden (bspw. Altersgruppen oder Einkommensklassen).\\
Die inneren Zellen der Kontingenztafeln werden in der Regel mit den absoluten Häufigkeiten $n_{ij}$ ("Wie oft beobachte ich die Ausprägung in der i-ten Zeile von Merkmal $X$ gemeinsam mit der Ausprägung in der j-ten Spalte vom Merkmal $Y$?") oder den relativen Häufigkeiten $f_{ij} = \frac{n_{ij}}{n}$ befüllt.\\
Die (absoluten) Randhäufigkeiten werden mit $n_{i\bullet}$ ("Wie oft beobachte ich die Ausprägung in der i-ten Zeile von Merkmal $X$?") und $n_{\bullet j}$ ("Wie oft beobachte ich die Ausprägung in der j-ten Spalte von Merkmal $Y$?") bezeichnet.\\
Teilt man alle Häufigkeiten der inneren Zellen in der i-ten Zeile durch die Randhäufigkeit der i-ten Zeile, so erhält man die bedingten relativen Häufigkeiten $f_{j|i} = \frac{n_{ij}}{n_{i\bullet}}$. Analog lassen sich auch die bedingen relativen Häufigkeiten $f_{i|j} = \frac{n_{ij}}{n_{\bullet j}}$ für die j-te Spalte bestimmen.

\noindent \paragraph{Beispiel:} \textit{Lungenkrebs bei weiblichen Rauchern}\footnote{Quelle: \tiny{\url{https://www.aerzteblatt.de/archiv/8369/Lungenkrebsrisiko-hoeher-als-bisher-angenommen-Europaweite-Studie-vorgestellt}}}\\

\begin{table}[htbp]
\begin{tabular}{c|cc|c}
    $X$/$Y$         & Lungenkrebs & kein Lungenkrebs & $\sum$ \\
     \hline
    Nicht-Raucher   & 458         & 1598             & 2056 \\
    Raucher         & 877         & 412              & 1289 \\
    \hline
    $\sum$          & 1335        & 2010             & 3345
\end{tabular}
    \caption{\textit{Absolute Häufigkeiten (Lungenkrebsinzidenz bei weiblichen Rauchern)}}
    \label{tab:abs}
\end{table}

\begin{table}[htbp]
\begin{tabular}{c|cc|c}
    $X$/$Y$         & Lungenkrebs & kein Lungenkrebs & $\sum$ \\
     \hline
    Nicht-Raucher   & $\frac{458}{3345}$         & $\frac{1598}{3345}$             & $\frac{2056}{3345}$ \\
    Raucher         & $\frac{877}{3345}$         & $\frac{412}{3345}$              & $\frac{1289}{3345}$ \\
    \hline
    $\sum$          & $\frac{1335}{3345}$        & $\frac{2010}{3345}$             & 1
\end{tabular}
    \caption{\textit{Relative Häufigkeiten (Lungenkrebsinzidenz bei weiblichen Rauchern)}}
    \label{tab:rel}
\end{table}

\noindent \framebox[\textwidth]{\texttt{R-Befehl für Kontingenztafeln: > table(data\$x, data\$y)} \hfill \href{https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/table}{Dokumentation}}

\clearpage

\begin{table}[htbp]
\begin{tabular}{c|cc|c}
    $X$/$Y$         & Lungenkrebs & kein Lungenkrebs & $\sum$ \\
     \hline
    Nicht-Raucher   & $\frac{458}{2056}$         & $\frac{1598}{2056}$             & 1 \\
    Raucher         & $\frac{877}{1289}$         & $\frac{412}{1289}$              & 1 \\
    \hline
\end{tabular}
    \caption{\textit{Bedingt auf Raucher/Nicht-Raucher (Lungenkrebsinzidenz bei weiblichen Rauchern)}}
    \label{tab:bed1}
\end{table}

\begin{table}[htbp!]
\begin{tabular}{c|cc|}
    $X$/$Y$         & Lungenkrebs & kein Lungenkrebs \\
     \hline
    Nicht-Raucher   & $\frac{458}{1335}$         & $\frac{1598}{2010}$   \\      
    Raucher         & $\frac{877}{1335}$         & $\frac{412}{2010}$    \\      
    \hline
    $\sum$          & 1         & 1           
\end{tabular}
    \caption{\textit{Bedingt auf Lungenkrebs-Status (Lungenkrebsinzidenz bei weiblichen Rauchern)}}
    \label{tab:bed2}
\end{table}

\noindent \framebox[\textwidth]{\texttt{R-Befehl für (bedingte) Kontingenztafeln: > prop.table(data)} \hfill \href{https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/prop.table}{Dokumentation}}

\subsection{Unabhängigkeit}
Ein zentrales Konzept bei der Betrachtung von Zusammenhängen ist die \textit{Unabhängigkeit}.\\
Betrachtet man in Tabelle \ref{tab:abs} die Randhäufigkeiten $n_{\bullet j}$ der Spalten, so erkennt man das Verhältnis von Lungenkrebs zu Nicht-Lungenkrebs, unabhängig davon ob jemand geraucht hat oder nicht. Dieses Verhältnis liegt bei etwas mehr als 2 zu 3. Betrachtet man nun lediglich die Häufigkeiten in der ersten Zeile (d.h. nur die Nicht-Raucher) $n_{11}$ und $n_{12}$, so erkennt man, dass unter diesen Beobachtungen das Verhältnis von Lungenkrebs zu Nicht-Lungenkrebs bei ca. 1 zu 4 liegt (bei den Nicht-Lungenkrebs-Fällen liegt es bei etwas mehr als 2 zu 1). Somit scheint es einen Zusammenhang zwischen Rauchen und dem Auftreten von Lungenkrebs zu geben, da man unter Unabhängigkeit erwarten würde, in beiden Gruppen (Nicht-Raucher und Raucher) dasselbe Verhältnis, nämlich das der Randhäufigkeiten zu beobachten. Berechenbar sind die \textit{unter Unabhängigkeit erwarteten absoluten Häufigkeiten} über die Formel $\hat{n_{ij}} = \frac{n_{i\bullet} \cdot n_{\bullet j}}{n}$.

\noindent \paragraph{Fortsetzung Beispiel:} \textit{Lungenkrebs bei weiblichen Rauchern}
\begin{table}[htbp]
\begin{tabular}{c|cc|c}
    $X$/$Y$         & Lungenkrebs & kein Lungenkrebs & $\sum$ \\
     \hline
    Nicht-Raucher   & $\frac{2056\cdot1335}{3345} = 820,5561$         & $\frac{2056\cdot2010}{3345} = 1235,4439$             & 2056 \\
    Raucher         & $\frac{1289\cdot1335}{3345} = 514,4439$         & $\frac{1289\cdot2010}{3345} = 774,5561$             & 1289 \\
    \hline
    $\sum$          & 1335        & 2010             & 3345
\end{tabular}
    \caption{\textit{Unter Unabhängigkeit erwartete absolute Häufigkeiten}}
    \label{tab:abs-unabh}
\end{table}

\noindent Betrachtet man in dieser Häufigkeitstabelle nun die Verhältnisse von Lungenkrebs zu Nicht-Lungen\-krebs bei den Rauchern bzw. bei den Nicht-Rauchern, so erkennt man, dass das Verhältnis in beiden Gruppen jeweils bei ca. 2 zu 3 liegt und somit dem Verhältnis der Randhäufigkeiten der Spalten entspricht.

\clearpage

\subsection{Zusammenhangsmaße für nominale Merkmale}\label{sec:zshg-nom}
Dadurch, dass einfach berechenbar ist, wie eine Häufigkeitstabelle unter Unabhängigkeit der beiden Merkmale aussieht (vgl. Tabelle \ref{tab:abs-unabh}), kann man nun messen wie stark die tatsächlich beobachtete Häufigkeitstabelle davon abweicht. Um diese beobachteten Abweichungen in eine einzige Maßzahl zusammenzufassen, geht man wie folgt vor:
\begin{itemize}
    \item Man bildet für jede Zelle die Differenz aus beobachteter absoluter Häufigkeit und unter Unabhängigkeit erwarteter absoluter Häufigkeit: $n_{ij} - \hat{n_{ij}}$ 
    \item Man quadriert diese Differenzen, damit sich negative und positive Abweichungen später beim Aufsummieren nicht gegenseitig aufheben: $(n_{ij} - \hat{n_{ij}})^2$ 
    \item Man teilt diese quadrierten Differenzen durch die $\hat{n_{ij}}$, d.h. man setzt sie ins Verhältnis zu dem Wert, den man eigentlich erwartet hätte: $\frac{(n_{ij} - \hat{n_{ij}})^2}{\hat{n_{ij}}}$
    \item Man summiert diese Werte der einzelnen Zellen auf: $\sum_{i=1}^k\sum_{j=1}^l \frac{(n_{ij} - \hat{n_{ij}})^2}{\hat{n_{ij}}}$
\end{itemize}
Die Maßzahl, welche durch diese Vorgehensweise berechnet wird, nennt sich $\chi^2$-Koeffizient und nimmt bei Unabhängigkeit (d.h. unter Unabhängigkeit erwartete sind gleich den beobachteten absoluten Häufigkeiten) den Wert 0 an, bei Abhängigkeit hingegen sehr hohe Werte. Diese Maßzahl besitzt jedoch noch einige gravierende Nachteile:
\begin{itemize}
    \item Je mehr Beobachtungen wir in unserer Stichprobe haben, desto leichter sind hohe Abweichungen zu beobachten, d.h. desto größer wird \textit{tendenziell} $\chi^2$
    \item Je mehr Zellen die Kontingenztafel besitzt, desto mehr Abweichungen sind zu beobachten, d.h. desto größer wird \textit{tendenziell} $\chi^2$
\end{itemize}
Um diesen Nachteilen entgegenzutreten existieren verschiedene Erweiterungen, welche eine (oder beide) dieser Einschränkungen beheben.\\

\noindent \framebox[\textwidth]{\texttt{R-Befehl für $\chi^2$: > chisq.test(data\$x, data\$y)} \hfill \href{https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/chisq.test}{Dokumentation}}

\noindent \dangersign[3ex] Diese R-Funktion führt neben der Berechnung von $\chi^2$ auch den zugehörigen $\chi^2$-Unabhängigkeits\-test durch. Dies ist erst Teil des Stoffs von Statistik II (vgl. Kap. \ref{sec:chitest2}) und kann vorläufig ignoriert werden. Der Wert \texttt{X-squared} in der Ausgabe entspricht dem $\chi^2$-Koeffizienten.

\paragraph{Loslösung vom Stichprobenumfang}
Der $\Phi$-Koeffizient und der Kontingenzkoeffizient $C$ sind zwei Möglichkeiten um den Wertebereich von $\chi^2$ vom Stichprobenumfang zu lösen. Sie berechnen sich auf folgende, leicht unterschiedliche Weisen
\begin{align*}
    \Phi &= \sqrt{\frac{\chi^2}{n}}, \qquad 0 \leq \Phi \leq \sqrt{min(k,l)-1}\\
    C &= \sqrt{\frac{\chi^2}{\chi^2 + n}}, \qquad 0 \leq C \leq \sqrt{\frac{min(k,l)-1}{min(k,l)}}
\end{align*}

\noindent \dangersign[3ex] \textbf{Wichtig:} Die Zeilen- und Spaltenzahl werden in diesem Kontext mit $k$ \& $l$ bezeichnet.\\

\noindent Eine Berechnung diese beiden Maßzahlen macht Sinn, wenn wir die Zusammenhänge aus zwei Kontingenztafeln vergleichen wollen, bei denen zwar der Stichprobenumfang, nicht aber jedoch die Dimensionen der Kontingenztafel unterschiedlich sind. Beispielsweise könnte man den berechneten Zusammenhang aus Tabelle \ref{tab:abs} mit einer Gruppe von männlichen Rauchern vergleichen. Hier wären dann die Dimensionen der beiden Tafeln identisch, die Stichprobenumfänge könnten sich jedoch unterscheiden.\\

\noindent \framebox[\textwidth]{\texttt{R-Befehl für den $\Phi$-Koeffizient: > Phi(data\$x, data\$y)} \hfill \href{https://www.rdocumentation.org/packages/DescTools/versions/0.99.37/topics/Association\%20measures}{Dokumentation}}\\
\noindent \framebox[\textwidth]{\texttt{R-Befehl für den $C$: > ContCoef(data\$x, data\$y)}\\ \mbox{} \hfill \href{https://www.rdocumentation.org/packages/DescTools/versions/0.99.37/topics/Association\%20measures}{Dokumentation}}

\noindent \dangersign[3ex] Die R-Funktionen sind \textbf{nicht} Teil von \texttt{base-R} sondern Teil des Paketes \texttt{DescTools}. Dieses muss zunächst installiert (\texttt{install.packages("DescTools")}) und anschließend importiert werden (\texttt{library(DescTools)}).

\paragraph{Loslösung von der Dimension der Kontingenztafel}
Möchte man diese beiden Maße nun auch noch von der Dimension der Kontingenztafel loslösen, so kann man dies durch Berechnung von $Cramers\; V$ oder des korrigierten Kontingenzkoeffizienten $C_{korr}$ tun:
\begin{align*}
    V &= \sqrt{\frac{\chi^2}{n\cdot(min(k,l)-1)}}, \qquad 0 \leq V \leq 1\\
    C_{korr} &= \sqrt{\frac{min(k,l)}{min(k,l)-1}}\cdot\underbrace{\sqrt{\frac{\chi^2}{\chi^2 + n}}}_{C}, \qquad 0 \leq C_{korr} \leq 1
\end{align*}

\noindent Eine Berechnung diese beiden Maßzahlen macht Sinn, wenn wir die Zusammenhänge aus zwei Kontingenztafeln vergleichen wollen, bei denen sowohl der Stichprobenumfang, als auch die Dimensionen der Kontingenztafel unterschiedlich sind. Beispielsweise könnte man den berechneten Zusammenhang aus Tabelle \ref{tab:abs} mit einer Gruppe von männlichen Rauchern vergleichen, bei denen nicht nur die Lungenkrebsinzidenz sondern auch das Auftreten anderer Krebsarten betrachtet wird. Hier wären dann auch die Dimensionen der beiden Tafeln unterschiedliche, da die Kontingenztafel der Männer mehr Spalten besäße als die der Frauen.\\

\noindent  \framebox[\textwidth]{\texttt{R-Befehl für den Cramers V: > CramerV(data\$x, data\$y)} \hfill \href{https://www.rdocumentation.org/packages/DescTools/versions/0.99.37/topics/Association\%20measures}{Dokumentation}}\\
\noindent  \framebox[\textwidth]{\texttt{R-Befehl für $C_{korr}$: > ContCoef(.., correct = TRUE)} \hfill \href{https://www.rdocumentation.org/packages/DescTools/versions/0.99.37/topics/Association\%20measures}{Dokumentation}}

\noindent \dangersign[3ex] Die R-Funktionen sind \textbf{nicht} Teil von \texttt{base-R} sondern Teil des Paketes \texttt{DescTools}. Dieses muss zunächst installiert (\texttt{install.packages("DescTools")}) und anschließend importiert werden (\texttt{library(DescTools)}).

\clearpage

\subsection{Odds Ratio}
Der Odds-Ratio ist ein Maß, welche speziell für Vier-Felder-Tafeln definiert ist. Durch Zwischenschritte über das (relative) Risiko bis letztendlich hin zum Odds Ratio lässt er sich gut nachvoll\-ziehbar erklären.

\begin{table}[htbp]
\centering
\begin{tabular}{c|cc|c}
&$y_1$&$y_2$&$\sum$\\
\hline
$x_1$ & a&b&a+b\\
$x_2$&c&d&c+d\\
\hline
$\sum$&a+c&b+d&n
\end{tabular}
    \caption{\textit{Allgemeine Darstellung einer $2\times2$-Kontingenztafel (Vier-Felder-Tafel)}}
    \label{tab:2k2}
\end{table}

\paragraph{Risiko} Betracht man zunächst ein \textit{Risikomerkmal}, so kann man für dieses Risikomerkmal die Wahrscheinlichkeit bestimmen, dass es eintritt. Diese Wahrscheinlichkeit wird als Risiko bezeichnet. So könnte man bspw. für Tabelle \ref{tab:abs} das Risiko bestimmen, dass eine Frau an Lungenkrebs erkrankt: Entweder für die gesamte Population ($\frac{1335}{3345}$) oder für eine bestimmte "Schicht" (z.B. für die Raucher: $\frac{877}{1289}$). Im Folgenden werden wir vom Geschlecht als \textit{Schichtungsmerkmal} und vom Lungenkrebs als \textit{Risikomerkmal} sprechen.

\paragraph{Relatives Risiko} Möchte man nun die Risiken in zwei verschiedenen Schichten miteinander vergleichen, so bietet sich das relative Risiko als Maßzahl an. Hierbei dividiert man die Risiken zweier Schichten durcheinander, z.B. das Risiko für Lungenkrebs bei den Rauchern und das Risiko für Lungenkrebs bei den Nicht-Rauchern:
\begin{align*}
    RR_{Lungenkrebs} = \dfrac{\frac{877}{1289}}{\frac{458}{2056}} = \dfrac{0,68}{0,22} = 3,09
\end{align*}
Dieser Wert bedeutet, dass das Risiko für Lungenkrebs bei den Rauchern ca. dreimal so hoch ist wie bei den Nicht-Rauchern. Welches Schichtungsmerkmal dabei im Zähler steht ist nicht fix vorgegeben, ist jedoch für die Interpretation des Ergebnisses im Nachhinein wichtig. Man könnte also auch die Nicht-Raucher in den Zähler packen: 
\begin{align*}
    RR_{Lungenkrebs} = \dfrac{\frac{458}{2056}}{\frac{877}{1289}} = \dfrac{0,22}{0,68} = 0,32
\end{align*}
Die Interpretation wäre hier, dass das Risiko für Lungenkrebs bei den Nicht-Rauchern lediglich einem Drittel des Risikos bei den Rauchern entspricht. Letztlich die gleiche Aussage wie zuvor, nur auf andere Art und Weise ausgedrückt.

\paragraph{Chance} Vergleicht man die Risiken von zwei konkurrierenden Risikomerkmalen für ein Schichtungsmerkmal, so spricht man von der Chance oder auf Englisch \textit{Odds}. Man spricht dabei immer von der Chance auf dasjenige Merkmal, dessen Risiko im Zähler steht, d.h. $$\frac{\mbox{Risiko für "Lungenkrebs" bei Rauchern}}{\mbox{Risiko für "kein Lungenkrebs" bei Rauchern}} = \dfrac{\frac{877}{1289}}{\frac{412}{1289}} = \dfrac{877}{412} = 2,13$$
beschreibt die Chance für Lungenkrebs (bei Rauchern), während
$$\frac{\mbox{Risiko für "kein Lungenkrebs" bei Rauchern}}{\mbox{Risiko für "Lungenkrebs" bei Rauchern}} = \dfrac{\frac{412}{1289}}{\frac{877}{1289}} = \dfrac{877}{412} = 0,47$$
die Chance für kein Lungenkrebs (bei Rauchern) beschreibt. Letztendliche liefern beide Berechnungen jedoch wieder dieselbe Aussage.

\paragraph{Odds Ratio} Ähnlich wie beim relativen Risiko möchte man auch beim Odds Ratio zwei verschiedene Schichten miteinander vergleichen, aber nicht hinsichtlich der Risiken sondern hinsichtlich der Chancen. Hierbei dividiert man also die Chancen zweier Schichten durcheinander, z.B. die Chance für Lungenkrebs bei den Nicht-Rauchern und die Chance für Lungenkrebs bei den Rauchern:
\begin{align*}
    OR = \dfrac{\frac{458}{1598}}{\frac{877}{412}} = 0,02
\end{align*}
Dieser Wert bedeutet, dass die Chance für Lungenkrebs bei den Nicht-Rauchern ca. 0,02 mal so hoch ist wie bei den Rauchern. Welches Schichtungsmerkmal dabei im Zähler steht ist nicht fix vorgegeben, ist jedoch für die Interpretation des Ergebnisses im Nachhinein wichtig. Man könnte also auch die Raucher in den Zähler packen: 
\begin{align*}
    OR = \dfrac{\frac{877}{412}}{\frac{458}{1598}} = 58,65
\end{align*}
Die Interpretation wäre hier, dass die Chance für Lungenkrebs bei den Rauchern 58,65 mal dem der Nicht-Raucher entspricht. Letztlich die gleiche Aussage wie zuvor, nur auf andere Art und Weise ausgedrückt.\\

\noindent \dangersign[3ex] Bei der generellen Formel für den Odds Ratio steht (sofern nicht explizit etwas anderes gesagt wird) das Schichtungsmerkmal aus der ersten Zeile im Zähler, was man an der Formel (bezogen auf Tabelle \ref{tab:2k2}) erkennen kann:
\begin{align*}
    OR = \dfrac{\frac{a}{b}}{\frac{c}{d}} = \dfrac{a \cdot d}{b \cdot c}
\end{align*}\\

\noindent  \framebox[\textwidth]{\texttt{R-Befehl für den Odds Ratio: > OddsRatio(data)} \hfill \href{https://www.rdocumentation.org/packages/DescTools/versions/0.99.37/topics/OddsRatio}{Dokumentation}}

\noindent \dangersign[3ex] Die R-Funktion ist \textbf{nicht} Teil von \texttt{base-R} sondern Teil des Paketes \texttt{DescTools}. Dieses muss zunächst installiert (\texttt{install.packages("DescTools")}) und anschließend importiert werden (\texttt{library(DescTools)}).

\clearpage

\subsection{Zusammenhangsmaße für ordinale Merkmale}\label{sec:zshg-ord}
Bei ordinalen Daten haben wir, durch die Möglichkeit die Daten zu ordnen, mehr Informationsgehalt. Deshalb macht es hier Sinn, andere Zusammenhangsmaße zu benutzen, damit dieser Informationsgehalt nicht verloren geht. Die Idee hinter den ordinalen Zusammenhangsmaßen ist die, dass man jede Beobachtung mit jeder anderen Beobachtung vergleicht und sich dabei anschaut, wie diese zueinander stehen. Dabei ordnet man solche Beobachtungspaare als \textit{konkordant}, \textit{diskordant} oder als \textit{Bindung} ein.

\paragraph{Konkordanz} liegt vor, wenn sich X und Y in die gleiche Richtung bewegen, also die Beobachtung mit einem größeren x-Wert auch einen größeren y-Wert aufweist.\\
\textit{Beispiel:} $(x_1,y_1)=(2,3)$ ist konkordant zu $(x_2,y_2)=(4,8)$, denn beim Übergang von der ersten zur zweiten Beobachtung steigt X von 2 auf 4 und Y von 3 auf 8. Konkordanz läge ebenfalls vor, wenn X fallen \& Y ebenfalls fallen würde. Somit bewegen sich beide Werte in die gleiche Richtung. "\textit{Je größer X, desto größer Y}" spricht für einen positiven/gleichgerichteten Zusammenhang.

\paragraph{Diskordanz} ist das Gegenteil, d.h. wenn X sich in eine andere Richtung als Y bewegt.\\
\textit{Beispiel} hierfür ist $(x_1,y_1)=(2,3)$ und $(x_2,y_2)=(5,1)$. Beim Übergang von der ersten zur zweiten Beobachtung steigt X von 2 auf 5, Y sinkt jedoch von 3 auf 1. Diskordanz läge ebenfalls vor, wenn X fallen \& Y steigen würde. Es geht somit nur um die Entwicklung in unterschiedliche Richtungen, also um einen negativen Zusammenhang.

\paragraph{Bindungen (Ties)} liegen vor wenn sich ein Wert verändert, der andere jedoch gleich bleibt.\\ 
\textit{Beispiel} für eine Bindung in X wäre hier $(x_1,y_1)=(3,5)$ und $(x_2,y_2)=(3,8)$ bzw. für eine Bindung in Y $(x_1,y_1)=(4,7)$ und $(x_2,y_2)(2,7)$. Bindungen enthalten keine Aussagekraft für mögliche Zusammenhänge.\\

\noindent Um basierend auf einer Kontigenztafel die Anzahlen der konkordanten/diskordanten Paare und der Bindungen zu berechne, ist es zunächst einmal wichtig, dass die Merkmalsausprägungen in den Zeilen \& Spalten geordnet sind. Ist dies nicht der Fall, ist ein strukturiertes Vorgehen unmöglich. Ist dies gewährleistet, gibt es eine bestimmte, strukturierte Vorgehensweise:\\

\noindent \textbf{Konkordante Paare:}
\begin{itemize}
    \item Man beginnt \textbf{links} oben (Zelle für die kleinste Ausprägung sowohl von X, als auch von Y).
    \item Man multipliziert die Beobachtungszahl in dieser Zelle mit allen Beobachtungszahlen, die in Zellen \textit{unter} UND \textit{rechts} von dieser Zelle liegen.\\
    Grund: Diese Beobachtungen haben sowohl größere x-Werte, als auch größere y-Werte.
    \item Man wendet dieses Prinzip auf \textbf{jede} Zelle in der Tabelle an und summiert die Werte für die einzelnen Zellen am Schluss auf, um die Gesamtzahl konkordanter Paare zu erhalten.
\end{itemize}

\noindent \textbf{Diskordante Paare}
\begin{itemize}
    \item Man beginnt \textbf{rechts} oben (Zelle für die kleinste Ausprägung von X und die größte von Y).
    \item Anders als bei der Konkordanz multipliziert man die Beobachtungszahl in dieser Zelle mit allen Beobachtungszahlen, die in Zellen \textit{unter} UND \textit{links} von dieser Zelle liegen.\\
    Grund: Diese Beobachtungen haben sowohl größere x-Werte, jedoch kleinere y-Werte.
    \item Man wendet dieses Prinzip auf \textbf{jede} Zelle in der Tabelle an und summiert die Werte für die einzelnen Zellen am Schluss auf, um die Gesamtzahl diskordanter Paare zu erhalten.
\end{itemize}

\noindent \textbf{Bindungen (Ties) in X}
\begin{itemize}
    \item Man beginnt \textbf{links} oben (Zelle für die kleinste Ausprägung sowohl von X, als auch von Y).
    \item Man multipliziert die Beobachtungszahl in dieser Zelle mit allen Beobachtungszahlen, die in Zellen \textit{in derselben Zeile} wie diese Zelle liegen.\\
    Grund: Diese Beobachtungen haben den gleichen x-Wert, jedoch andere y-Werte.
    \item Man wendet dieses Prinzip auf \textbf{jede} Zelle in der Tabelle an und summiert die Werte für die einzelnen Zellen am Schluss auf, um die Gesamtzahl an Bindungen in X zu erhalten.
\end{itemize}

\noindent \textbf{Bindungen (Ties) in Y}
\begin{itemize}
    \item Man beginnt \textbf{links} oben (Zelle für die kleinste Ausprägung sowohl von X, als auch von Y).
    \item Man multipliziert die Beobachtungszahl in dieser Zelle mit allen Beobachtungszahlen, die in Zellen \textit{in derselben Spalte} wie diese Zelle liegen.\\
    Grund: Diese Beobachtungen haben andere x-Werte, jedoch den gleichen y-Wert.
    \item Man wendet dieses Prinzip auf \textbf{jede} Zelle in der Tabelle an und summiert die Werte für die einzelnen Zellen am Schluss auf, um die Gesamtzahl an Bindungen in Y zu erhalten.
\end{itemize}

\noindent \dangersign[3ex] \textit{Notation:} Für alle nachfolgend aufgeführten Formeln werden die Anzahlen der konkordanten \& diskordanten Paare, sowie der Bindungen in X bzw. Y, mit $K$, $D$, $T_x$ \& $T_y$ bezeichnet.\\

\noindent Ein schönes Beispiel für die Berechnung von $K$ und $D$ findet sich hier: \hfill \href{https://support.minitab.com/de-de/minitab/19/help-and-how-to/statistics/tables/supporting-topics/other-statistics-and-tests/what-are-concordant-and-discordant-pairs/}{Beispiel}\\

\noindent Mit der Kenntnis über die Anzahlen der konkordanten \& diskordanten Paare, sowie der Bindungen in X bzw. Y, kann man nun einige Zusammenhangsmaße berechnen.

\clearpage

\subsubsection{Gamma nach Goodman and Kruskal}\label{sec:gamma}
Das $\gamma$ nach Goodman and Kruskal quantifiziert den Zusammenhang zwischen zwei Merkmalen allein auf Grundlage der konkordanten und diskordanten Paare. Im Zähler wird die Differenz der beiden Werte berechnet und im Nenner werden die beiden addiert.
\begin{align*}
    \gamma=\frac{K-D}{K+D}
\end{align*}

Aufgrund der Differenzenbildung im Zähler dann der Bruch sowohl positive, als auch negative Werte annehmen. Liegen nur diskordante Paare vor (d.h. $K = 0$), besteht ein perfekter negativer Zusammenhang und $\gamma$ nimmt den Wert -1 an. Liegen ausschließlich konkordante Paare vor (d.h. $D = 0$), so spricht man von einem perfekten positiven Zusammenhang und $\gamma$ beträgt +1.\\
Abstufungen im negativen \& positiven Bereich deuten auf \textit{tendenziell} negative bzw. positive Zusammenhänge hin und je näher der Wert betragsmäßig an der 1 liegt, als desto stärker wird der Zusammenhang bewertet. Liegen (annähernd) gleich viele konkordante und diskorante Paar vor, so ist wird $\gamma$ (annähernd) einen Wert von 0 annehmen und es ist kein eindeutiger Zusammenhang erkennbar.\\

\noindent \dangersign[3ex] Im Gegensatz zu den den Maßen für nominale Merkmale beinhaltet $\gamma$ nicht nur eine Aussage über die \textbf{Stärke}, sondern auch über die \textbf{Richtung} des Zusammenhangs. Dies spiegelt sich im Wertebereich wieder, der in Kapitel \ref{sec:zshg-nom} bei allen Maßen nur im positiven lag und bei den Maßen in diesem Kapitel sowohl im positiven als auch im negativen liegt.\\

\noindent \dangersign[3ex] Bei der Berechnung von $\gamma$ werden die Bindungen komplett außen vor gelassen werden. Dies führt (tendenziell) zu einer Überschätzung des Zusammenhangs, da Beobachtungspaare die gegen einen Zusammenhang sprechen (die Bindungen) ignoriert werden. Die $\tau$-Maße beheben dieses Problem.\\

\noindent  \framebox[\textwidth]{\texttt{R-Befehl für $\gamma$: > GoodmanKruskalGamma(data)} \hfill \href{https://www.rdocumentation.org/packages/DescTools/versions/0.99.37/topics/GoodmanKruskalGamma}{Dokumentation}}

\noindent \dangersign[3ex] Die R-Funktion ist \textbf{nicht} Teil von \texttt{base-R} sondern Teil des Paketes \texttt{DescTools}. Dieses muss zunächst installiert (\texttt{install.packages("DescTools")}) und anschließend importiert werden (\texttt{library(DescTools)}).

\clearpage

\subsubsection{Tau-Maße}\label{sec:tau}

\paragraph{Kendalls $\tau_b$}
Hier werden die $T_x$ und $T_y$ Bindungen berücksichtigt. Der Zähler ist identisch zu $\gamma$, jedoch werden im Nenner die Bindungen mit eingerechnet:
\begin{align*}
    \tau_b = \frac{K-D}{\sqrt{(K+D+T_x)(K+D+T_y)}}
\end{align*}

\noindent \dangersign[3ex] Liegen keine Bindungen vor, so kollabiert der Nenner zu 
$$\sqrt{(K+D+0)(K+D+0)} = \sqrt{(K+D)^2} = K+D\;,$$ sodass $\tau_b$ identisch zu $\gamma$ ist.\\

\noindent  \framebox[\textwidth]{\texttt{R-Befehl für $\tau_b$: > cor(data\$x, data\$y, method = "kendall")} \hfill \href{https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/cor}{Dokumentation}}

\paragraph{Kendalls/Stuarts $\tau_c$}
Bei Kendalls/Stuarts $\tau_c$ werden nicht die Bindungen, sondern das Minimum aus der Spaltenzahl und der Zeilenanzahl in der Berechnung berücksichtigt. Ausgangspunkt ist jedoch weiterhin die Differenz aus den Anzahlen der konkordanten und diskordanten Paare im Zähler.
\begin{align*}
    \tau_c = \frac{2min(k,l) (K-D)}{n^2(min(k,l)-1)}
\end{align*}

\noindent \dangersign[3ex] \textbf{Remember:} Die Zeilen- und Spaltenzahl werden in diesem Kontext mit $k$ \& $l$ bezeichnet.\\

\noindent  \framebox[\textwidth]{\texttt{R-Befehl für $\tau_c$: > StuartTauC(data)} \hfill \href{https://www.rdocumentation.org/packages/DescTools/versions/0.99.37/topics/StuartTauC}{Dokumentation}}

\noindent \dangersign[3ex] Die R-Funktion ist \textbf{nicht} Teil von \texttt{base-R} sondern Teil des Paketes \texttt{DescTools}. Dieses muss zunächst installiert (\texttt{install.packages("DescTools")}) und anschließend importiert werden (\texttt{library(DescTools)}).

\noindent \dangersign[3ex] Kendalls $tau_b$ ist das wohl gebräuchlichste diese drei Maße, was man auch daran erkennen kann, dass es in \textit{base-R} verfügbar ist während die anderen beiden nur über eine spezielles Paket verfügbar sind.

\subsubsection{Rangkorrelationskoeffizient nach Spearman}
Der Spearman Korrelations-Koeffizient ($r_{SP}$) wird auch Rangkorrelationskoeffizient genannt, da man nicht die Abstände der echten Datenpunkte zueinandner in Beziehung setzt, sondern nur die \textit{Rangabstände} betrachtet. Dies ist auch der große Unterschied zum Korrelationskoeffizient nach Bravais-Pearson (vgl. Kap. \ref{sec:cor-bp}).

\paragraph{Berechnung:} Die Basis bildet eine Auflistung der Beobachtungen und die Zuordnung der passende Ränge zu den einzelnen Merkmalsausprägungen. Anschließend bildet man \textit{für jeder Beobachtung} die Differenz des Ranges von $x_i$ und des Ranges von $y_i$. Auf Basis dieser Rangdifferenzen $d_i$ wird $r_{SP}$ schließlich berechnet:

\begin{align*}
    r_{SP}=1-\frac{6\sum_{i=1}^nd_i^2}{n(n^2-1)}
\end{align*}

\noindent \dangersign[3ex] Diese Formel funktioniert nur wenn \textbf{keine} Bindungen vorliegen. Ist dies doch der Fall, so muss man dafür korrigieren (siehe unten).

\paragraph{Korrigierter Rangkorrelationskoeffizient nach Spearman} Liegen \textit{Bindungen} innerhalb eines Merkmals oder beider Merkmale vor, wird die Bestimmung der Ränge etwas komplexer. Zusätzlich zu den $d_i$ kommen hier noch $b_j$ und $c_k$ hinzu.\\

\noindent \dangersign[3ex] $b_j$ und $c_k$ haben einen anderen Index als die $d_i$. Während $i$ der Laufindex für die Beobachtungen ist, beziehen sich $j$ und $k$ auf die verschiedenen Merkmalsausprägungnen von X bzw. Y.\\

\noindent Die $b_j$ sind die Anzahlen, die angeben wie oft die unterschiedlichen Merkmalsausprägungen beim Merkmal X jeweils auftreten. Die $c_k$ bedeuten analog dazu dasselbe für Y.\\
Liegen keine Bindungen vor, so passieren zwei Dinge: Alle $b_j$ und alle $c_k$ nehmen den Wert 1 an, da jede Merkmalsausprägung nur einmal vorkommt. Die Teile der Formel mit $b_j$ und $c_k$ werden dadurch gleich Null und fallen also weg. Passiert dies, so reduziert sich die Formel auf die obige Formel, welche also nur einen Spezielfall der nachfolgenden Formel darstellt:

\begin{align*}
    r_{SP}=\frac{n(n^2-1)-\frac{1}{2}\sum_jb_j(b_j^2-1)-\frac{1}{2}\sum_kc_k(c_k^2-1)-6\sum_id_i^2}{\sqrt{n(n^2-1)-\sum_jb_j(b_j^2-1)}\sqrt{n(n^2-1)-\sum_kc_k(c_k^2-1)}}
\end{align*}

\noindent Der Wertebereich von $r_{SP}$ geht von -1 bis +1 und auch die Interpretation ist analog zu den $\gamma$- \& $\tau$-Maßen: Bei $r_{SP}<0$ liegt ein negativer Zusammenhang zwischen den beiden Merkmalen vor, bei $r_{SP}>0$ liegt ein positiver Zusammenhang vor und bei $r_{SP}=0$ liegt kein Zusammenhang vor. Zudem ist $r_{SP}$ dimensionslos und symmetrisch. Symmetrisch bedeutet, dass $r_{SP}(X,Y) = r_{SP}(Y,X)$.\\

\noindent \framebox[\textwidth]{\texttt{R-Befehl für $r_{SP}$: > cor(data\$x, data\$y, method = "spearman")} \hfill \href{https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/cor}{Dokumentation}}\\

\noindent \dangersign[3ex] Auch wenn alle Maße aus diesem Kapitel für ordinale Merkmale gedacht sind, kann man sie auch für metrische Daten verwenden. Dabei entsteht allerdings Informationsverlust, da diese Maße nicht die volle Information der metrischen Skala (Interpretierbarkeit von Abständen) ausnutzen.

\clearpage

\subsection{Zusammenhangsmaße für metrische Merkmale}\label{sec:zshg-metr}
\subsubsection{Kovarianz}
Die Kovarianz ist ein Maß für den Zusammenhang zweier metrischer Merkmale und ist abhängig von der Varianz.\\
\paragraph{Berechnung} 
\begin{enumerate}
    \item Die einzelnen Beobachtungspaare werden multipliziert und aufsummiert $\sum_{i=1}^nx_iy_i$
    \item Von der Summe wird das Produkt aus n und der Mittelwerte von x und y subtrahiert$\sum_{i=1}^nx_iy_i-n\bar{x}\bar{y}$
    \item zuletzt wird die Differenz mit $\frac{1}{n}$ multipliziert $Cov(X;Y)=\frac{1}{n}\sum_{i=1}^nx_iy_i-n\bar{x}\bar{y}$
\end{enumerate}
Ist die Kovarianz positiv bzw. negativ, so liegt ein positiver bzw. negativer Zusammenhang zwischen den beiden Merkmalen vor. Ein Sonderfall ist die Kovarianz des gleichen Merkmals X, denn diese ist dann wieder die Varianz von X $Cov(X;X)=Var(X)$
Wie oben genannt, hängt die Kovarianz nicht nur vom Zusammenhang der beiden Merkmale ab sondern auch von der Streuung. Um das Zusammenhangsmaß von der Streuung loszulösen, braucht man ein anderes Maß, nämlich den Korrelationskoeffizient nach Bravais-Pearson.

\subsubsection{Korrelationskoeffizient nach Bravais-Pearson}\label{sec:cor-bp}
Der Korrelationskoeffizient nach Bravais-Pearson standardisiert die Kovarianz und hängt somit nicht mehr von der Streuung sondern nur noch vom Zusammenhang der beiden Merkmale ab. Bei diesem Zusammenhangsmaß werden die \textit{Abstände} zwischen den Beobachtungen beider Merkmale und deren arithmetischen Mittel in Beziehung gesetzt. Da der Korrelationskoeffizient nach Bravais-Pearson die standardisierte Kovarianz ist, hängt dieser nicht mehr von der Streuung, sondern nur noch vom Zusammenhang der beiden Merkmale ab. Daher berechnet man diesen auch mit der Kovarianz dividiert durch die Wurzel aus dem Produkt der beiden Varianzen. 
\begin{align*}
    r_{BP}&=\frac{Cov(X;Y)}{\sqrt{Var(X)Var(Y)}} \\
    \mbox{und daher eingesetzt:}\quad
   r_{BP}&=\frac{\sum_{i=1}^n(x_i-\bar{x}((y_i-\bar{y}}{\sqrt{(\sum_{i=1}^nx_i^2-n\bar{x^2})(\sum_{i=1}^ny_i^2-n\bar{y^2})}}
\end{align*}
Dies ist nun auch wieder ein symmetrisches Maß.
Hier geht der Wertebereich nun wieder von -1 bis 1, da es ein standardisiertes, dimensionsloses Konzentrationsmaß ist. Ist $r_BP>0$, dann sind die beiden Merkmale X und Y positiv linear abhängig, ist $r_BP<0$ sind die beiden Merkmale negativ abhängig und bei $r_BP=0$ sind die beiden Merkmale unabhängig voneinander.\\
Um nun alle Skalenniveaus und deren passenden Zusammenhangsmaße auf einen Blick. Hier wird auch nochmals deutlich, dass es kein Problem ist, wenn ein ordinales Merkmal mit einem nominalen Zusammenhangsmaß gemessen wird. Ein Zusammenhangsmaß eines weniger aussagekräftigen Merkmals zu benutzen ist möglich, jedoch andersherum nicht.\\

\begin{tabular}{c|c|c|c}
&Nominal & Ordinal & metrisch\\
\hline
Nominal& $\chi^2$& & \\
Ordinal&$\chi^2$&$\tau$-und$\gamma$-Maße/ Rangkorrelation& \\
Metrisch &$\chi^2$& Rangkorrelation & Korrelation (Kovarianz) \\
\end{tabular}

\clearpage

\subsection{Aufgaben}

\paragraph{1. Welche Aussagen bzgl. relativem Risiko \& Odds Ratio sind wahr?}

\begin{itemize}
    \item[a)] Bei beiden Maßzahlen werden zwei Gruppen verglichen. \hfill $\square$
    \item[b)] Der Odds Ratio kann auf Basis von relativen Risiken berechnet werden. \hfill $\square$
    \item[c)] Beim relativen Risiko werden zwei Risikomerkmale verglichen. b\hfill $\square$
    \item[d)] Ein Odds Ratio < 0 bedeutet eine geringere Chance in der ersten Gruppe. \hfill $\square$
\end{itemize}

\paragraph{2. Die unter Unabhängigkeit erwarteten absoluten Häufigkeiten ..}

\begin{itemize}
    \item[a)] .. müssen stets ganzzahlig sein. \hfill $\square$
    \item[b)] .. können ohne Kenntnis der gemeinsamen Verteilung berechnet werden. \hfill $\square$
    \item[c)] .. sind identisch zu der bedingten Verteilung. \hfill $\square$
    \item[d)] .. sind maximal so hoch wie die tatsächlich beobachteten Häufigkeiten. \hfill $\square$
\end{itemize}

\paragraph{3. Welche der folgenden Aussagen über Zusammenhangsmaße für nominale Merkmale sind wahr?}

\begin{itemize}
    \item[a)] Mit Cramers $V$ sind Zusammenhänge für Kontingenztafeln von verschiedener Dimension\\und mit unterschiedlichem $n$ vergleichbar. \hfill $\square$
    \item[b)] $\Phi$ ist besitzt einen kleineren Wertebereich als $\chi^2$. \hfill $\square$
    \item[c)] Um Kontingenztafeln mit dem korrigierten Kontingenzkoeffizienten vergleichen zu\\können muss deren Dimension gleich sein. \hfill $\square$
    \item[d)] Kein Zusammenhangsmaß für nominale Merkmale kann negative Werte annehmen. \hfill $\square$
\end{itemize}

\paragraph{4. Welche der folgenden Aussagen sind wahr?}

\begin{itemize}
    \item[a)] Rang-basierte Zusammenhangsmaße sind bei metrischen Merkmalen nicht anwendbar. \hfill $\square$
    \item[b)] Das Prinzip der Kon-/Diskordanz kann auch bei nominalen Merkmalen angewendet\\werden. \hfill $\square$
    \item[c)] Zusammenhangsmaße für nominale Merkmale können nicht bei ordinalen oder\\metrischen Merkmalen verwendet werden. \hfill $\square$
    \item[d)] Zusammenhangsmaße für nominale Merkmale können keine Richtung des\\Zusammenhangs angeben. \hfill $\square$
\end{itemize}

\paragraph{5. Bindungen in $Y$ ..}

\begin{itemize}
    \item[a)] .. sprechen für einen negativen Zusammenhang. \hfill $\square$
    \item[b)] .. haben keinen Einfluss auf den Wert von $\gamma$. \hfill $\square$
    \item[c)] .. erhöhen den Wert von Kendalls $\tau_b$. \hfill $\square$
    \item[d)] .. haben einen Einfluss auf den Wert von Kendalls/Stuarts $\tau_c$. \hfill $\square$
\end{itemize}

\newpage


\section{Lineare Einfachregression}\label{sec:lineare Einfachregression}
\subsection{Einführung}
Das Ziel der Regression ist es herauszufinden, welchen Einfluss ein Merkmal X (Einflussgröße) auf ein Merkmal Y (Zielgröße) hat und somit aus einem oder mehreren Werten ein Wert vorhersagen zu können, da zwischen diesen ein Zusammenhang besteht. Die Einflussgröße X wird auch Regressor oder unabhängige Variable genannt. Das Merkmal Y wird daher abhängige Variable, da sie abhängig von X ist, Response oder Regressand genannt.
Um eine Regressionsanalyse sinnvoll erstellen zu können, benötigt man Beobachtungspaare, mit unterschiedlichen Merkmalsausprägungen von X und Y. Damit kann man dann die Merkmale auf einen (linearen) Zusammenhang prüfen und versuchen, diesen mit einem Modell zu schätzen.\\
Eine Voraussetzung, um ein Modell anzuwenden ist, dass das Merkmal X die unabhängige Variable ist, welche gegeben oder beeinflussbar ist und das Merkmal Y als Reaktion auf X (abhängige Variable).\\
Der einfachste Zusammenhang zwischen zwei Merkmalen ist der lineare Zusammenhang, welcher durch den Ansatz $Y=a+bX$ angegeben werden kann. Kann man X und Y durch diesen Ansatz abbilden, dann spricht man von einer \textit{linearen Regression}.\\
Da normalerweise aufgrund von Abweichung bzw. Streuung nicht alle Merkmalsausprägungen auf einer Gerade liegen, ergänzt man das vorige Modell mit dem \textit{Fehlerglied (bzw. -term)}oder \textit{Residuum e}. Somit wird die Abweichung von der Geraden in das Modell miteinbezogen und es entsteht das Modell: $Y=a+bX+e$

\subsection{Plots und Annahmen}
Da man bei zwei Merkmalen nicht direkt von einem linearen Zusammenhang bzw. einem Ursachen-Wirkung-Beziehung ausgehen kann, ist es ratsam, zuerst einmal durch eine \textit{graphische Darstellung} zu überprüfen, ob ein linearer Zusammenhang überhaupt absehbar ist. Hierbei ist ein \textit{Streudiagramm} oder \textit{Scatter-Plot} hilfreich.\\
Hierbei trägt man auf die X-Achse die Einflussgröße X und die Zielgröße auf der y-Achse abgetragen. Danach werden Merkmalsausprägungen als ein Punk $(x_i;y_i)$ in das Koordinatensystem eingetragen. Liegt ein Zusammenhang zwischen den Merkmalen vor, kann man das oft schon hier erkennen. Es gibt jedoch nicht nur einen linearen, sondern unter anderem auch einen zyklischen, exponentiellen, logarithmischen und polynomischen Zusammenhang. Im Folgenden werden jedoch hauptsächlich die linearen Zusammenhänge thematisiert. \\
Häufig wird ein Zusammenhang durch ein nicht dazu passender Wert,\textit{Ausreißer}, gestört. Diese müssen gesondert betrachtet werden und gegebenenfalls unter Rechtfertigung entfernt werden, da diese sogar die Richtung des Zusammenhangs ändern können. 

\subsection{Kleinste-Quadrate-Schätzer}\label{sec:KQS}
Kann man nun davon ausgehen, dass es sich bei den vorliegenden Merkmalen um einen linearen Zusammenhang handelt, muss man die \textit{optimale Gerade} aus den Merkmalsausprägungen gewinnen. Somit liegt hier ein Optimierungsprozess vor, bei dem die Parameter der linearen Gleichung b, als Steigung und a als y-Achsenabschnitt optimiert werden müssen. \\
Jedem Beobachtungspunkt $P_i0(x_i;y_i)$ wird ein angepasster Punkt $\hat{P}_i=(x_i;\hat{y}_i)$ zugeordnet, der nun auf der Geraden liegt. An dem "Dach" über dem y sieht man, dass die x-Werte dieselben bleiben, jedoch die y-Werte an die Gerade angepasst werden. Daraus ergibt sich dann die Gerade: $\hat{y}_i=a+bx_i$.\\
Die Differenz zwischen dem Beobachtungspunkt $P_i$ und dem geschätzten Punkt $\hat{P}_i$ auf der Geraden ist das vorher angesprochene \textit{Residuum} oder \textit{Fehlerglied}, also die Abweichung der real beobachteten Werten von der linearen geschätzten Geraden.\\
Um nun zu optimieren besteht die Möglichkeit, die Gesamtheit der Residuen zu minimieren. Da sich diese jedoch ausgleichen könnten, wenn wir sie einfach nur aufsummieren, sollten wir die Residuen entweder in Betrag setzen, mit denen jedoch umständlich zu rechnen ist oder diese quadrieren. Da dies die einfachste Lösung ist, wird im Folgenden die Summe der quadrierten Residuen minimiert. Daher kommt auch der Name, Kleinste-Quadrate-Schätzer, zustande. Um die Gesamtheit der quadrierten Residuen zu minimieren, löst man die quadrierte lineare Gleichung nach $\sum_{i=1}^ne_i^{2}$ auf und erhält $\sum_{i=1}^n(y_i-a-bx_i)^{2}$. Diese Summe muss nun minimiert werden.
\paragraph{Vorgehensweise der Schätzung:} Um ein Minimum, also eine Extremstelle zu finden, leitet man die Gleichung jeweils nach a und nach b ab. Anschließend sucht man die Nullstellen durch gleichsetzen mit 0. Danach setz man $\hat{a}$ in die Gleichung für b ein und erhält: 
\begin{align*}
    \hat{b}=\frac{\sum_{i=1}^nx_iy_i-n\bar{y}\bar{x}}{\sum_{i=1}^nx_i^{2}-n\bar{x}^{2}}
\end{align*}
Um nun zu prüfen, ob an dieser Extremstelle wirklich ein Minimum liegt, bildet man die 2. Ableitung von a und b nach der Hesse-Matrix und setzt die Lösung der 1. Ableitung ein. Ist das Ergebnis der 2.Ableitung nun größer als 0 ist bestätigt, dass an der Herausgefundenen Extremstelle ein Minimum vorliegt. Da dies hier der Fall ist, ist $\hat{b}=\frac{\sum_{i=1}^nx_iy_i-n\bar{y}\bar{x}}{\sum_{i=1}^nx_i^{2}-n\bar{x}^{2}}$ die Lösung des Optimierungsproblems und eingesetzt in die lineare Gleichung ergibt sich für das optimierte $\hat{a}$:
\begin{align*}
    \hat{a}=\bar{y}-\hat{b}\bar{x}
\end{align*}
Dabei sind die Werte $\bar{x}$ und $\bar{y}$ die Mittelwerte aus den Beobachtungen.
Man kann $\hat{b}$ auch umschreiben in $\hat{b}=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^{2}}$, was einem wieder bekannter vorkommen wird, da im Zähler die Summe über die Produkte der Abweichungen von x und y von ihrem jeweiligen Mittelwert steht. Dies kann man auch schreiben als $S_{xy}$. Teilt man dies durch n, würde man die Kovarianz. Der Nenner des Quotienten ist die Summe über die quadrierten Abweichungen von x von seinem Mittelwert, dies schreibt man auch als $S_{xx}$. Teilt man dies auch durch n, so haben wir die Varianz von x. Da nun Zähler und Nenner jeweils geteilt durch n, Kovarianz und Varianz ergibt, kann man n heraus kürzen und man erhält $\hat{b}=\frac{Cov(X;Y)}{Var(X)}$. Zudem kann man $\hat{b}$ auch mit dem Rangkorrelationskoeffizient nach Bravais-Pearson berechnen: $\hat{b}=r_{BP}\sqrt{\frac{S_{yy}}{S_{xx}}}$. Dabei ist $S_{yy}$ die Summe über die quadrierten Abweichungen von y von seinem Mittelwert und geteilt durch n, würde man die Varianz von y erhalten.

\subsection{Eigenschaften der Regressionsgerade}
\paragraph{Sinnvoller Wertebereich} Da wir nur in einem Wertebereich von 1 bis n Merkmalsausprägungen haben, können wir nur über diesen Bereich eine sinnvolle Aussage treffen, da es außerhalb des Bereichs auch möglich wäre ein ganz anderen Zusammenhang zu haben. Deshalb können wir nur über den uns bekannten Bereich eine Aussage treffen. 
\paragraph{"arithmetisches Mittel"} Da im vorigen Kapitel zur Berechnung von $\hat{a}$ die Mittelwerte verwendet wurden, liegt der Punkt mit den Mittelwerten $(\bar{x},\bar{y}$ auch auf der Regressionsgeraden. Kleiner Tipp: Wenn man die Regressionsgerade zeichnen muss, dann kann man diesen Punkt immer beim einzeichnen verwenden, da die arithmetischen Mittel meistens schon bekannt sind.

\paragraph{Fehlerausgleich}Wie zuvor in Kapitel \ref{sec:KQS} erläutert wollten wir die Gerade optimieren und dabei möglichst kleine Residuen erhalten. In der optimierten Gerade sind dann schlussendlich die Summer der positiven und negativen Residuen ausgeglichen. Deshalb ist dann auch das arithmetische Mittel der angepassten Daten $\bar\hat{y}$ gleich dem arithmetischen Mittel aus den Originaldaten $\bar{y}$.

\paragraph{Bedeutung des Korrelationskoeffizient} Wie weiter oben bereits gezeigt, kann man $\hat{b}$ mit Hilfe des Korrelationskoeffizient nach Pravais-Pearson berechnen. Dabei gibt dieser Auskunft über den positiven bzw. negativen Zusammenhang der beiden Merkmale. Jedoch kann man nicht direkt von einer höheren Korrelation auf einen größeren Anstieg der Regressionsgerade schließen, da nicht nur die Korrelation eine Rolle bei der Berechnung von $\hat{b}$ spielt sondern auch $S_{XX}$ und $S_{YY}$.

\subsection{Güte der Anpassung}
Nun haben wir in den vorigen Unterkapiteln gelernt, wie man ein Regressionsmodell aufstellen und berechnen kann. Nun ist jedoch die Frage \textit{Wie gut} repräsentiert unser Modell die Originaldaten? Über die Varianzanalyse kann man ein Maß gewinnen, welches schließlich eine Aussage machen kann, inwiefern unser Modell mit den Originaldaten übereinstimmt. \\
Um auf das gesuchte Maß der Anpassung zu kommen, geht man von den geschätzten Residuen, der Differenz von den beobachteten Werten und deren geschätzten Werten: $\hat{e}_i=y_i-\hat{y}_i$. Um auf die Varianz zu kommen, baut man in die Differenz den Mittelwert ein und quadriert und nummeriert über alle Beobachtungswerte. Schlussendlich erhält man die Gleichung:
\begin{align*}
    \sum_{i=1}^n(y_i-\bar{y})^2=\sum_{i=1}^n(\hat{y}_i-\bar{y})^2-\sum_{i=1}^n(y_i-\hat{y}_i)^2
\end{align*}
Die erste der drei Summen $\sum_{i=1}^n(y_i-\hat{y}_i)^2$ ist die aufsummierte Varianz der gesamten y-Messreihe und wird mit $SQ_{Total}$ bezeichnet. Die zweite Summe $\sum_{i=1}^n(y_i-\bar{y})^2$ misst den Anteil an der gesamten Varianz, der durch das Regressionsmodell erklärt wird. Das heißt je größer dieser im Verhältnis zur Gesamtvarianz ist, desto besser passt das Regressionsmodell zu den Originaldaten. Diese Summe wird als $SQ_{Rest}$ oder $SQ_{Residual}$ bezeichnet. Und die letzte Summe misst die Abweichung der durch die Regressionsgerade vorhergesagten Werten von den Originaldaten und gibt den Anteil an der Varianz an, der nicht von dem Modell erklärt wird. Um eine gute Repräsentation der Originaldaten durch das Modell zu erhalten, sollte diese Summe möglichst klein sein. Diese Summe bezeichnet man auch als $SQ_{Rest}$ oder $SQ_{Residual}$.\\
Somit erhält man schließlich die Formel der Streuungszerlegung \begin{align*}
    SQ_{Total}=SQ_{Regression}+SQ_{Residual}
\end{align*}
\paragraph{Bestimmtheitsmaß $R^2$}Das Maß, welches angibt inwiefern das Modell die Originaldaten widerspiegelt, wird \textit{Bestimmtheitsmaß $R^2$} genannt und gibt den Anteil der erklärten Varianz $SQ_{Regression}$ an der gesamten Varianz an. Gleichzeitig kann man es auch durch den Anteil der nicht vom Modell erklärten Varianz subtrahiert von 1 berechnen.$R^2$ hängt von der Abweichung der Regressionsgeraden und der Streuung der X-Werte ab.
Da es sich hier um Anteile handelt, geht der Wertebereich von $R^2$ von 0 bis 1.
Man sollte bei einer Regression immer das Bestimmtheitsmaß $R^2$ mit angeben, um zu sehen, wie gut das Modell wirklich die Wahrheit widerspiegelt. 
\paragraph{Grenzwerte von $R^2$} Da sich die Anteile von $SQ_{Rest}$ und $SQ_{Residual}$ zu 1 aufsummieren, ist bei einem Bestimmtheitsmaß von 1, $SQ_{Rest}=0$. In diesem Fall liegen sämtliche beobachteten Werte auf der Regressionsgerade. Somit kann das Regressionsmodell den linearen Zusammenhang zwischen Einflussgröße und Zielgröße optimal wiedergeben.\\
Wenn jedoch das Bestimmtheitsmaß 0 beträgt, dann wird keine Varianz der Originaldaten durch das Modell erklärt. Dabei verläuft die Regressionsgerade dann parallel zur x-Achse, da $\hat{b}=0$ ist und somit erhält jeder x-Wert den gleichen geschätzten $\hat{y}$-Schätzwert. Bei diesem Grenzwert hat das Merkmal X dann \textit{keinen} Einfluss auf das Merkmal Y. Somit hat das Modell keine Aussagekraft. Man spricht auch von \textit{Nullanpassung}\\
Ein anderer Sonderfall haben wir im Übungsblatt 9 besprochen. Dabei ist $SQ_{Total}=0$. Wie kann das zustande kommen. Wir haben vorhergesehen, dass $SQ_{Total}$ die aufsummierte Varianz der y-Messreihe ist, also die aufsummierte quadrierte Differenz von wahren y-Wert und dem Mittelwert von y. Somit müssen die Differenzen 0 sein, damit $SQ_{Total}=0$ ist. Damit dies der Fall ist, darf es keine Streuung bzw. Varianz zwischen den y-Werten liegen und somit liegen alle y-Werte auf einer Geraden. In diesem Fall ist dann $R^2$ auch nicht definiert, da man durch 0 nicht teilen darf und das Bestimmtheitsmaß den Anteil der erklärten Variabilität von y angibt, welche jedoch überhaupt nicht vorhanden ist.
\paragraph{Zusammenhang von $R^2$ und der Korrelation}
Schlussendlich ist das Bestimmtheitsmaß $R^2$ die quadrierte Korrelationskoeffizient nach Bravais-Pearson. Daher kann man bei gegebenen oder berechnetem Bravais-Pearson einfach das Bestimmtheitsmaß berechnen und andersherum.

\subsection{Kategoriale Regression} \label{sec:Kategoriale Regression}
Bis jetzt haben wir die lineare Regression kennengelernt, bei der wir diese für metrische Merkmale berechnen. Was machen wir jedoch, wenn wir keine metrischen Merkmale vorliegen haben, welche nicht quantitative stetig sind, sondern ein kategoriales bzw. nominales Skalenniveau haben? Hierzu gibt es eine andere Vorgehensweise, die \textit{kategoriale} Regression, welche wir im Folgenden näher betrachten.\\
Wir können nicht einfach die verschiedenen Merkmale kodieren, indem wir dem ersten Merkmal die eins und dem zweiten Merkmale die zwei zuordnen und dann normal die Berechnung der Parameterschätzung $\hat{a}$ und $\hat{b}$ ausführen, da nicht unbedingt eine Ordnung zugrunde liegt und die Abstände nicht definiert sind. Deshalb gibt es zwei Möglichkeiten die Merkmale \textit{umzukodieren}: \textbf{Dummykodierueng} und \textbf{Effektkodierung}.\\
Diese beiden Möglichkeiten sind sehr ähnlich in ihrer Vorgehensweise. Bei beiden werden aus k Merkmalsausprägungen k-1 Merkmalsausprägungen. Das heißt, es gibt ein Regressor (Dummy) weniger, da eine beliebige Merkmalsausprägung zur \textit{Referenzkategorie} wird.
Der Unterschied zwischen den Kodierungen ist, dass es bei der Dummykodierung nur zwei mögliche Varianten gibt entweder 0 oder 1 und bei der Effektkodierung 0, 1 oder -1.
Bei der Dummykodierung erhält die Merkmalsausprägung i eine 1, die gerade vorliegt und alle anderen eine 0. Liegt die Referenzkategorie vor, dann sind alle Dummys 0. Das ist auch der Unterschied zur Effektkodierung, da beim Vorliegen der Referenzkategorie alle Dummys den Wert -1 erhalten, ansonsten ist es genau gleich, dass bei vorliegen eines Dummys i, diese den Wert 1 bekommt und alle anderen eine 0.
Hat man allen Merkmalsausprägungen umkodiert und eine Zahl zugeordnet, dann kann man $\hat{b}$ und $\hat{a}$ wieder normal berechnen. \\
Bei der Interpretation der Ergebnisse kommt es dann auch noch einmal zu Unterschieden.\\
\textbf{Dummykodierung:} Da man eine Referenzkategorie eingeführt hat, beziehen sich die Parameterschätzer auf die Referenzkategorie. $\hat{a}$ ist dabei der Mittelwert der Referenzkategorie und $\hat{b}_1\hat{b}_2$ bilden dann jeweils die Abweichungen der Mittelwerte der anderen Kategorien (Dummys) von der Referenzkategorie ab. Das heißt man interpretiert bspw. zu $\hat{b}$: Die Kategorie $X_1$ ist im Mittel um XY teurer als die Referenzkategorie.\\
\textbf{Effektkodierung}: Hier beziehen sich die Parameterschätzungen nicht auf die Referenzkategorie, sondern auf die durchschnittliche Kategorie, in die die Ergebnisse aller Klassen  mit eingehen. Dies liegt daran, dass die Referenzkategorie als Wert -1 erhalten hat.
Somit ist $\hat{a}$ der Mittelwert der durchschnittlichen Kategorie, in die alle Kategorien mit einfließen und $\hat{b}_1,\hat{b}_2$ sind die Abweichungen der Mittelwerte der einzelnen Kategorien von der durchschnittlichen Kategorie. Das heißt man interpretiert bspw. zu $\hat{b}$: Das Merkmal Z ist im Mittel um XY teurer als der Durchschnitt.

\clearpage

\subsection{Aufgaben}

\paragraph{1. Welche der folgenden Aussagen sind wahr?}

\begin{itemize}
    \item[a)] Regressionskoeffizient \& Korrelationskoeffizient haben die gleiche Aussagekraft. \hfill $\square$
    \item[b)] Man kann bereits aus dem Korrelationskoeffizienten auf das Vorzeichen des\\Regressionskoeffizienten schließen. \hfill $\square$
    \item[c)] Höherer Korrelationskoeffizient, bedeutet automatisch auch höherer\\Regressionskoeffizient. \hfill $\square$
    \item[d)] Sowohl Korrelation als auch Regressionskoeffizient haben einen Wertebereich von -1 bis 1. \hfill $\square$
\end{itemize}

\paragraph{2. $R^2$ bei der linearen Einfachregression ..}

\begin{itemize}
    \item[a)] .. ist immer kleiner/gleich dem Korrelationskoeffizienten. \hfill $\square$
    \item[b)] .. kann Werte von -1 bis 1 annehmen. \hfill $\square$
    \item[c)] .. beschreibt den Anteil der erklärten Streuung der Zielgröße durch die Einflussgröße. \hfill $\square$
    \item[d)] .. deutet bei kleinen Werten auf eine eher schlechte Modellgüte hin. \hfill $\square$
    \item[e)] .. beschreibt den Anteil der erklärten Streuung des Modells. \hfill $\square$
\end{itemize}

\paragraph{3.Was ist bei der Interpretations der geschätzten Koeffizienten im Regressionsmodell wichtig?}

\begin{itemize}
    \item[a)] Stets nur den Absolutbetrag interpretieren. \hfill $\square$
    \item[b)] Der Intercept ist der erwartete Wert der Zielgröße wenn die Einflussgröße ihren\\Durchschnittswert annimmt. \hfill $\square$
    \item[c)] Interpretation des Steigungsparameters pro Einheit. \hfill $\square$
    \item[d)] Der Intercept ist nicht immer sinnvoll interpretierbar. \hfill $\square$
    \item[e)] Vorhersagen sollte man nur für Werte der Einflussgröße durchführen, die auch so\\(ähnlich) in der Stichprobe vorkommen. \hfill $\square$
\end{itemize}

\paragraph{4. Welche Aussagen bzgl. kategorialen Regressoren sind korrekt?}

\begin{itemize}
    \item[a)] Sowohl Dummy- als auch Effekt-Kodierung führen zu gleichen Zahl an Dummy-Variaben. \hfill $\square$
    \item[b)] Die Interpretation der geschätzten Koeffizienten ist bei Dummy- \& Effekt\\-Kodierung identisch. \hfill $\square$
    \item[c)] Der Intercept ist weder bei Dummy- noch bei Effekt-Kodierung interpretierbar. \hfill $\square$
    \item[d)] Bei einer höheren Anzahl an verschiedenen Kategorien ist die Dummy-Kodierung\\sinnvoller. \hfill $\square$
\end{itemize}

\paragraph{5. Interpretation des Intercepts bei Dummy-Kodierung:}

\begin{itemize}
    \item[a)] Der Intercept entspricht dem erwarteten Wert der Zielgröße bei Vorliegen der\\Referenzkategorie. \hfill $\square$
    \item[b)] Eine Änderung der Referenzkategorie hat eine Änderung der geschätzten Koeffizienten\\für alle Dummy-Variablen zur Folge. \hfill $\square$
    \item[c)] Eine Änderung der Referenzkategorie hat (potenziell) eine Änderung der\\Anpassungsgüte ($R^2$) zur Folge. \hfill $\square$
    \item[d)] Die Referenzkategorie ist immer auf natürliche Art \& Weise vorgegeben. \hfill $\square$
\end{itemize}

\newpage

\section{Indizes}
\subsection{Verhältniszahlen}
Verhältniszahlen oder Indizes sind Quotienten, die aus zwei Maßzahlen bestehen und bei denen es Sinn macht, diese im Zähler und Nenner zu summieren. Man kann diese in Gliederungszahlen, Beziehungszahlen und einfache Index-/Messzahlen unterteilen.
\subsubsection{Gliederungszahlen}
Gliederungszahlen sind Quotienten, bei denen im Zähler eine Teilmenge und im Nenner die dazugehörige Gesamtmenge befindet. Die Zahlen können als Quotient oder als Quotient multipliziert mit 100 in Prozent geschrieben werden. Beispiel hierzu ist die Erwerbsquote oder die Arbeitslosenquote.
\subsubsection{Beziehungszahlen}
Bei \textit{Beziehungszahlen} gehören Zähler und Nenner nicht zur gleichen Grundgesamtheit, also werden nicht gleich gemessen, jedoch stehen sie in sachlich sinnvollem Zusammenhang zueinander. Dabei wird noch unterschieden zwischen Verursachungszahlen und Entsprechungszahlen. Bei \textit{Verursachungszahlen} stehen Bewegungsmasse und Bestandsmasse im Verhältnis, wie bspw. bei der Geburtenziffer, bei der die Bewegungsmasse "Lebendgeborene" im Verhältnis zur Bestandsmasse "Bevölkerung" steht.
Die \textit{Entsprechungszahlen} haben keinen Bezug auf einen Bestand wie bspw. die Bevölkerungsdichte bei der der Quotient aus Einwohnerzahl und Fläche in $km^2$ besteht.
\subsubsection{Indexzahlen}
Einfache Indexzahlen beschreiben den Zusammenhang zwischen Maßzahlen, die zu verschiedenen Zeitpunkten gemessen wurden. Somit sieht man eine Entwicklung einer Grundgesamtheit, also eine Zeitreihe von Maßzahlen. 
Dabei ist $x_0$ der Wert der Basiszahl in der Basisperiode, worauf sich dann $x_1$, der Wert derselben Maßzahl, jedoch in einer späteren \textit{Berichtsperiode} bezieht. Somit berechnet sich die Indexzahl aus dem Quotienten der Maßzahl der Basisperiode zur Basiszahl der Berichtsperiode.
\begin{align*}
    I_{0t}=\frac{x_t}{x_0}
\end{align*}
Da im Zähler und im Nenner die gleiche Maßzahl befindet, die die gleiche Einheit hat, kürzt sich diese raus und somit hat ein Index keine Einheit.\\
Einfache Indexzahlen sind beispielsweise die Preismesszahlen, auch Preisindex genannt oder der Mengenindex. 
\begin{align*}
    P_{0t}=\frac{p_t}{p_0} (Preisindex)\\
    Q_{0t}=\frac{q_t}{q_0} (Mengenindex)
\end{align*}
Dabei ist $p_t$ der Preis eines Produkts und $q_t$ die Menge eines Produkts zur Berichtsperiode t und $p_0$ bzw. $q_0$ der Preis bzw. die Menge eines Produkts zur Basisperiode 0. Hierbei wird sozusagen eine Zeitreihe von Messungen standardisiert, in dem sie auf die Basisperiode bezogen wird.

\paragraph{Veränderungen des Basisjahres} Bei langen Zeitreihen kann es vorkommen, dass es Sinn macht, irgendwann ein neues Basisjahr festzulegen, da es bspw. zu strukturellen Umbrüchen kam oder sich anderes gravierend verändert hat. Um dieses Problem zu lösen und ein neues Basisjahr einzuführen bildet man den Index $I_{kt}$. Dieser wird durch den Quotienten aus Index der Berichtsperiode zur alten Basisperiode und dem Index aus neuer Basisperiode zur alten Basisperiode berechnet. \\
Durch Umformung der dieses Quotienten, erhält man die Verkettungsregel$I_{0t}=I_{0k}\cdot I_{kt}$. Dabei kann man zwei Indexzahlen mit aufeinander folgenden Zeitabschnitte multiplizieren und erhält so den Index über den gesamten Zeitabschnitt.

\subsection{Preisindizes}
Im Folgenden schauen wir uns \textit{zusammengesetzte Indexzahlen} genauer an, die n verschiedene Güter miteinander verknüpft. Um diese auch wirklich wahrheitsgetreu verknüpfen zu können, werden von den Gütern gleichartige Indexreihen verwendet, das heißt, die Güter werden zur gleichen Basis -und Berichtsperiode miteinander verknüpft. Dabei werden die Vektoren der Preise p und Mengen q zur Basis-/ und Berichtsperiode mit einem Strich rechts oben dargestellt.\\
Der erste Ansatz, den man vermuten könnte, mit dem man eine Preismesszahl, also die Entwicklung der Preise verschiedener Güter im Zeitverlauf, messen kann, wäre das arithmetische Mittel zu bilden. Jedoch ist das Problem beim arithmetischen Mittel, dass alle Preise der Güter zu gleichen Anteilen in die Messzahl mit eingehen würden. Dies wäre jedoch nicht repräsentativ, da man nicht von jedem Produkt die gleiche Menge kauft. Deshalb ist es besser, wenn man jedes Produkt einzeln gewichtet. Daraus erhält man dann die Formel \begin{align*}
    P_{0t}=I_{0t}^P(1) \tilde w(1)+....+I_{0t}^P(n)\tilde w (n)
\end{align*}
bei der die Indexzahlen mit den relativen Gewichten $\tilde w(i)$ gewichtet werden.\\
Es gibt zwei Arten, wie man den Preisindex berechnen kann, den Preisindex nach Laspeyres und den Preisindex nach Paasche

\subsubsection{nach Laspeyres}\label{PI nach Laspeyres}
Im Zähler des Preisindex steht das aufsummierte Produkt aus dem Preis der Berichtsperiode und der Menge aus der Basisperiode und im Nenner das aufsummierte Produkt aus Preis und Menge zur Basisperiode.
\begin{align*}
    P_{0t}^L =\frac{\sum_{i=1}^n p_t(i)q_0(i)}{\sum_{i=1}^n p_0(i)q_0(i)} =\frac{p_t'q_0}{p_0'q_0}
\end{align*}
Der Preisindex nach Laspeyres gibt an, wie sich der der Preis im Vergleich zur Basisperiode bei gleicher Menge (Menge zur Basisperiode) verändert hat. \\
Der Vorteil dieses Preisindex ist, dass man nach Erheben einer neuen Berichtsperiode sofort den Preisindex mit früher ermittelten Indizes ausrechnen kann. Zudem ist dieser leicht ermittelbar, da der Inhalt, also die Menge der jeweiligen Güter des Warenkorbs schon von früheren Untersuchungen bekannt sind, da man die Menge zur Basisperiode und nicht zur Berichtsperiode heranzieht.\\
Der Nachteil ist jedoch, dass sich der Warenkorb mit der Zeit veraltet, es kommen neue Produkte dazu und alte werden außen vorgelassen. Daher muss man immer wieder den Warenkorb in regelmäßigen Abständen aktualisieren.

\subsubsection{nach Paasche}
Im Vergleich zum Preisindex nach Laspeyres werden im Preisindex von Paasche die Mengen zur Berichtsperiode verwendet. Somit steht im Zähler des das aufsummierte Produkt aus Preis und Menge zur Berichtsperiode. Im Nenner steht das aufsummierte Produkt aus Preis zur Basisperiode und der Menge zur Berichtsperiode. 
\begin{align*}
    P_{0t}^P =\frac{\sum_{i=1}^n p_t(i)q_t(i)}{\sum_{i=1}^n p_0(i)q_t(i)} =\frac{p_t'q_t}{p_0'q_t}
\end{align*}
Der Preisindex nach Paasche gibt die Veränderung des Preises an, wenn man den Warenkorb aus der Berichtsperiode heranzieht. \\
Der Vorteil ist, dass der Warenkorb immer aktuell ist, da die Mengen durch jährliche Anpassung nie veralten. Es ist nicht so einfach den Preisindex mit alten Preisindizes zu vergleichen, da man unterschiedliche Warenkörbe benutzt hat. Somit kann man die Preisindizes nur vergleichen, wenn diese auf die neuen Warenkörbe umgestellt werden. Zudem muss man für jedes neue Berichtsjahr auch einen neuen Warenkorb zusammenstellen.

\subsection{Mengenindizes}
Man erhält den Mengenindize durch Vertauschen von Preis und Menge, dabei vergleicht man die Menge von Berichtsperiode zu Basisperiode bei gleichbleibendem Preis. Diese können zudem wieder nach Laspeyres und nach Paasche berechnet werden.\\

\subsubsection{nach Laspeyres}
Der Mengenindex von Laspeyres wird mit dem konstanten Preis der Basisperiode berechnet.
\begin{align*}
    Q_{0t}^L = \frac{p_0'q_t}{p_0'q_0}
\end{align*}
Der Mengenindex gibt an, wie sich der Wert des Warenkorbs verändert hat, bei gleichbleibenden Preisen aus der \textit{Basisperiode} und Veränderung der Mengen.

\subsubsection{nach Paasche}
Der Mengenindex nach Paasche wird mit konstanten Preisen aus der Berichtsperiode berechnet.
\begin{align*}
    Q_{0t}^P = \frac{p_t'q_t}{p_t'q_0}
\end{align*}
Der Mengenindex gibt an, wie sich der Wert des Warenkorbs verändert hat, bei gleichbleibenden Preisen aus der \textit{Berichtsperiode} und Veränderung der Mengen.

\subsection{Umsatzindex}
Beim Umsatzindex berechnet man im Zähler das Produkt aus Preis und Menge der Berichtsperiode und im Nenner das Produkt aus Preis und Menge zur Basisperiode.
\begin{align*}
    W_{0t}=\frac{p_t'q_t}{p_0'q_0}
\end{align*}
Der Umsatzindex gibt die Veränderung des Wertes des Warenkorbs in der Berichtsperiode im Verhältnis zum Wert des Warenkorbs aus der Basisperiode an.
Somit kann man direkt die Entwicklung des Umsatzes betrachten.

\subsection{Verknüpfung von Indizes}
Verknüpft man den Lespeyres-Preisindex mit dem Paasche-Mengenindex oder den Paasche-Preisindex mit dem Laspeyres-Mengenindex, so ergibt sich durch kürzen der Umsatzindex.

\subsection{Spezielle Probleme}
\subsubsection{Erweiterung des Warenkorbs}
In Kapitel \ref{PI nach Laspeyres} haben wir den Nachteil des Preisindex nach Laspeyres betrachtet, dass man den Warenkorb immer wieder aktualisieren muss.  Im Folgenden werden wir nun sehen, wie wir beim Preisindex von Laspeyres den Warenkorb erweitern können. \\
Die neue Ware, um die der Warenkorb erweitert werden soll, hat natürlich keinen so alten Preis zur Basisperiode wie die anderen Produkte, da es jetzt erst mit in den Warenkorb aufgenommen wird. Somit kann man den Preis im Basisjahr nicht mehr auf die alte Preise der Basisperiode beziehen, sondern verwendet den Preis zu dem Basisjahr, in dem die neue Ware zum ersten Mal gemessen wurde. Damit die neue Ware aufgenommen werden kann, braucht es somit zwei Preisindizes, einen zur Berichtsperiode $(p_{t'+1}(n+1))$ und einen zu Basisperiode $(p_{t'}(n+1))$, also muss die neue Ware schon mindestens zwei Jahre lang gemessen worden sein.

Somit berechnet man den erweiterten Preisindex zum einen für die alten Waren im Zähler mit dem Produkt aus dem Preis des neuen Berichtsjahres $(p_{t'+1})$ und den Mengen zur Basisperiode $q_0$  und addiert dieses mit dem Produkt aus dem Preis der neuen Ware zur Berichtsperiode $(p_{t'+1}(n+1))$ und der dazugehörigen Menge aus der Basisperiode der neuen Ware $(q_{t'}(n+1))$.
Im Nenner steht nun für die alten Waren das Produkt aus dem Preis zur neuen Basisperiode $(p'_{t'})$ und der Menge aus der alten Basisperiode $(q_0)$ und dieses Produkt wird erweitert durch das Produkt der neuen Ware aus dem Preis  $(p_{t'}(n+1))$ und der Menge $(q_{t'}(n+1))$ jeweils zur neuen Basisperiode.\\
Um nun den verketteten Preisindex nach Laspeyres $(P^L_{0,t'+1)})$ zu erhalten multipliziert man den Index des kleineren Warenkorbs $(P^L_{0,t'})$ mit dem Preisindex mit dem erweiterten Warenkorb $(P^L_{t',t'+1})$.

\subsubsection{Substitution einer Ware}
Man muss eine Substitution durchführen, wenn ein Produkt, das veraltet ist, durch ein neues ersetzen lässt. Ein Beispiel hierfür ist die Substitution des Schwarz-Weiß-Fernsehgerätes durch das Farbfernsehgerät.\\
\noindent Bei der Substitution nimmt man an, dass die gleiche Anzahl von Waren wie davor gekauft werden, jedoch kann sich der Preis ändern, weshalb man dann Stück für Stück den neuen Preis einführt. Nachdem man mindestens zweimal den Preisindex des Substituts gemessen hat, kann man den angepassten Preisindex hierfür berechnen. Man bildet den Quotienten aus der zweiten Messung und der vorangegangenen. Somit erhält man die Preisveränderung und multipliziert diesen mit dem Preis des alten, zu ersetzenden Produkts.

\subsubsection{Subindizes}
Große Warenkörbe bestehen oft aus kleineren Subkörben. Dies hilft dabei den Überblick über große Warenkörbe zu behalten. Da die Subkörbe unterschiedlich groß sein können, gehen diese gewichtet in den Preisindex ein. Dafür berechnet man zuerst den Gesamtumsatz aller Warenkörbe $U=\sum_{i=1}^n p_0(i)q_0(i)$ und berechnet dann jeweils den Anteil eines Subwarenkorbes am Gesamtumsatz. Dieser Anteil wird dann bei der Berechnung des Preisindex nach Laspeyres jeweils zu den Werten der Subkörbe multipliziert und die einzelnen Subkörbe addiert. So erhält man dann schlussendlich den Preisindex eines Warenkorbes mit mehreren Subkörben.

\clearpage

\subsection{Aufgaben}

\paragraph{1. Welche der folgenden Aussagen über Indexzahlen sind wahr?}

\begin{itemize}
    \item[a)] Zur Umbasierung (Veränderung Basisjahr) werden lediglich die Indexzahlen und nicht\\die Rohdaten benötigt. \hfill $\square$
    \item[b)] Zur Verkettung von Indexzahlen werden die Rohdaten benötigt \hfill $\square$
    \item[c)] Zur Umbasierung müssen sich alle bereits vorliegenden Indexzahlen auf das\\gleiche Basisjahr beziehen. \hfill $\square$
    \item[c)] Indexzahlen werden für Mengen und Preise getrennt berechnet. \hfill $\square$
\end{itemize}

\paragraph{2. Welche Aussagen bzgl. der verschiedenen Indizes sind wahr?}

\begin{itemize}
    \item[a)] Der Preisindex nach Laspeyres ist stets größer als der nach Paasche. \hfill $\square$
    \item[b)] Sind die Mengen in Berichts- und Basisperiode gleich, so sind die Preisindizes nach\\Laspeyres und Paasche ebenfalls identisch. \hfill $\square$
    \item[c)] Bei konstanten Preisen sind sowohl der Mengenindex nach Laspeyres als auch der\\nach Paasche gleich 1. \hfill $\square$
    \item[d)] Der Mengenindex nach Paasche gewichtet die Mengen mit den Umsatzanteilen aus\\der Berichtsperiode. \hfill $\square$
\end{itemize}

\paragraph{3. Welche Aussagen bzgl. "Spezieller Probleme" sind wahr?}

\begin{itemize}
    \item[a)] Bei der Substitution muss für mindestens eine Periode der Preis für beide Güter\\beobachtet werden. \hfill $\square$
    \item[b)] Bei der Erweiterung muss für das neue Produkt auch eine Menge in der Basisperiode\\bekannt sein. \hfill $\square$
    \item[c)] Bei der Substitution werden die Preissteigerungen des neuen Produkts einfach auf das\\alte übertragen. \hfill $\square$
    \item[d)] Subindizes können durch Gewichtung mit Mengenanteilen zu einem Gesamtindex\\kombiniert werden. \hfill $\square$
\end{itemize}

\newpage

\section{Zeitreihen}
Bei Zeitreihen beobachtet man ein Merkmal über einen längerer Zeit wiederholt und schaut, wie dieses Merkmal sich im Zeitverlauf entwickelt. Diese Entwicklung kann man in \textit{Kurvendiagrammen} darstellen. Dabei befindet sich auf der x-Achse die Zeit (bspw. Tageswerte, Monatswerte, Quartals -/oder Jahreswerte) und auf der y-Achse die Merkmalsausprägungen. Im Folgenden werden Methoden vorgestellt, wie man solche Kurvendiagramme analysieren kann. Dabei geht man von \textit{äquidistanten} Zeitreihen aus, was bedeutet, dass die Zeit zwischen zwei Messungen immer dieselbe ist.


\subsection{Zerlegung von Zeitreihen, Komponentenmodell}
Zeitreihen kann man in drei verschiedene Komponenten zerlegen, in die glatte Komponente $g_t$, die saisonale Komponente $s_t$ und die irreguläre oder Restkomponente $r_t$.
Die glatte Komponente $g_t$ gibt den Trend, also die langfristige Entwicklung an. Die saisonale Komponente $s_t$ beschreibt die saisonalen Schwankungen bspw. pro Quartal oder Monat und die Restkomponente gibt den Anteil an $y_t$ an, der nicht durch die saisonale und die glatte Komponente erklärt beschrieben wird. Diese sollte jedoch im Mittel 0 sein.\\
Somit kann das Komponentenmodell unter der Bedingung, dass die Summe von $r_t$ gleich 0 ist so dargestellt werden: $y_t=g_t+s_t+r_t$ . Dies wird additives Modell genannt. Da es oft besser ist einen multiplikativen Ansatz zu verfolgen erhält man diesen durch Logarithmieren: $\tilde{y_t}=\tilde{g_t}\cdot \tilde{s_t}\cdot\tilde{r_t}$, dabei ist dann wichtig, dass der Erwartungswert von $\tilde{r_t}$ 1 sein soll.


\subsection{Gleitende Durchschnitte}
Gleitende Durchschnitte benötigt man um den Trend, also die glatte Komponente zu berechnen. Durch die Glättung erhält man eine geringere Variabilität da man die saisonalen Schwankungen herausfiltert.
Man unterscheidet zwischen gerader und ungerader Ordnung.\\ 
Bei der ungeraden Ordnung mittelt man immer k Werte vor, k Werte nach einem speziellen Wert $y_t$ und den Wert $y_t$ selbst um einen glättenden Durchschnitt des Wertes $y_t$ zu erhalten. K gibt an wie viele Werte vor und nach dem Wert $y_t$ in den gleitenden Durchschnitt aufgenommen werden. Mit 2k+1 kann man dann die Ordnung bestimmen.
\\
Bei gerader Ordnung ist es ein klein wenig komplizierter, da man nun die Randwerte nur mit halbem Gewicht in den Durschnitt miteinberechnet, damit man auf eine gerade Zahl kommt. So hat man beispielsweise bei k=2 zum Schluss fünf Werte, die in den glatten Durchschnitt mit einberechnet werden. Jeweils die Randhäufigkeiten zur Hälfte, dann die jeweils direkten "Nachbarn" des Wertes $y_t$ und der Wert $y_t$ selbst. Somit kommt man schließlich auf 4 "ganze" Werte, also Rang 4. Um nun den gleitenden Durchschnitt zu berechnen muss man wie bei der ungeraden Ordnung das arithmetische Mittel aus den Werten bilden.\\
Je höher ein Rang ist, desto weniger gleitende Durchschnitte einer Zeitreihe kann man machen, da man jeweils an den Enden einer Zeitreihe keinen Durchschnitt bilden kann, da die vorigen bzw. folgenden Werte an den Enden fehlen.


\subsection{Saisonale Komponente, konstante Saisonfigur}
Gilt $s_t=s_{t+p}$, dann spricht man von einer konstanten Saisonfigur mit Periode p, da nach einer Periode p die Saisonfigur wieder den gleichen Wert wie den Anfangswert annimmt. Summiert man somit alle Werte einer Periode auf, sollte 0 herauskommen.

\subsection{Zerlegung in Trend und Saison}
Die saisionale Komponente ist eine regelmäßige Wiederholende Schwankung um die glatte Komponente. Wenn man daher die Ordnung des gleitenden Durchschnitts so wählt, dass sie einem Vielfachen der Periode entspricht ($2k=l\cdotp p (l=1,2,...))$, kann man die Saisonkomponente sozusagen "glätten", also herausrechnen.
Die kleinste mögliche Ordnung enstpricht somit der Periode, um die Saisonkomponente zu eliminieren.

Die Differenz zwischen dem vorigen Wert $y_t$ und dem durch den gleitenden Durchschnitt berechneten $y*_t$ mit also mit der bereinigten Saisonkomponente ist dann die Saisonkomponente selbst, jedoch mit zusätzlichem Fehler.
Denn bis jetzt ist $d_t\approx d_{t+p}$. 
Um den Fehler herausrechnen zu können, bildet man mit den berechneten Saisonkomponenten $d_t$ den Mittelwert pro Saisonteil. Also beispielsweise der Mittelwert von allen Werten im ersten Quartal, im zweiten Quartal, im dritten und vierten Quartal $(\bar{d}_1...\bar{d}_p)$ \\
Anschließend wird der Mittelwert von den soeben berechneten Mittelwerten berechnet, da man so schlussendlich den Fehler erhält, um den man die Saisonkomponente $(\bar{d}_1...\bar{d}_p)$ bereinigen muss. Nach dem man den Fehler von $\bar{d}$ subtrahiert hat, erhält man die Saisonkomponente $\hat{s}$.

Bildet man schlussendlich die Differenz aus der ursprünglichen Reihe $y_t$ und der geschätzten Saisonkomponente $\hat{s}$, so erhält man die saisonbereinigte Reihe, also nur noch den Trend.

\subsubsection{Trend und Saisonkomponente mit Regression}
Es gibt nicht nur lineare Trends, sondern beispielsweise auch quadratische Trends und exponentielles Wachstum, welches man mit der Regression berechnen kann.
In Kapitel \ref{sec:lineare Einfachregression} haben wir schon die lineare Einfachregression kennengelernt, mit der man hier die Trendkomponente der Form $g(t)=\hat{y}_t=\hat{a}+\hat{b}\cdot t$ schätzen kann.\\

\noindent Die Saisonkomponente kann man mit der KQ-Methode in der Form $y_t=\gamma_1s_1(t)+...+\gamma_{i}s_{i}(t)+e_t$ schreiben. Dabei wird die Regression verwendet um die $\gamma$'s zu schätzen. $s_j(t)$ ist eine Dummyvariable, um jeweils den Monat oder das Quartal berechnen zu können, welches gerade gefragt ist. Dies funktioniert wie in Kapitel \ref{sec:Kategoriale Regression}: für die Saisonkomponente die man berechnen möchte setzt man 1 ein und bei allen anderen 0.\\

\noindent Schlussendlich kann man die Trend -/und Saisonkomponente in der Form wie wir sie gerade besprochen haben in ein Modell zusammen bringen, das nennt sich dann Trend-Saison-Modell. 
\begin{align*}
    y_t=a+b\cdot t+\gamma_1s_1(t)+...+\gamma_{i}s_{i}(t)+e_t
\end{align*}
Je nachdem welche y man berechnen möchte kann man nun auf den linearen Trend noch die jeweilige Saisonkomponente dazu addieren oder abgezogen werden.

\subsection{Alternative Ansätze}
Es gibt noch viele weitere Ansätze wie man die Zeitreihen bestimmen kann unter anderem das lokale lineare Trendmodell, eine Einbeziehung von trigonometrische Funktionen, Census X-11 ARIMA, Census X-12 ARIMA, BV 4.1,.....








\clearpage

\subsection{Aufgaben}

\paragraph{1. Welche Aussagen bzgl. gleitender Durchschnitte sind wahr?}

\begin{itemize}
    \item[a)] Durch k wird festgelegt, ob der die Ordnung der gl. Durchschnitt gerade oder ungerade\\ist. \hfill $\square$
    \item[b)] Bei einem gl. Durchschnitt ungerader Ordnung fallen am Rand der Zeitreihe mehr\\Werte weg als bei gerader Ordnung. \hfill $\square$
    \item[c)] Je höher die Ordnung, desto mehr Werte fallen am Rand der Zeitreihe weg. \hfill $\square$
    \item[d)] Bei einem gl. Durchschnitt gerader Ordnung gehen (bei gleichem k) mehr Werte\\in die Berechnung mit ein als bei ungerader Ordnung. \hfill $\square$
\end{itemize}

\paragraph{2. Welche Aussagen bzgl. Zeitreihenmodellen sind wahr?}

\begin{itemize}
    \item[a)] Mit gleitenden Durchschnitten kann die Trendkomponente geschätzt werden. \hfill $\square$
    \item[b)] Mit gleitenden Durchschnitten kann die Saisonkomponente geschätzt werden. \hfill $\square$
    \item[c)] Jede Zeitreihe besitzt eine Trend- und eine Saisonkomponente. \hfill $\square$
    \item[d)] Welche Ordnung für die gleitenden Durchschnitte gewählt wird, hängt von der\\Saisonkomponente ab. \hfill $\square$
\end{itemize}

\clearpage

\hspace{0pt}
\vfill
\begin{center}
    {\Huge Statistik II}
\end{center}
\vfill
\hspace{0pt}

\clearpage


\section{Kombinatorik}
In der Kombinatorik stellt man sich hauptsächlich zwei Fragen: \\
Zum einen, wie viele verschiedene Möglichkeiten gibt es eine Menge anzuordnen, dabei berechnet man die Permutationen und zum anderen, wie viele Möglichkeiten gibt es, m Elemente aus n Elementen auszuwählen, dabei handelt es sich dann um die Kombination. Das heißt bei der Permutation nimmt man alle Elemente einer Menge und bei der Permutation nimmt man nur eine bestimmte Anzahl der Elemente einer Menge.\\
Dabei ist es wichtig den Begriff \textit{Mächtigkeit}, Anzahl der Elemente einer Menge M, formal dargestellt |M| zu kennen und ob Elemente \textit{geordnet}, also die Reihenfolge von Bedeutung ist oder nicht, dann sind sie \textit{ungeordnet}.

\subsection{Permutation}
Die Permutation gibt an, wie viele verschiedene Möglichkeiten es gibt die Elemente n einer Menge anzuordnen. Dabei spielt somit die Reihenfolge immer eine Rolle. Als Permutation bezeichnet man explizit eine mögliche Anordnung einer bestimmten Reihenfolge der Elemente n.\\
Man kann die Permutation unterteilen in \textit{mit Wiederholungen} und \textit{ohne Wiederholungen}.

\subsubsection{Permutation ohne Wiederholungen}
Bei Permutationen ohne Wiederholungen gibt es nur unterschiedliche Elemente, die der Reihenfolge nach angeordnet werden. Dabei gibt es auf dem erste Platz der Reihenfolge n verschiedene Möglichkeiten diesen zu besetzen, auf dem zweiten Platz nur noch n-1, da schon eines der Elemente auf dem ersten Platz sitzt und dieses nun nicht mehr auf dem zweiten Platz sein kann. So geht es weiter bis am Ende nur noch auf dem letzten Platz eine Möglichkeit noch besteht, nämlich das letzte noch übrige Element.\\
Wenn man diese verschiedenen Möglichkeiten auf den Plätzen multipliziert, dann erhält man schlussendlich die Anzahl verschiedener Anordnungen. Dies schreibt man dann als n! (ausgesprochen: n \textit{Fakultät}


\subsubsection{Permutation mit Wiederholungen}
Bei der Permutation mit Wiederholungen sind nicht mehr alle Elemente unterschiedlich. Daher gibt es hier nun weniger Permutationen, da es bei einem gleichen Element egal ist, ob die Elemente auf Platz 3 und 4 oder auf Platz 4 und 3 sind. Somit dürfen diese Anordnungen nicht doppelt gezählt werden, sondern müssen herausgerechnet werden.
Deshalb teilt man n! durch das Produkt der verschiedenen Gruppengrößen.
\begin{align*}
    \frac{n!}{n_1!\cdot n_2!\cdot...\cdot n_s!}
\end{align*}

\subsection{Kombination}
Bei der Kombination wähle ich aus einer Menge mit n Elementen m aus. Hierbei ist immer die Frage, ob eine Wiederholung möglich ist, also ob ich ein Element mehrmals aussuchen darf und ob die Reihenfolge bei der Auswahl eine Rolle spielt oder nicht. 

\subsubsection{Kombination ohne Wiederholung und ohne Reihenfolge}
Die Anzahl an möglichen Kombinationen berechnet man mit $\binom{n}{m}$. Da es keine Wiederholungen gibt ist in diesem Fall m immer kleiner als n. Im Taschenrechner kann man den Binomialkoeffizient, so heißt die Funktion, mit \textit{nCr} berechnen z.B. 24nCr3 = $\binom{24}{3}$.
Man kann den Binomialkoeffizient auch als Permutation mit Wiederholung umschreiben, da man zwei Gruppen hat, zum einen die Gruppe mit den gezogenen Elementen m und die Gruppe n-m  mit den nicht gezogenen Elementen. Somit ergibt sich: $\binom{n}{m}=\frac{n!}{m!\cdot(n-m)!}$

\subsubsection{Kombination ohne Wiederholung und mit Reihenfolge}
Hier spielt nun die Reihenfolge eine Rolle, jedoch darf ein gleiches Element nicht zweimal verwendet werden, da es ohne Wiederholung ist. Dies ist ein Mix aus \textit{Kombination ohne Wiederholung und ohne Reihenfolge} und der \textit{Permutation ohne Reihenfolge}, da der Binomialkoeffizient  mit der Fakultät der  Anzahl ausgewählter Elemente multipliziert wird.
Mächtigkeit an Zusammenstellungen: $\binom{n}{m}=\frac{n!}{(n-m)!}$

\subsubsection{Kombination mit Wiederholung und ohne Reihenfolge}
Nun wird die Reihenfolge nicht berücksichtigt, jedoch kann ein Element mehrmals gewählt werden. Somit kann nun auch die Anzahl der ausgewählten Elemente m größer sein als n. Zusätzlich zum Fall der Kombination ohne Wiederholung vergrößert sich jetzt die Menge um m-1 Elemente. Bei der Kombination wird daher die die Anzahl an möglichen Kombinationen durch Wiederholungen größer:$\binom{n+m-1}{m}=\frac{(n+m-1)!}{m!(n-1)!}$

\subsubsection{Kombination mit Wiederholung und mit Reihenfolge}
Hier gibt es nun die größte Anzahl an möglichen Kombinationen, da die Reihenfolge berücksichtigt wird und Wiederholungen möglich sind. Bei dieser Variante gibt es bei jeder Auswahl eines m, n verschiedene Wahlmöglichkeiten, deshalb berechnet man die Anzahl an Kombinationen mit $n^m$. \\
\paragraph\dangersign Bei der Permutation wird die Anzahl der möglichen Permutationen durch die Wiederholungen geringer im Gegensatz zur Kombination. Denn bei der Kombination wird die Anzahlen der möglichen Kombinationen durch Wiederholungen größer.
\clearpage
\subsection{Aufgaben}
\paragraph{1. Welche Aussagen bzgl. der Permutation sind wahr?}
\begin{itemize}
    \item[a)]  Die Reihenfolge der Elemente kann eine Rolle spielen muss sie aber nicht. \hfill $\square$
    \item[b)]Permutation wird eine mögliche Anordnung von Elementen in einer bestimmten\\ Reihenfolge genannt.  \hfill $\square$
    \item[c)] Die Permutation ohne Reihenfolge und ohne Wiederholung berechnet man mit n!\hfill $\square$
    \item[d)]  Die Anzahl der Permutation mit Wiederholung ist größer als ohne Wiederholungen. \hfill $\square$
\end{itemize}
\paragraph{2. Welche Aussagen zur Kombination sind richtig?}
\begin{itemize}
    \item [a)]Die Kombination ohne Wiederholung und ohne Reihenfolge hat eine größere Anzahl an \\ Kombinationsmöglichkeiten als die Kombination mit Wiederholungen und ohne \\Reihenfolge.\hfill $\square$
    \item[b)]Wenn die Reihenfolge bei der Kombination mit einbezogen werden soll, dann wird die \\Anzahl an möglichen Kombinationen größer.\hfill $\square$
    \item[c)]Die Mächtigkeit an Kombination mit Wiederholung und ohne Reihenfolge berechnet\\ man mit $\binom{n}{m}\cdot m!$. \hfill $\square$
    \item[d)]Bei der Kombination mit Wiederholungen und mit Reihenfolge gibt es auf dem \\"ersten Platz" n verschiedene Möglichkeiten, auf dem zweiten Platz n-1 und auf\\ dem letzten Platz eine Möglichkeit. \hfill $\square$
\end{itemize}
\paragraph{3. Ein Zahlenschloss besteht aus 4 Rädern mit den Zahlen von 0 bis 9. Welche Aussage ist richtig?}
\begin{itemize}
    \item[a)]Um die Anzahl der möglichen Kombinationen zu berechnen benutzt man die\\ Permutation mit Wiederholung.\hfill $\square$
    \item[b)]Um die Anzahl der möglichen Kombinationen zu berechnen benutzt man die\\ Kombination mit Wiederholung und mit Reihenfolge.\hfill $\square$
    \item[c)]Um die Anzahl der möglichen Kombinationen zu berechnen benutzt man die\\ Kombination mit Wiederholung und ohne Reihenfolge.\hfill $\square$
    \item[d)]Um die Anzahl der möglichen Kombinationen zu berechnen benutzt man die\\ Kombination ohne Wiederholung und ohne Reihenfolge.\hfill $\square$
    \item[e)]Es gibt 10000 verschiedene Kombinationen.\hfill $\square$
\end{itemize}
\paragraph{4. Bei einem Basketballspiel laufen nacheinander 5 Spieler auf das Spielfeld. Man berechnet die Anzahl der Möglichkeiten für das Einlaufen mit...}
\begin{itemize}
    \item[a)] der Kombination ohne Wiederholung und mit Reihenfolge.\hfill $\square$
    \item[b)] der Permutation ohne Wiederholung.\hfill $\square$
    \item[c)] der Permutation mit Wiederholung.\hfill $\square$
    \item[d)] der Kombination ohne Wiederholung und ohne Reihenfolge.\hfill $\square$
    \item[e)] der Kombination mit Wiederholung und mit Reihenfolge.\hfill $\square$
\end{itemize}
\paragraph{5. Bei einem Rosenzüchter gibt es 14 verschiedene Rosenarten und du möchtest ein Strauß mit 20 Rosen. Wie viele unterschiedliche Möglichkeiten an Sträußen gibt es?}
\begin{itemize}
    \item[a)] $2,432902008 × 10^{18}$\hfill $\square$
    \item[b)]573166440\hfill $\square$
    \item[c)] $8,366825543 × 10^{22}$\hfill $\square$
    \item[d)]$8,71782912 × 10^{10}$\hfill $\square$
\end{itemize}


\newpage
\section{Wahrscheinlichkeitsrechnung}
Als Erstes sollte man über ein paar Begriffe im Klaren sein, damit es im Folgenden verständlicher ist.\\
Ein \textit{Elementarereignis} ist ein Ereignis, deren Menge nur aus einem Element besteht. Bei einem Münzwurf ist somit das Werfen von zum Beispiel Kopf ein Elementarereignis {Kopf}\\
Ein \textit{zufälliges Ereignis} sind mehrere Elementarereignisse eines Zufallsexpermiments zusammen, also eine Menge von Elementarereignissen. Ein zufälliges Ereignis tritt dann ein, wenn mindestens ein Elementarereignis, das in der Menge enthalten ist, zutrifft. Beim Münzwurf wäre das {Kopf, Zahl}\\
Der \textit{Ereignisraum} bzw. der \textit{Grundraum} $\omega$ ist die Menge aller Elementarereignissen. Bei einem Münzwurf wäre das 2, da es die möglichen Elementarereignisse Kopf und Zahl gibt.\\
Ein \textit{unmögliches Ereignis} tritt dann auf, wenn keines der in der Menge enthaltenen Elementarereignissen zutrifft. Es kann daher bei keinem Zufallsexperiment auftreten.\\
Das \textit{sichere Ereignis} tritt immer ein, da alle möglichen Elementarereignisse in der Menge $\omega$ enthalten sind. Somit tritt das Ereignis eins Zufallsexperiments bei jeder Wiederholung ein. Beim Münzwurf wäre das sichere Ereignis, "entweder Kopf oder Zahl würfeln".
Ein \textit{Komplementärereignis} $\bar A$ tritt dann ein, wenn A nicht eintritt. Also $\bar A$ ist genau das Gegenteil oder "LKomplementär". Beim Münzwurf wäre "Zahl" das Komplementärereignis, wenn das zufällige Ereignis, {Kopf} wäre. Oder "ungerade" Zahlen sind das Komplementärereignis, wenn "gerade" Zahlen das zufällige Ereignis wären.\\

Zufällige Ereignisse können nicht nur alleine auftreten und da diese Mengen von Elementarereignisse sind, benötigt man Menegenoperation.

\textbf{$A\cap B$} Dies ist die Schnittmenge der beiden Mengen A und B. Das heißt die Elementarereignisse, die in A \textit{und} auch in B auftreten.\\
\textbf{$A\cup B$} Dies ist die Vereinigungsmenge. Das heißt tritt das Ereignis A \textit{oder} B oder beide auf, dann tritt die Vereinigungsmenge auf. Es werden sozusagen beide Mengen addiert, wobei Elementarereignisse, die in  beiden Mengen vorkommen, nur einmal gezählt werden dürfen.\\

\textbf{$\bar A$} Das zufällige Ereignis \textit{A quer} tritt genau dann auf, wenn ein Elementarereignis nicht in der Menge der Elemtentarereignissen des zufälligen Ereignisses A liegt.\\

\textbf{A \ B}Das Ereignis \textit{A ohne B} tritt dann auf, wenn ein Elementarereignis in A liegt, jedoch nicht in B.

\textbf{disjunktes Ereignis} Ein disjunktes Ereignis sind zwei Mengen, die keine gleichen Elementarereignisse haben, das heißt sie können niemals gleichzeitig auftreten. Somit gibt es keine Schnittmenge der beiden Mengen, daher ist $A\cup B=\emptyset$.


\subsection{Relative Häufigkeit}
Um eine Quantifizierung vorzunehmen, wie häufig ein Ergebnis zu erwarten ist bzw. wie wahrscheinlich der Ausgang eines Versuchsergebnisses ist, schaut man sich die relative Häufigkeit an. Dabei gewinnt man die absolute Häufigkeit durch n-fache unabhängige Wiederholung des zufälligen Versuches.
Betrachtet man ausreichend viele Wiederholungen des zufälligen Ereignisses A, dann nähert sich die relative Wahrscheinlichkeit dem typischen Wert von A. Dies wird als \textit{Wahrscheinlichkeit} des Ereignisses A bezeichnet und wird als P(A) geschrieben.

\subsection{Laplacesche Wahrscheinlichkeit}
Damit ein Laplacesche Wahrscheinlichkeit vorliegt, müssen zwei Voraussetzungen erfüllt sein, zum einen muss die Ergebnismenge endlich sein und zum anderen müssen die Ereignisse die gleiche Wahrscheinlichkeit besitzen. Wenn dies der Fall ist, dann bildet der Quotient aus der Anzahl der für A günstigen Fälle, also in den Fällen, in denen A zutrifft und der Anzahl aller möglichen Fälle die Laplace-Wahrscheinlichkeit $\frac{|A|}{|\omega|}=P(A)$.
Der Würfelwurf hat beispielsweise eine Laplacesche Wahrscheinlichkeit, da jede Zahl die gleiche Wahrscheinlichkeit von $\dfrac{1}{6}$ hat und die Ergebnismenge auf die Zahlen von 1-6 beschränkt ist.

\subsection{Wahrscheinlichkeitsrechnung}
A.N.Kolmogorov hat das Axiomensystem der Wahrscheinlichkeitsrechnung erfunden. Dieses bietet die formale Grundlage für die Wahrscheinlichkeit und besteht aus drei Axiomen.
Das erste Axiom besagt, dass jedem zufälligen Ereignis eines zufälligen Versuchs eine Wahrscheinlichkeit zwischen 0 und 1 zugeordnet wird.\\
Das zweite Axiom besagt, dass das sichere Ereignis, welches wir zuvor schon kennen gelernt haben, immer die Wahrscheinlichkeit von 1 hat.\\
Das dritte Axiom sagt, dass zwei disjunkte Ereignisse in der Vereinigungsmenge einfach addiert werden können.
Aus diesen Axiomen lassen sich 5 Folgerungen ableiten. Als erstes ist die Wahrscheinlichkeit des komplementären Ereignisses $\bar A$ eins minus die Wahrscheinlichkeit für A.\\
Als zweite Folgerung ist die Wahrscheinlichkeit des unmöglichen Ereignisses 0, da die Wahrscheinlichkeit des sicheren Ereignisses eins ist und dies das Komplementär zum unmöglichen Ereignis ist.\\
Die Wahrscheinlichkeit von zwei sich gegenseitig nicht ausschließenden, also nicht disjunkten Ereignissen $A_1$ und $A_2$ ist die Summe der beiden Einzelwahrscheinlichkeiten, abzüglich der Schnittmenge der beiden Ereignisse. 
\dangersign Hier muss zusätzlich einmal die Schnittmenge abgezogen werden, da sonst die Schnittmenge doppelt in die Wahrscheinlichkeit mit einberechnet werden würde. Dies ist anders bei zwei disjunkten Ereignissen, da diese keine gemeinsamen Schnittmenge haben und somit auch diese nicht wegen Doppelberechnung abgezogen werden muss. Somit ist die Wahrscheinlichkeit bei zwei disjunkten Ereignissen die Summe der beiden Einzelwahrscheinlichkeiten.\\
Die vierte Folgerung besagt, wenn A eine Teilmenge von B ist, dann ist die Wahrscheinlichkeit von A geringer als von B. \\
Als letzte Schlussfolgerung ergibt sich, wenn man ein Ereignisraum vollständig in mehrere disjunkte Teile $A_1,....,A_n$ zerlegt, und ein Ereignis B innerhalb des Ereignisraumes liegt, dann berechnet sich ein beliebiges Ereignis als Summe aller Schnittmengen zwischen B und einem zerlegten Teil $A_i$.

\subsection{Bedingte Wahrscheinlichkeit}

\newpage
\section{Zufallsexpermieent}

In der deskriptiven Statistik (Statistik I) haben wir fest vorgegebenes Datenmaterial beschrieben. Im Gegensatz dazu gehen wir in der induktiven Statistik (Statistik II) von Zufallsexperimenten mit sogenannten Zufallsvariablen aus. Mit Zufallsvariablen können Ergebnisse eines noch nicht durchgeführten Zufallsexperiments beschrieben werden.\\
\noindent \textbf{Beispiel}:\\
Roulette Spiel mit der Zufallsvariable Z ("groß Z"): Bevor wir das Roulette drehen, ist der Wert von Z nicht bekannt. Nach einer Runde nimmt die Variable Z aber einen Wert zwischen $0,1,2,...,36$ an. Dieser Wert heißt auch Realisierung der Zufallsvariable und wird mit z ("klein z") beschrieben.

\noindent Grundsätzlich gibt es zwei verschiedene Arten von Zufallsvariablen -- \textbf{diskrete} und \textbf{stetige}. 

\subsection{Diskrete Zufallsvariablen} \label{sec:DZV}
Wenn Zufallsvariablen nur eine endliche oder abzählbar unendliche Menge an Werten annehmen, spricht man von diskreten Zufallsvariablen. Das bedeutet grob, dass es z.B. eine fixe Anzahl an Werten gibt (wie z.B. beim Roulette-Spiel oder Würfelwurf), oder dass es sich um sogennante Zähldaten handelt, wie etwa die Anzahl an Versicherungsschäden an einem bestimmten Tag. Beachtet jedoch, dass theoretisch beliebig hohe Werte möglich sind, diese jedoch abzählbar sein müssen. 

\subsubsection{Abzählbarkeit: Mächtigkeit von Mengen} \label{sec:Mengen}
Für Interessierte gibt es hier einen kleinen Ausflug in die Mathematik:\\ \url{https://de.wikibooks.org/wiki/Mathe_f\%C3\%BCr_Nicht-Freaks:_M\%C3\%A4chtigkeit_von_Mengen}


\subsection{Stetige Zufallsvariablen} \label{sec:SZV}
Stetige Zufallsvariablen sind innerhalb eines beliebigen Intervalls definiert und können unendlich viele Werte annehmen. Man nennt diesen Wertebereich überabzählbar unendlich.\\
\noindent \textbf{Beispiel}:\\
Geschwindigkeit von Autos bei einer Radarkontrolle. Hier sind theoretisch unendlich viele Werte zwischen z.B. 50km/h und 60km/h möglich. Obwohl natürlich die Messgenauigkeit durch das verwendete Messgerät beschränkt ist (z.B. auf 1km/h genau), werden solche Variablen in der Statistik also meist als stetige Variablen behandelt.

\subsection{Träger einer Zufallsvariablen} \label{sec:Traeger}

Der Träger einer Zufallsvariablen bezeichnet alle möglichen Ergebnisse einer Zufallsvariablen. Beim Roulette-Spiel wäre der Träger z.B. $ T = 0,1,2,...,36$. Für die Geschwindigkeit bei der Radarkontrolle kommen theoretisch alle positiven reellen Zahlen in Frage, hier wäre der Träger also die Menge der reellen Zahlen $\R^+$.

\newpage

\section{Spezielle Verteilungen} \label{sec:Distr}

\subsection{Diskrete Verteilungen} \label{sec:discretedistr}
\textbf{Diskret} heißt grob gesagt, dass ein Experiment eine endliche Zahl an möglichen Ergebnissen hat. Beispiele für diskrete Verteilungen in der Vorlesung sind die diskrete Gleichverteilung, Bernoulliverteilung, Binomialverteilung, (hyper--)geometrische Verteilung, Poissonverteilung und die Multinomialverteilung. Diese diskreten Verteilungen beschreiben Wahrscheinlichkeiten, mit denen die einzelnen Werte von diskreten Zufallsvariablen (=ZV) auftreten können. Diese ZV besitzt abzählbar viele Werte (Beispiel Würfelwurf mit den Zahlen 1--6).


\subsubsection{Diskrete Gleichverteilung} \label{sec:duni}

\noindent \textbf{Idee/Anwendung}: Modellierung einer diskreten ZV, deren mögliche Ausprägungen alle mit derselben Wahrscheinlichkeit auftreten. \\

\noindent Ein Beispiel dafür ist das Würfeln mit einem fairen Würfel. \textit{Fair} bedeutet in diesem Fall, dass jede Augenzahl (das sind die möglichen Ausprägungen) mit derselben Wahrscheinlichkeit vorkommt. \\

\noindent \textbf{Verteilung}: 
\begin{itemize}
\item[] Wahrscheinlichkeitsfunktion: $P(X=x_i)=p_i=\frac{1}{k}, \hspace{0.2cm} i=1,2,...,k$
	\begin{itemize}
	\item[$\rightarrow$] Hier ist die Wahrscheinlichkeitsfunktion an der Stelle $x_i$ einfach die Wahrscheinlichkeit, dass die ZV den Wert $x_i$ annimmt. Und diese Wahrscheinlichkeit ist für alle $x_i$, $i=1,2,...,k$, gleich und hat dementsprechend jeweils den Wert $\frac{1}{k}$.
	\end{itemize}
\item[] Erwartungswert und Varianz: $\E[X]=\frac{k+1}{2}$ und $Var[X]=\frac{1}{12}(k^2-1)$
\end{itemize}
\noindent \textbf{Bemerkung}: $k$ ist die Anzahl der möglichen Ausprägungen, d.h. die Mächtigkeit des Ereignisraums

\subsubsection{Bernoulliverteilung} \label{sec:Ber}

\noindent \textbf{Idee/Anwendung}:  Ein einziges Experiment mit nur zwei möglichen Ergebnissen, wobei wir 0 für "Misserfolg" und 1 für "Erfolg" kodieren. Erfolg tritt mit Wahrscheinlichkeit $p$ und Misserfolg mit der entsprechenden Gegenwahrscheinlichkeit $(1-p)$ auf. Formell: $P(X=1)=p$ und $P(X=0)=1-p$. \\

\noindent Ein Beispiel ist der einfache Münzwurf einer fairen Münze. Hierbei spielt es keine Rolle, ob man Kopf oder Zahl als Erfolg (Misserfolg) wählt, es sei denn, man hat eine weitere, persönliche Interpretation der Ereignisse. Wenn man z.B. in einer Wette auf Kopf setzt, ist es sinnvoll, dem Ereignis $Kopf$ den Wert $1$, also "Erfolg", zuzuweisen. \\

\noindent \textbf{Verteilung}: 
\begin{itemize}
\item[] Notation: $X\sim B(1,p)$
\item[] Wahrscheinlichkeitsfunktion: $P(X=x) = \begin{cases}
			p & \text{für x = 1,}\\
            1-p & \text{für x = 0}
		 \end{cases}$ 
\item[] \hspace{4.5cm} bzw. $P(X=x)=p^x(1-p)^{1-x}$ für $x \in \{0,1\}$
	\begin{itemize}
	\item[$\rightarrow$] Wahrscheinlichkeitsfunktion nimmt für beide möglichen Werte (es handelt sich ja um eine binäre ZV) die entsprechende Eintrittswahrscheinlichkeit an. Ansonsten ist sie nicht definiert (was für uns so viel bedeutet, dass sie dort einfach "0" ist).
	\end{itemize}
\item[] Erwartungswert und Varianz: $\E[X]=p$ und $Var[X]=p(1-p)$

\end{itemize}
\noindent \textbf{Bemerkung}: Binomialverteilung ($B(n,p)$) mit $n=1$.

\subsubsection{Binomialverteilung} \label{sec:Bin}

\noindent \textbf{Idee/Anwendung}: Die Binomialverteilung entsteht durch mehrmaliges ($n$--maliges) Wiederholen desselben Bernoulli-Experiments. Außerdem gilt, dass diese Wiederholungen unabhängig voneinander sind. Von Interesse ist hierbei die gesamte Anzahl der Erfolge, nicht jedoch die konkrete Abfolge von Erfolgen und Misserfolgen. \\

\noindent Ein Beispiel ist der wiederholte Münzwurf. Angenommen, man ist daran interessiert, wie oft das Ereignis $Kopf$ bei 100 Münzwürfen vorkommt. Dann wählt man $Kopf$ als "Erfolg" im einfachen Bernoulli-Experiment, d.h. man weist $Kopf$ den Wert 1 zu. Die entsprechende Wahrscheinlichkeit bei einer fairen Münze ist $P(X=1)=0,5$. Da man an der Anzahl von $Kopf$ bei 100 Münzwürfen interessiert ist, setzt man nun noch $n=100$ in der Binomialverteilung. \\

\noindent \textbf{Verteilung}:
\begin{itemize}
\item[] Notation: $X\sim B(n,p)$
\item[] Wahrscheinlichkeitsfunktion: $P(X=k)=\binom{n}{k}p^k(1-p)^{n-k}$, $k=0,1,...,n$
	\begin{itemize}
	\item[$\rightarrow$] Die Wahrscheinlichkeitsfunktion lässt sich intuitiv in ihre einzelnen Faktoren zerlegen:
		\begin{itemize}
		\item[] $\binom{n}{k}$: Dies ist die Anzahl der möglichen Kombinationen, bei denen genau $k$ Erfolge bei $n$ Versuchen eintreten. Der Binomialkoeffizient (Ziehen ohne Zurücklegen und ohne Reihenfolge) wird benutzt, weil uns die Stellen, an denen der Erfolg eintritt, egal sind. So kann z.B. in den ersten $k$ versuchen Erfolg eintreten und in den letzten $n-k$ nicht. Genauso könnte auch in den ersten $n-k$ Versuchen kein Erfolg eintreten, dafür aber in den letzten $k$.
		\item[] $p^k$: Wenn wir uns unabhängige Ereignisse anschauen, multiplizieren wir einfach die entsprechenden Wahrscheinlichkeiten für das, was eintreten soll, auf. $p^k$ ist das Produkt der $k$ Erfolgswahrscheinlichkeiten für die $k$ Erfolge.
		\item[] $(1-p)^{n-k}$: Da außerdem $n-k$ Misserfolge eintreten, müssen diese mit $n-k$ Misserfolgswahrscheinlichkeiten, gesammelt in dem Faktor $(1-p)^{n-k}$, berücksichtigt werden.
		\end{itemize}
	\end{itemize}
\item[] Erwartungswert und Varianz: $\E[X]=np$ und $Var[X]=np(1-p)$

\end{itemize}

\noindent \textbf{Bemerkung}: Kann durch $\mathcal{N}(np,np(1-p))$ approximiert werden (vgl. Kapitel \ref{sec:BinN}).

\subsubsection{Geometrische Verteilung} \label{sec:Geo}

\noindent \textbf{Idee/Anwendung}: Die geometrische Verteilung modelliert die \textit{Anzahl der Versuche} bis zum ersten Eintreten des Erfolgs bei unabhängigen (identischen) Bernoulliversuchen. \\

\noindent Ein Beispiel ist hier wieder der wiederholte Münzwurf. Angenommen, man ist daran interessiert, wie oft man eine faire Münze werfen muss, bis das Ereignis $Kopf$ vorkommt, dann wählt man erneut $Kopf$ als "Erfolg" im einfachen Bernoulli-Experiment. Da die Wahrscheinlichkeit $P(X=1)=0,5$ ist, erwartet man in diesem Fall, dass zwei Versuche genügen, um den ersten Erfolg zu erzielen. \\

\noindent \textbf{Verteilung}:
\begin{itemize}
\item[] Notation: $X\sim G(p)$
\item[] Wahrscheinlichkeitsfunktion: $P(X=k)=p(1-p)^{k-1}$, $k \in \mathbb{N}$
\item[] \hspace{4.5cm} mit $P(X \leq k)=1-(1-p)^k$
	\begin{itemize}
	\item[$\rightarrow$] Die Wahrscheinlichkeitsfunktion lässt sich intuitiv in ihre einzelnen Faktoren zerlegen:
		\begin{itemize}
		\item[] $(1-p)^{k-1}$: Es geht hier um die Wahrscheinlichkeit, genau im $k$--ten Versuch den ersten Erfolg zu erzielen. Dementsprechend müssen bis dahin $k-1$ Misserfolge aufgetreten sein. Diese $k-1$ Misserfolge (in \textit{unabhängigen} Bernoulli-Versuchen) treten genau mit der Wahrscheinlichkeit $\underbrace{(1-p)\cdot(1-p)\cdot ... \cdot (1-p)}_{k-1 \hspace{0.2cm} mal} = (1-p)^{k-1}$ auf.
		\item[] $p$: Dann fehlt nur noch die Wahrscheinlichkeit, dass im $k$--ten Versuch der Erfolg auch eintritt. Dafür wird dann einmal mit der Erfolgswahrscheinlichkeit $p$ multipliziert.
		\end{itemize}
	\end{itemize}
\item[] Erwartungswert und Varianz: $\E[X]=\frac{1}{p}$ und $Var[X]=\frac{1}{p}\Big(\frac{1}{p}-1\Big)$

\end{itemize}

\noindent \textbf{Bemerkung}: 

\subsubsection{Hypergeometrische Verteilung} \label{sec:hypergeo}

\noindent \textbf{Idee/Anwendung}: $n$-maliges Ziehen (ohne Zurücklegen!) aus einem Topf mit $N$ Kugeln, von denen $M$ ($ \leq N$) Kugeln das gewünschte Merkmal tragen. Es ist hierbei egal, ob die anderen $N-M$ Kugeln alle dasselbe Merkmal oder verschiedene Merkmale tragen. Die Anzahl der gezogenen Kugeln mit gewünschtem Merkmal nach $n$--maligem Ziehen folgt dann einer hypergeometrischen Verteilung. \\

\noindent Ein Beispiel ist hier das Lottospielen. Es gibt ingesamt $N=49$ Kugeln, von denen $M=6$ das Merkmal \textit{Auf Lottoschein angekreuzt} tragen und die anderen $N-M=43$ nicht. Die Anzahl der richtigen Zahlen im Lotto bei $n=6$ Zügen ist entsprechend hypergeometrisch verteilt ($H(6,6,49)$). \\

\noindent \textbf{Verteilung}:
\begin{itemize}
\item[] Notation: $X\sim H(n,M,N)$
\item[] Wahrscheinlichkeitsfunktion: $P(X=x)=\frac{\binom{M}{x}\cdot\binom{N-M}{n-x}}{\binom{N}{n}}$
	\begin{itemize}
	\item[$\rightarrow$] Die Wahrscheinlichkeitsfunktion lässt sich intuitiv in ihre einzelnen Faktoren zerlegen, wobei im Zähler die Anzahl der günstigen Fälle und im Nenner die Anzahl der ingesamt möglichen Fälle stehen:
		\begin{itemize}
		\item[] $\binom{M}{x}$: Der erste Binomialkoeffizient steht für alle möglichen Kombinationen, von den $M$ Einheiten mit dem gewünschten Merkmal genau $x$ zu ziehen.
		\item[] $\binom{N-M}{n-x}$: Bei $N$ Einheiten und $M$ mit gewünschten Merkmal bleiben $N-M$ weitere Einheiten übrig. Aus diesen sollen noch $n-x$ Einheiten stammen, also die Anzahl der Züge minus der gewünschten Anzahl der Einheiten mit dem Merkmal (es sind ja bereits $x$ Züge "verbraucht").
		\item[] $\binom{N}{n}$: Das Produkt der oberen beiden Binomialkoeffizienten ergibt die Anzahl der günstigen Fälle. Da wir eine Wahrscheinlichkeit berechnen, muss diese noch durch die Anzahl der möglichen Fälle geteilt werden. Und diese Gesamtzahl sind einfach alle Möglichkeiten, $n$--mal aus $N$ zu ziehen. 
		\end{itemize}
	\end{itemize}
\item[] Erwartungswert und Varianz: $\E[X]=n\frac{M}{N}$ und $Var[X]=n\frac{N}{M}\Big(1-\frac{M}{N} \Big)\Big(\frac{N-n}{N-1} \Big)$

\end{itemize}

\noindent \textbf{Bemerkung}: Kann durch $B\Big(n,\frac{M}{N}\Big)$ approximiert werden (vgl Kapitel \ref{sec:HypBin}).

\subsubsection{Poissonverteilung} \label{sec:Poi}
\textbf{Idee/Anwendung}: Mit der Poissonverteilung kann man die Anzahl von Ereignissen in einem gegebenen Zeitintervall modellieren. Man zählt also die Anzahl der Ereignisse, die in einem fest vorgegebenen Zeitintervall eintreten, und möchte die Wahrscheinlichkeiten modellieren, mit der genau $x$ Ereignisse in diesem Zeitraum auftreten. Die Länge des Zeitintervalls kann dabei je nach Anwendung unterschiedlich gewählt werden (z.B. 1 Sekunde, 1 Minute, 1 Stunde, 1 Tag oder auch 1 Jahr). \\

\noindent Beispiele sind die \textbf{Anzahl} von Fischen die \textbf{täglich} unter einem Sensor in der Isar vorbeischwimmen oder \textbf{Anzahl} von Haftpflichtfällen \textbf{pro Jahr} eines Versicherungsnehmers. \\

\noindent \textbf{Verteilung}:
\begin{itemize}
\item[] Notation: $X\sim Po(\lambda)$, $\lambda > 0$
\item[] Wahrscheinlichkeitsfunktion: $P(X=x)=\frac{\lambda^x}{x!}\cdot exp(-\lambda)$, $x \in \mathbb{N}_0$
\item[] Erwartungswert und Varianz: $\E[X]=\lambda$ und $Var[X]=\lambda$

\end{itemize}

\noindent \textbf{Bemerkung}: Kann durch $\mathcal{N}(\lambda,\lambda)$ approximiert werden (vgl. Kapitel \ref{sec:PoN}). Außerdem gilt, dass die Wartezeit bis zum Eintreten des ersten Ereignisses exponentialverteilt mit Parameter $\lambda$ ist, wenn die Anzahl der Ereignisse in einem fixem Kontinuum (i.d.R. Zeitraum) der Poissonverteilung mit Parameter $\lambda$ folgt (vgl. Exponentialverteilung, Kapitel \ref{sec:Expo}). 

\subsubsection{Multinomialverteilung} \label{sec:Multi}

\textbf{Idee/Anwendung}: Allgemein stellt die Multinomialverteilung eine Erweiterung der Binomialverteilung auf mehr als zwei mögliche Ereignisse dar. Man kann sie sich als Verteilung für das Ziehen mit Zurücklegen aus einer Urne mit $k$ Sorten Kugeln vorstellen. Jede dieser verschiedenen Sorten hat eine individuelle Wahrscheinlichkeit $p_i$, $i=1,2,...,k$, mit der sie in der Urne vorkommt. Diese Wahrscheinlichkeiten sind einfach nur die relativen Häufigkeiten. Mit der Wahrscheinlichkeitsfunktion berechnet man die Wahrscheinlichkeit für das Eintreten gewisser Anzahlen $x_i$, $i=1,2,...,k$. Uns ist hierbei die Gesamtzahl der Kugeln in der Urne egal, da wir mit Zurücklegen ziehen\footnote{vgl. \url{https://de.wikipedia.org/wiki/Multinomialverteilung}}. \\

\noindent Seien zum Beispiel $k=10$ und $p_i=0,1$ , $i=1,2,...,10$, dann ist es egal, ob es insgesamt 100 Kugeln gibt und jede Sorte zehnmal vorkommt oder 1000 Kugeln und jede Sorte 100-mal. Da wir mit Zurücklegen ziehen, ändern sich die Wahrscheinlichkeiten bei den verschiedenen Zügen nicht. \\

\noindent \textbf{Verteilung}:
\begin{itemize}
\item[] Notation: $X \sim M(n;p_1,p_2,...,p_k)$
\item[] Wahrscheinlichkeitsfunktion: $P(X_1=x_1,X_2=x_2,...,X_k=x_k)=\frac{n!}{x_1! \cdot x_2! \cdot ... \cdot x_k!} \cdot p_1^{x_1} \cdot p_2^{x_2} \cdot ... \cdot p_k^{x_k}$
\item[] \hspace{4.5cm} mit $\sum_{i=1}^kp_i=1$ und $\sum_{i=1}^kx_i=n$ (vgl. \footnote{Die erste Summe ist die übliche Bedingung, die wir an Wahrscheinlichkeiten stellen} und \footnote{Wenn wir $n$--mal ziehen, muss die Gesamtzahl der Ereignisse natürlich gleich $n$ sein; das stellt die zweite Summe dar})
	\begin{itemize}
	\item[$\rightarrow$] Die Wahrscheinlichkeitsfunktion lässt sich intuitiv in ihre einzelnen Faktoren zerlegen:
		\begin{itemize}
		\item[] $\frac{n!}{x_1! \cdot x_2! \cdot ... \cdot x_k!}$:  "Permutationen mit Wiederholung", da es $k$ unterscheidbare Ausprägungen, die mit einer entsprechenden Häufigkeit $x_i$, $i=1,2,...,k$, vorkommen sollen, gibt.  
		\item[] $p_i^{x_i}$: Die Wahrscheinlichkeit, dass Ausprägung $i$ drankommt ist $p_i$. Nun soll Ausprägung $i$ genau die Anzahl $x_i$ haben. Deshalb nehmen wir wieder das Produkt  $\underbrace{p_i\cdot p_i \cdot ... \cdot p_i}_{x_i \hspace{0.2cm} mal} = p_i^{x_i}$ und zwar für alle $i=1,2,...,k$.
		\end{itemize}
	\end{itemize}
\item[] Erwartungsvektor und Kovarianzmatrix\footnote{Für die vollständige Kovarianzmatrix siehe Vorlesung Slide 13.36}: $\E[X]=(np_1,np_2,...,np_k)^{\top}$ und
\item[] \hspace{6.5cm} $Cov[X_i,X_j]=\begin{cases}
			np_i(1-p_i) & \text{für i = j,}\\
            -np_ip_j & \text{für i $\neq$ j}
		 \end{cases}$ 

\end{itemize}

\noindent \textbf{Bemerkung}: 


\subsection{Stetige Verteilungen} \label{sec:contdistr}
\textbf{Stetig} heißt grob gesagt, dass ein Experiment unendliche viele Zahlen an möglichen Ergebnissen hat. Beispiele aus der Vorlesung sind die stetige Gleichverteilung, Exponentialverteilung, Normalverteilung, $\chi^2$-Verteilung, t-Verteilung und die F-Verteilung. Diese basieren auf stetigen Zufallsvariablen. Die Menge der Werte, die diese stetigen ZV annehmen können ist unendlich und nicht zählbar (Beispiel: das Intervall [1,6]). 

\subsubsection{Stetige Gleichverteilung}\label{sec:Uni}

\textbf{Idee}: Die möglichen Ausprägungen der ZV liegen auf einem fixen Invervall [a;b] (z.B. [0;1]). Wichtig ist, dass die ZV in diesem Intervall jeden möglichen Wert, also überabzählbar unendlich viele Werte, annehmen kann. In [0;1] kann die Zahl $0,5$ genauso vorkommen wie die Zahl $0,4358984$. Es sind also beliebig viele Nachkommastellen und Kombinationen dieser möglich, also auch beliebig viele Werte. \\

\noindent Wie bei der diskreten Gleichverteilung hat hier jede mögliche Ausprägung dieselbe Wahrscheinlichkeit. Da es nun unendlich viele Werte $k$ gibt, hat jeder einzelne Wert die Wahrscheinlichkeit: $\lim\limits_{k \rightarrow +\infty}{\frac{1}{k}}=0$. Deshalb trifft man in der Regel Aussagen darüber, mit welcher Wahrscheinlichkeit die ZV in einem bestimmten Intervall landet. Zum Beispiel landet eine ZV, die der stetigen Gleichverteilung auf dem Intervall [0;1] folgt, zu 50 \% im Intervall [0;0,5], was genau die Hälfte des Ereignisraums ist.

\noindent \textbf{Verteilung}:
\begin{itemize}
\item[] Notation: $X \sim U(a,b)$
\item[] Dichtefunktion\footnote{Zur Erinnerung: Bei stetigen ZVs gibt es eine Dichte-, bei diskreten ZVs eine Wahrscheinlichkeitsfunktion} : $f(x)=\begin{cases}
			\frac{1}{b-a} & \text{für $x \in [a;b]$}\\
            0 & \text{sonst}
		 \end{cases}$ 
	\begin{itemize}
	\item[$\rightarrow$] Die Dichtefunktion näher erklärt:
		\begin{itemize}
		\item[] Die Dichtefunktion ist in gewissermaßen das stetige Pendant zu Wahrscheinlichkeitsfunktion. Deshalb mag es erstmal komisch anmuten, dass sie positive Werte an allen möglichen Ausprägungen hat, obwohl diese jeweils mit Wahrscheinlichkeit 0 vorkommen. Wenn wir uns aber daran erinnern, dass wir für die Berechnung von der Wahrscheinlichkeit, dass X in einem bestimmten Intervall landet, Integrale benutzen, ergibt das ganze mehr Sinn. \\
		\noindent Da X mit Wahrscheinlichkeit 1 im Intervall $[a;b]$ landet, muss gelten: $\int_{a}^{b} f(x)dx = 1$. Da die Dichtefunktion hier nicht unterschiedlichen Werten im Ereingnisraum $[a;b]$ verschiedene Werte zuweisen darf, erhalten alle denselben Wert $\frac{1}{b-a}$. D.h. auf dem Intervall $[a;b]$ ist die Dichte eine horizontale Linie auf Höhe des y-Wertes $\frac{1}{b-a}$ (und sonst 0). Mit dem Wert $\frac{1}{b-a}$ erhalten wir aber auch, dass die Fläche unter der Dichte (also das Integral; hier ein Rechteck) genau 1 ergibt, da die Breite (auf der x-Achse) $b-a$ beträgt und die Höhe des Rechtecks eben genau $\frac{1}{b-a}$ ist.
		\end{itemize}
	\end{itemize}
\item[] Erwartungswert und Varianz: $\E[X]= \frac{a+b}{2}$ und $Var[X]=\frac{(b-a)^2}{12}$

\end{itemize}

\noindent \textbf{Bemerkung}: 

\subsubsection{Exponentialverteilung}\label{sec:Expo}

\textbf{Idee/Anwendung}: Wird für Warte- und Ausfallzeiten verwendet und kann als stetige Version der geometrischen Verteilung angesehen werden. Zur Erinnerung: Auch bei der stetigen Gleichverteilung hatten wir angenommen, dass man damit eine Wartezeit messen kann (vgl. Vorlesung Slide 13.41 - Wartezeit auf S-Bahn $\sim U(0,10)$). Der bedeutendste Unterschied bei der Exponentialverteilung ist, dass die weitere Wartezeit unabhängig von der bereits verstrichenen Wartezeit ist (das nennt sich "Gedächtnislosigkeit der Exponentialverteilung"). \\

\noindent Ein Beispiel ist die Lebenszeit einer Glühbirne (also die Wartezeit bis zum Ausfall), bzw. allgemein Wartezeiten bis zum Eintreffen eines Ereignisses. Die Annahme der Gedächtnislosigkeit ist natürlich bei solchen Beispielen etwas kritisch zu betrachten.\\

\noindent \textbf{Verteilung}:
\begin{itemize}
\item[] Notation: $X \sim Expo(\lambda)$
\item[] Dichtefunktion: $f(x)=\begin{cases}
			\lambda \cdot exp(-\lambda x) & \text{für $x \geq 0$}\\
            0 & \text{sonst}
		 \end{cases}$ 
\item[] Verteilungsfunktion: $F(x)=\begin{cases}
			1 - exp(-\lambda x) & \text{für $x \geq 0$}\\
            0 & \text{sonst}
		 \end{cases}$ 
\item[] Erwartungswert und Varianz: $\E[X]= \frac{1}{\lambda}$ und $Var[X]=\frac{1}{\lambda^2}$

\end{itemize}

\noindent \textbf{Bemerkung}: Wenn eine ZV poissonverteilt mit Parameter $\lambda$ ist ($X \sim Po(\lambda)$), dann ist die Wartezeit bis zum ersten Eintreffen exponentialverteilt mit demselben Parameter $\lambda$. Hierbei ist zu beachten, dass man dann die erwartete Wartezeit (also den Erwartungswert der exponentialverteilten ZV), wenn möglich, in der richtigen (Zeit-)Einheit angibt. Ist der fixe Zeitraum der Poissonverteilung beispielsweise eine Stunde (1 h), dann ist die erwartete Wartezeit in Stunden auch $\frac{1}{\lambda} \cdot 1$ h.

\subsubsection{Normalverteilung (aka Gauß'sche Glockenkurve)} \label{sec:Norm}

Gründe für die Verwendung und Wichtigkeit in der Statistik:

\begin{itemize}
    \item Oftmals sind normalverteilte Modelle sehr einfach zu rechnen.
    \item Der Durchschnitt einer Stichprobe mit beliebiger Verteilung folgt einer Normalverteilung, d.h. man kann $n$ Zufallszahlen aus egal welcher Verteilung ziehen, der Mittelwert wird jedoch immer der Normalverteilung folgen (\textbf{zentraler Grenzwertsatz}: Idee klassischen t-Test -- Bildung des Stichprobenmittelwerts der normalverteilt ist).
    \item viele Naturphänomene folgen einer Normalverteilung (Bsp: Körpergröße -- "Durchschnitt" vieler genetischer Faktoren).
\end{itemize}

\noindent \textbf{Idee/Anwendung}: Eine Intuition wie bei den anderen Funktion gibt es hier nicht wirklich. Aber wie oben beschrieben, gibt es viele gute Gründe, die Normalverteilung zu benutzen. Sie ist deshalb auch eine der häufigsten Verteilungen in der Statistik. \\

\noindent \textbf{Verteilung}:
\begin{itemize}
\item[] Notation: $X \sim \mathcal{N}(\mu,\sigma^2)$
\item[] Dichtefunktion: $f(x)=\frac{1}{\sigma\sqrt{2\pi}} \cdot exp \Big(-\frac{(x-\mu)^2}{2 \sigma^2}\Big)$
\item[] Erwartungswert und Varianz: $\E[X]=\mu$ und $Var[X]=\sigma^2$
\item[] Standardisierung: Sei $X \sim \mathcal{N}(\mu,\sigma^2)$, dann gilt: $Z=\frac{X-\mu}{\sigma} \sim \mathcal{N}(0,1)$

\end{itemize}

\noindent \textbf{Bemerkung}: Die Dichtefunktion der Standardnormalverteilung ($\mathcal{N}(0,1)$) wird in der Regel als $\phi(x)$ und die Verteilungsfunktion als $\Phi(x)$ bezeichnet. Die Quantile der Standardnormalverteilung werden in der Regel mit $z$ beschrieben.


\subsubsection{Chi-Quadrat-Verteilung} \label{sec:Chi}

\textbf{Idee/Anwendung}: Ähnlich wie die Normalverteilung haben wir hier keine direkte Intuition, sondern benutzen die Verteilung, weil viele theoretische Konzepte auf ihr beruhen. Die Summe der Quadrate von $n$ standardnormalverteilten ZVs ist $\chi^2$--verteilt mit n Freiheitsgraden (degrees of freedom, df) (vgl. Vorlesung Slide 13.57). 

\noindent \textbf{Bemerkung}: Die Quantile der $\chi^2$--Verteilung werden in der Regel mit $c$ beschrieben.


\subsubsection{t--Verteilung} \label{sec:tdistr}

\textbf{Idee/Anwendung}: Ähnlich wie die Standardnormalverteilung und bei uns hauptsächlich für statistische Tests benutzt. In Bezug auf statistische Tests ist hier anzumerken, dass die t-Verteilung der Standardnormalverteilung sehr ähnlich ist. Sie hat allerdings breitere Enden, was bedeutet, dass kleinere Quantile (kleiner als der Median, der $0$ ist) einen kleineren Wert haben als bei der Standardnormalverteilung (d.h. dass der Betrag ist größer, da die Quantile ja negativ sind). Größere Quantile (größer als $0$) haben analog dazu einen größeren Wert als die der Standardnormalverteilung. Das ist wichtig für den Ablehnbereich in der Testteheorie (vgl. dazu Fig. \ref{Fig:a}). Dazu später mehr (vgl. Kapitel \ref{sec:Gaussvst}). \\

\noindent \textbf{Verteilung}:
\begin{itemize}
\item[] Notation: $X \sim t_{df}$

\end{itemize}

\noindent \textbf{Bemerkung}: Die Quantile der t--Verteilung werden in der Regel mit $t$ beschrieben. Außerdem kann für hohe Freiheitsgrade die t-Verteilung durch die Standardnormalverteilung approximiert werden. Das gilt dann natürlich auch für die entsprechenden Quantile. \\

\noindent \dangersign[3ex] Bitte verwendet immer die korrekten Quantile, wenn diese angegeben sind. \\

\begin{figure}[H] 
\caption{Annäherung der t-Verteilung an die Standardnormalverteilung}\label{Fig:a}
\centering
\includegraphics[scale=0.35]{figures/T-distribution.png}
\footnotesize{Quelle: \url{https://de.wikipedia.org/wiki/Studentsche_t-Verteilung}}
\end{figure}

\noindent Die Grafik verdeutlicht die Annäherung der t-Verteilung mit zunehmenden Freiheitsgraden an die Standardnormalverteilung.

\subsubsection{F--Verteilung}\label{sec:fdistr}

\textbf{Idee/Anwendung}: Auch für die F-Verteilung müssen wir ohne weitere Intuition auskommen. Sie wird wiederum für diverse theoretische Konzepte benutzt. 

\noindent \textbf{Verteilung}:
\begin{itemize}
\item[] Notation: $X \sim F_{df_1,df_2}$

\end{itemize}

\noindent \textbf{Bemerkung}: 

\noindent \dangersign[3ex] Die F-Verteilung hat zwei Freiheitsgrade. Bitte denkt immer an beide (z.B. bei einem Overall-F-test für die Betakoeffizienten eines multiplen Regressionsmodells)!


\subsection{Wichtige Schlüsselbegriffe und "Konzepte" anhand der diskreten Gleichverteilung} \label{sec:concept}

\subsubsection{Parameter von Verteilungen}\label{sec:Param}

Mögliche Ergebnisse werden oft mit den Variablen $x_1,x_2,...,x_n$ bezeichnet. Für das Beispiel des Roulette-Spiels wären $x_1=0,x_2=1,...,x_{37}=36$. Die \textbf{Parameter} im Fall der diskreten Gleichverteilung werden oft als a und b definiert. Beim Roulettespiel gilt: $a = 0$ und $b = 36$, d.h. a und b sind sozusagen die Grenzen des Ergebnisraums.

\subsubsection{Träger einer Verteilung} \label{sec:Träger}
Der \textbf{Träger} der diskreten Gleichverteilung sind alle Ausprägungen $x_1,...,x_n$, also alle natürlichen Zahlen zwischen den (und einschließlich der) Grenzen a und b. Beim Roulette-Beispiel also alle Zahlen von $0,..,36$. 

\subsubsection{Verteilungsfunktion, Wahrscheinlichkeitsfunktion und Dichtefunktion} \label{sec:Functions}
In der Statistik I Vorlesung haben wir schon die empirische Verteilungsfunktion kennengelernt (z.B. als Treppenfunktion oder Polygonzug). Eine \textbf{Verteilungsfunktion} ist grob gesagt einfach nur eine Art Hilfestellung zur Beschreibung von diskreten/stetigen Wahrscheinlichkeitsverteilungen. Sie wird oftmals als Funktion $F$ definiert, die jedem Ergebnis $x_i$ einer Zufallsvariablen $X$ eine Wahrscheinlichkeit $P(X \leq x_i)$ zuordnet. Mathematisch ausgedrückt einfach nur:

\begin{equation*}
F: x \rightarrow P(X \leq x_i)
\end{equation*}

 
\noindent Sowohl die \textbf{Wahrscheinlichkeitsfunktion}\footnote{Die Wahrscheinlichkeitsfunktion wird häufig auch Dichte genannt und mit $f(x)$ notiert} für diskrete ZVs als auch die \textbf{Dichtefunktion} für stetige ZVS dienen als Beschreibung von Wahrscheinlichkeitsverteilungen. Sie beschreiben Wahrscheinlichkeiten für jedes mögliche Ergebnis $x_i$ eines Zufallsexperiments (mathematisch einfach $P(X=x_i)$). Oftmals wird sie aber einfach als $f(x)$ definiert. Liegt eine \textbf{stetige ZV} zugrunde, besitzt die Dichtefunktion folgende Eigenschaften:

%\newpage

\begin{enumerate}
    \item Die Funktion besitzt keinen negativen Wert: also $f(x)\geq 0$ für alle $x_i \in \mathbb{R}$
    \item Die Fläche unter der Funktion (berechnet als ihr Integral) ergibt 1 (analog diskrete ZV: Summe aller Einzelwahrscheinlichkeiten ergibt ebenfalls 1) -- mathematisch: $\int_{-\infty}^{\infty} f(x) dx = 1$
    \begin{itemize}
	\item[$\rightarrow$] Die Integralgrenzen können durch die Grenzen des Definitionsbereichs ersetzt werden.
	\end{itemize}
\end{enumerate}

\noindent Wichtige mathematische Zusammenhänge zwischen der Verteilungsfunktion, der Dichtefunktion und der Quantilsfunktion sind folgende:

\begin{enumerate}
    \item $f(x)= \frac{d}{dx} F(x)$, d.h. die Dichte ist also einfach die Ableitung der Verteilungsfunktion.
    \item $F(x)= \int_{-\infty}^{x} f(t)dt$, d.h. die Verteilungsfunktion ist die Fläche unter der Dichte (also das Integral der Dichte).
    \item $Q(x)= F^{-1}(x)$, d.h. die Quantilsfunktion ist die Umkehrfunktion der Verteilungsfunktion. (Rückblick Statistik I: Quantile sind einfach nur bestimmte Schwellenwerte, d.h. ein bestimmter Anteil der Werte ist kleiner doer gleich dem Quantil, der Rest ist größer -- Beispiel: Median).
    \item $F(x) = Q^{-1}(x)$, d.h. die Verteilungsfunktion ist die Umkehrfunktion der Quantilsfunktion.
\end{enumerate}

\noindent Aber zurück zum Beispiel der diskreten Gleichverteilung unseres Roulette--Spiels:\\

\noindent \textbf{Verteilungsfunktion}
Allgemein ergibt sich für eine diskrete Gleichverteilung auf den ganzen Zahlen 
$\left\{a,a+1,a+2,\ldots, b-1, b\right\}$ für die Verteilungsfunktion:
\begin{equation*}
    F(x)=\begin{cases}
			0, & \text{x < a}\\
            \frac{\floor*{x}-a+1}{b-a+1}, & \text{$x \in [a,b]$}\\
            1, & \text{x > b}
		 \end{cases}
\end{equation*}

\noindent $\floor*{x}$ bedeutet, dass wir x abrunden, d.h. $\floor{3.7} = 3$. Speziell für das Beispiel Roulette: Will man z.B. wissen wie wahrscheinlich eine Zahl kleiner als 3.5 ist berechnet man:

\begin{equation*}
    F(3.5) = \frac{\floor*{3.5}-0+1}{36-0+1} = \frac{4}{37}
\end{equation*}


\noindent \textbf{Wahrscheinlichkeitsfunktion}\\
Die Wahrscheinlichkeitsfunktion sieht folgendermaßen aus:

\begin{equation*}
    f(x)=\begin{cases}
			\frac{1}{37}, & \text{$x \in \{0,1,2,..,36\}$}\\
            0, & \text{sonst}
		 \end{cases}
\end{equation*}


\subsubsection{Erwartungswert und Varianz} \label{sec:Expec}
Der \textbf{Erwartungswert} der diskreten Gleichverteilung ist definiert als $E(X) = \frac{a+b}{2}$ und die Varianz ist definiert als $Var(X) = \frac{(b-a+1)^2-1}{12}$. Vorsicht: Dies sind etwas allgemeinere Formeln als der Spezialfall in der Formelsammlung, im Prinzip stellt es aber das Gleiche dar.




\newpage




\section{Grenzwertsätze und Approximationen von Verteilungen} \label{sec:GWSapprox}

Dieses Kapitel der Vorlesung beschreibt Grundbegriffe über das Verhalten von Folgen von Zufallsvariablen, wenn $n$ gegen unendlich strebt. Die "normale" mathematische Folge sollte schon aus der Vorlesung "Mathematik für Wirtschaftswissenschaftler" bekannt sein. Diese Idee wird nun durch das Hinzunehmen von Zufallsvariablen erweitert. Für eine Auffrischung der Theorie von Folgen und Reihen siehe:\\
\url{https://www.statistik.uni-muenchen.de/formulare/skripte_u_aehnliches/mathehandrechnung_schneider.pdf}

\subsection{Grenzwertsätze}\label{sec:GWS}
\subsubsection{Gesetz der großen Zahlen}\label{sec:LLN}

Das Gesetz der großen Zahlen besagt, dass sich beobachtete relative Häufigkeiten mit zunehmendem $n$ immer näher an die theoretischen Wahrscheinlichkeiten annähern. Genau dann konvergiert die durchschnittliche mittlere Abweichung zwischen den Zufallsvariablen $X_i$ und ihrem Erwartungswert $E(X_i)$ gegen Null. Die Gesetzmäßigkeit lässt sich dadurch erklären, dass der Einfluss von Ausreißern mit zunehmendem Stichprobenumfang des Experiments abnimmt. Man kann sich dies z.B. anhand des einfachen Münzwurfs klarmachen: Wirft man eine Münze 10 mal, wird sich nur in seltenen Fällen genau eine relative Häufigkeit von $1/2$ für Kopf oder Zahl ergeben. Wirft man sie dagegen 1000 mal, wird der Wert sehr nahe an $1/2$ liegen. 


\subsubsection{Zentraler Grenzwertsatz (ZGS)}\label{sec:ZGS}
Der zentrale Grenzwertsatz gehört zu den wichtigsten Aussagen der Wahrscheinlichkeitstheorie. Er gibt eine Charakterisierung der Normalverteilung als Grenzverteilung von Überlagerungen einer Vielzahl unabhängiger zufälliger Einzeleffekte. Der ZGS besagt, dass sich der Mittelwert einer jeden beliebigen Verteilung von iid Zufallsvariablen mit zunehmenden Stichprobenumfang der Normalverteilung annähern wird. Wegen des zentralen Grenzwertsatzes können wir Hypothesentests durchführen, auch wenn die Grundgesamtheit keiner Normalverteilung unterliegt, vorausgesetzt, dass die Stichprobe ausreichend groß ist.\\

\noindent \dangersign[3ex] Vorsicht: Oftmals wird angenommen, dass der zentrale Grenzwertsatz sagt, dass eine Stichprobe ab einer gewissen Größe automatisch normalverteilt sein wird. Dies stimmt aber nicht!


\subsection{Approximationen}\label{sec:Approx}
Wir können unter bestimmten Voraussetzungen Verteilungen durch andere Verteilungen approximieren. Doch warum und wann sind Approximationen sinnvoll?

\begin{itemize}
    \item Bei Rechenintensiven Tasks.
    \item Bei  diskreten Verteilungen: Addition vieler Einzelwahrscheinlichkeiten wird vermieden.
    \item Bei Approximation durch Normalverteilung: Standardisierung möglich.
\end{itemize}

\noindent Insgesamt haben wir vier Approximationen kennengelernt, die nachfolgend nochmal -- überwiegend bezüglich ihrer Intuition -- besprochen werden.

\noindent \dangersign[3ex] Sehr wichtig: Immer die Annahmen für die Approximation prüfen.

\subsubsection{Approximation der Binomial- durch die Normalverteilung}\label{sec:BinN}
\begin{itemize}
\item[] \textbf{Voraussetzung}: $np(1-p) \geq 9$
\item[] \textbf{Approximation}: $B(n,p) \rightarrow \mathcal{N}(np,np(1-p))$
\item[] \textbf{Intuition}: Bei erfüllter Voraussetzung ist die Wahrscheinlichkeitsfunktion der Binomialverteilung (annähernd) symmetrisch und sieht der Dichte der Normalverteilung sehr ähnlich. In diesem Fall ist die Approximation zulässig. Um euch davon zu überzeugen, wie die Wahrscheinlichkeitsfunktion der Binomialverteilung für verschiedene $n$ und $p$ aussieht könnt ihr auf der Seite \url{https://matheguru.com/stochastik/binomialverteilung.html} das Tool "Interaktive Binomialverteilung" nutzen und verschiedene Kombinationen für $n$ und $p$ ausprobieren. Ihr könnt z.B. Werte, die die Voraussetzung nicht erfüllen, mit Werten, die die Voraussetzung erfüllen, vergleichen. Beachtet dabei, dass ihr "PDF" anklickt (und nicht "CDF").
\item[] \textbf{Bemerkung 1}: Auch bei z.B. $n=10$ und $p=0.5$ ist die Wahrscheinlichkeitsfunktion symmetrisch und sieht der Normalverteilung ähnlich. Wir dürfen trotzdem nicht approximieren (vgl. $10\cdot0.5\cdot0.5=2.5<9$). Der Grund dafür, dass wir hier noch nicht approximieren dürfen ist, dass wir eine diskrete Verteilung (Binomialverteilung) durch eine stetige Verteilung (Normalverteilung) approximieren. Die Schritte in der Wahrscheinlichkeitsfunktion sind einfach noch zu groß, um durch eine stetige Verteilung approximiert werden zu dürfen.
\item[] \textbf{Bermerkung 2}: Außerdem gilt nach dieser Approximation, dass die ZV $\hat{p}=\frac{1}{n} \sum X_i$ auch normalverteilt ist: $\hat{p} \sim \mathcal{N}\Big(p,\frac{p(1-p)}{n}\Big)$
\end{itemize}


\subsubsection{Approximation der Binomial- durch die Poissonverteilung}\label{sec:BinPo}
\begin{itemize}
\item[] \textbf{Voraussetzung}: Großes $n$ und kleines $p$.
\item[] \textbf{Approximation}: $B(n,p) \rightarrow Po(np)$
\item[] \textbf{Intuition}: Da wir sowohl die Binomial- als auch die Poissonverteilung durch die Normalverteilung approximieren können (wenn die entsprechenden Voraussetzungen erfüllt sind), ist diese Approximation dann einfach nur logisch (vgl. dazu $B(0,05;200)$ und $Po(10)$). \textit{Aber was ist, wenn die jeweiligen Voraussetzungen nicht erfüllt sind?} In diesem Fall schauen wir uns einfach wieder die Binomialverteilung für ein großes $n$ und ein kleines $p$ in dem Tool "Interaktive Binomialverteilung" an: 
	 Die Binomialverteilung ist rechtsschief/linkssteil (für kleine $p$). Dann ist $np$ auch so klein, dass die Poissonverteilung rechtsschief/linkssteil ist (vgl. Poissonverteilung mit $\lambda=1$, z.B. als Approximation für die Binomialverteilung mit $p=0,01$ und $n=100$).
			
\end{itemize}
\subsubsection{Approximation der Poisson- durch die Normalverteilung}\label{sec:PoN}
\begin{itemize}
\item[] \textbf{Voraussetzung}: $\lambda \geq 10$
\item[] \textbf{Approximation}: $Po(\lambda) \rightarrow \mathcal{N}(\lambda,\lambda)$\footnote{Zur Erinnerung: Für $Po(\lambda)$ gilt $\E[X]=Var[X]=\lambda$, d.h. man übernimmt Erwartungswert und Varianz der Poissonverteilung einfach als Parameter für die Normalverteilung.}
\item[] \textbf{Intuition}: Auch hier schauen wir uns einfach die Wahrscheinlichkeitsfunktion der Poissonverteilung an und sehen, dass diese mit steigendem $\lambda$ \textit{immer symmetrischer} wird. Und wenn letztere annähernd symmetrisch ist (ab $\lambda = 10$), können wir durch die symmetrische Normalverteilung approximieren. Außerdem ist hierbei wichtig, dass die Normalverteilung "weit genug" auf dem positiven Teil der x-Achse liegt und negative Werte nur mit einer sehr, sehr geringen Wahrscheinlichkeit vorkommen, da die Poissonverteilung ja Anzahlen modelliert, die immer nicht-negativ sind. Das ist wiederum erfüllt, wenn die Normalverteilung um $\lambda=\mu \geq 10$ zentriert ist.
\end{itemize}
Zur Veranschaulichung könnt ihr ein ähnliches Tool wie für die Binomialverteilung nutzen. Ihr findet es unter \url{https://matheguru.com/stochastik/poisson-verteilung.html}.

\subsubsection{Approximation der hypergeometrischen durch die Binomialverteilung}\label{sec:HypBin}
\begin{itemize}
\item[] \textbf{Voraussetzungen}: \begin{enumerate}
\item $n \leq 0,1\cdot M$
\item $n \leq 0,1\cdot (N-M)$
\end{enumerate}
\item[] \textbf{Approximation}: $H(n,M,N) \rightarrow B(n, \frac{M}{N})$
\item[] \textbf{Intuition}: Hinter dieser Approximation steckt, dass jedes Ziehen (insgesamt $n$--mal) mit der hypergeometrischen Verteilung ein Bernoulli-Experiment ist, ob das gewünschte Merkmal gezogen wird oder nicht. Beim ersten Ziehen ist noch sehr offensichtlich, dass die Wahrscheinlichkeit, das gewünschte Merkmal zu ziehen, $p=\frac{M}{N}$ ist. Nach jedem Zug reduzieren sich aber $N$ bzw. $N$ und $M$, d.h. $p$ im einfachen Bernoulli-Experiment verändert sich. Bei erfüllten Faustregeln hingegen ist die Anzahl der Züge im Vergleich zur Anzahl der Objekte mit dem gewünschten Merkmal ($M$) bzw. ohne das gewünschte Merkmal ($N-M$) so gering, dass angenommen werden kann, dass sich $p$ nicht ändert. D.h. wir nehmen $p=\frac{M}{N}$ als konstant an. Dann sind die $n$ Züge unabhängige Bernoulli-Experimente und daher ist die Approximation durch die Binomialverteilung zulässig.
\end{itemize}




\newpage




\section{Die Maximum Likelihood Schätzung}\label{sec:ML}

Die ML Schätzung ist ein Prinzip für die Konstruktion von Parameterschätzern bei gegebener Verteilung einer Zufallsvariable.\\

\noindent \textbf{Idee:} Wähle die Schätzwerte für die wahren Parameter der Grundgesamtheit so, dass unter diesen die beobachtete Stichprobe am wahrscheinlichsten sind (dies erklärt auch den Namen der Methode). Die ML-Methode beinhaltet 5 Schritte:

\begin{enumerate}
    \item Liegt eine iid-Stichprobe vor?
    \item Bestimme die Dichte für die einzelnen Beobachtungen.
    \item Stelle die Likelihoodfunktion auf.
    \item Stelle die log-Likelihoodfunktion auf.
    \item Maximiere die log-Likelihood (Ableitung und 0 setzen) und prüfe die Bedingung 2. Ordnung (Maximim oder Minimum?).
\end{enumerate}
% https://www.mathe-online.at/materialien/georg.pernerstorfer/files/Kap1/ml_methode.pdf

\noindent Beispiele der ML-Schätzung für Normalverteilung, Binomialverteilung, Poissonverteilung und Exponentialverteilung:\\
\url{https://mars.wiwi.hu-berlin.de/mediawiki/mmstat3/index.php/Maximum-Likelihood-Methode}




\newpage




\section{Konfidenzintervalle (KI)}\label{sec:KI}

\textbf{Wozu braucht man eigentlich Konfidenzintervalle?} Im Bereich der induktiven Statistik wird mit Hilfe einer Stichprobe versucht, allgemeine Aussagen über die Grundgesamtheit zu machen. Mit Hilfe der ML-Schätzung haben wir dafür schon eine Methode kennengelernt um Punktschätzer zu "generieren". Die erhobenen Daten einer Stichprobe werden dann in einem Punktschätzer zusammengefasst (z.B. Mittelwert oder Varianz), um damit auf die wahren Werte in der Grundgesamtheit zu schließen.\\

\noindent \dangersign[3ex] Probleme:

\begin{itemize}
    \item Wie präzise ist diese Punktschätzung eigentlich?
    \item In welchem Bereich liegt der wahre Mittelwert der Grundgesamtheit höchstwahrscheinlich?
    \item Ist es möglich, dass in Wirklichkeit im Mittel doch ein anderer Wert herauskommt, aber wir in dieser Stichprobe einfach nur Pech hatten?
\end{itemize}

\noindent $\Rightarrow$ Diese Fragen kann ein Punktschätzer nicht beantworten – aber ein Intervallschätzer kann das!\\

\noindent \textbf{Doch was ist ein Konfidenzintervall genau?} Die folgende Unterscheidung ist sehr wichtig für das Verständnis von Konfidenzintervallen:

\begin{itemize}
    \item Mit einer Stichprobe schätzen wir einen Parameter, z.B $\hat{\mu}$.
    \item Der wahre Parameter $\mu$ in der Grundgesamtheit ist dann zwar in der Nähe von $\hat{\mu}$, aber quasi nie genau gleich.
\end{itemize}

\noindent Den wahren Parameter $\mu$ werden wir also nie exakt bestimmen können, dennoch können wir versuchen einen Bereich zu bestimmen in dem er ziemlich sicher liegt -- und genau das ist die Idee von Konfidenzintervallen.\\

\noindent \textbf{Definition KI (ohne Formel)}:\\
\noindent Ein 95\%-KI ist ein Intervall [a,b], das, wenn es sehr häufig mit neuen Stichproben berechnet wird, den wahren Parameter, z.B. ($\mu$), mit einer Wahrscheinlichkeit von 95\% auch überdeckt.

\noindent $\Rightarrow$ ein einzelnes 95\%-KI ist mit 95\%-iger Wahrscheinlichkeit eines von denen, das den wahren Parameter ($\mu$) beinhaltet.\\

\noindent \textbf{Und wie schätzt man ein Konfidenzintervall?} Das zentrale Prinzip für alle Konfidenzintervalle:

\begin{enumerate}
    \item Berechne einen Punktschätzer für einen Parameter, z.B. für den Anteilswert $p$ einer Bernoulli oder Binomialverteilung.
    \item Um diese Punktschätzer bildet man dann ein (meistens) symmetrisches Intervall, das abhängig von der Varianz in der Stichprobe und des gewünschten Konfidenzniveaus $1-\alpha$ enger oder breiter wird.
\end{enumerate}




\newpage



\section{Testtheorie}\label{sec:Tests}

\subsection{Der p-Wert}\label{sec:pWert}

Der p-Wert gibt die Wahrscheinlichkeit an, dass die Teststatistik, unter der Annahme, dass $H_0$ wahr ist, den beobachteten/realisierten Wert $t$ oder einen noch extremeren Wert annimmt. Noch extremer heißt in diesem Fall, dass man sich davon wegbewegt, $H_0$ anzunehmen. Um das zu verdeutlichen, müssen wir uns erstmal vor Augen halten, wo genau diese Wahrscheinlichkeit herkommt: \\

\noindent Wir wissen, dass die Teststatistik $T(\textbf{X})$ unter $H_0$ eine bestimmte Verteilung hat (Merke: $T(\textbf{X})$ ist eine ZV). Von dieser Verteilung kommen ja dann auch die Quantile, die wir als kritische Werte heranziehen, um die Testentscheidung mittels der Teststatistik zu treffen. Was genau das mit dem p-Wert zu tun hat, gehen wir jetzt am Beispiel eines einseitigen und eines zweiseitigen t-Tests durch.

\subsubsection{Der p-Wert beim einseitigen t-Test}\label{sec:pWert1}

Der einseitige t-Test wird genutzt, um zu testen, ob der Erwartungswert einer normalverteilten ZV größer (kleiner) als ein unter $H_0$ angenommener Wert $\mu_0$ ist, wenn die Varianz $\sigma^2$ unbekannt ist und geschätzt werden muss.  Man kann die Hypothese (je nachdem, was man zeigen möchte) für beide Richtungen aufstellen. Wir zeigen es an dem Beispiel, dass wir testen wollen, ob der Erwartungswert signifikant kleiner als ein $\mu_0$ ist. D.h. die Hypothesen lauten: \\
\begin{center} $H_0$: $\mu \geq \mu_0$ \hspace{2cm }gegen \hspace{2cm } $H_1$: $\mu < \mu_0$ \end{center}
Die Teststatistik und ihre Verteilung sind dann unter der $H_0$--Hypothese:\\
\begin{center} $T(\textbf{X})=\sqrt{n}\cdot\frac{\overline{X}-\mu_0}{S_x} \simtext{$H_0$} t_{n-1}$ \end{center}

\noindent Mit den vorliegenden Daten berechnet man nun eine Realisation $t$ der Zufallsvariable T(\textbf{X}). Dann wird $H_0$ abgelehnt, wenn $t \in (-\infty, -t_{n-1,1-\alpha})$. Je kleiner die Realisation $t$ von T(\textbf{X}) also ist, desto eher lehnen wir $H_0$ ab bzw. desto weiter entfernen wir uns davon, $H_0$ anzunehmen. Die Wahrscheinlichkeit, dass man dann eine genauso kleine oder noch kleinere Teststatistik (im Vergleich zur Realisation $t$) erhält, wenn man die unter $H_0$ angenommene Verteilung ($t_{n-1}$) zugrunde legt, ist dann der p-Wert, also:

\begin{center} $p-Wert=P(T \leq t)$, \end{center} 
wobei T ($\sim t_{n-1}$) wieder die ZV ist und $t$ die Realisation. Ist diese Wahrscheinlichkeit kleiner als das Signifikanzniveau $\alpha$, dann ist es sehr unwahrscheinlich, dass die Realisation $t$ von der ZV $T(\textbf{X})$ "unterboten" wird, wenn denn die Verteilungsannahme aus $H_0$ stimmt. Aber wenn es eben so unwahrscheinlich ist, dass $t$ von der ZV $T(\textbf{X})$ unterboten wird, muss $t$ selbst (unter der Verteilung in $H_0$) ein Ausreißer, also ein seltenes Ereignis sein. D.h. wir hätten mit unseren Daten zufällig ein seltenes Ereignis getroffen. Wir wollen aber nicht glauben, dass wir ausgerechnet ein seltenes Ereignis gefunden haben und verwerfen deshalb die unter $H_0$ angenommene Verteilung und somit die $H_0$--Hypothese. \\

\textbf{Merke}: $p-Wert=P(T \leq t) < \alpha$ $\Rightarrow$ $H_0$ verwerfen! \\[1.5cm]

\noindent \dangersign[3ex] Für die Hypothesen: \\
\begin{center} $H_0$: $\mu \leq \mu_0$ \hspace{2cm }gegen \hspace{2cm } $H_1$: $\mu > \mu_0$ \end{center}
ist der p-Wert entsprechend:
\begin{center} $p-Wert=P(T \geq t)=1-P(T < t)$, \end{center} 
da wir ja dann den Ablehnbereich $(t_{n-1,1-\alpha}, + \infty)$ haben.

\subsubsection{Der p-Wert beim zweiseitigen t-Test}\label{sec:pWert2}

Hier werden folgende Hypothesen gegeneinander getestet:
\begin{center} $H_0$: $\mu = \mu_0$ \hspace{2cm }gegen \hspace{2cm } $H_1$: $\mu \neq \mu_0$ \end{center}
Die Teststatistik ist wiederum:
\begin{center} $T(\textbf{X})=\sqrt{n}\cdot\frac{\overline{X}-\mu_0}{S_x} \simtext{$H_0$} t_{n-1}$ \end{center}
und der Ablehnbereich ist: $(-\infty, -t_{n-1,1-\frac{\alpha}{2}}) \cup (t_{n-1,1-\frac{\alpha}{2}}, +\infty)$.

\noindent D.h. dann, dass wir uns sowohl für Teststatistiken, die größer als $t_{n-1,1-\frac{\alpha}{2}}$ sind, immer mehr vom Annahmebereich entfernen als auch für Teststatistiken, die kleiner als $-t_{n-1,1-\frac{\alpha}{2}}$ sind. Für den p-Wert bedeutet das, dass wir beide Richtungen berücksichtigen müssen (was wir mit Hilfe des Betrags der Teststatistik machen, wobei wir die Symmetrie der t-Verteilung ausnutzen):
\begin{align*} 
p-Wert &= P(|T| \geq |t|)  \\
	&=	P(T \leq -|t|) + P(T \geq |t|)			\\
	&=	2 \cdot P(T \geq |t|)			\\
	&=	2 \cdot (1-P(T < |t|))			\\
	&=	2 \cdot (1-F(|t|)),		
\end{align*} 

\noindent wobei $F(\cdot)$ die Verteilungsfunktion der entsprechenden Verteilung ist -- hier also der t-Verteilung mit $n-1$ Freiheitsgraden.
\subsection{Hypothesentests}\label{sec:HypTests}

In der Vorlesung haben wir eine Reihe von Hypothesentests kennengelernt. Auf die wichtigsten wird hier noch einmal intuitiv eingegangen. Bevor wir auf die verschiedenen Tests eingehen eine kleine Bemerkung zu einfachen bzw. doppelten und einseitigen bzw. zweiseitigen Tests:
Ein einfacher Test (im Vergleich zu einem doppelten Test) betrachtet eine einzige Stichprobe. Der einfache Test kann aber sowohl einseitig ($H_0:$ $\mu \geq \mu_0$ gg. $H_1:$ $\mu < \mu_0$ bzw. umgekehrt) als auch zweiseitig ($H_0:$ $\mu = \mu_0$ gg. $H_1:$ $\mu \neq \mu_0$) formuliert werden. Ein doppelter Test hingegen betrachtet zwei Stichproben -- auch dieser kann sowohl ein- als auch zweiseitig formuliert werden.

\subsubsection{Einfacher Gauss-Test}\label{sec:Gausstest1}
Test des Erwartungswerts $\mu_x$ einer (per Annahme) normalverteilten ZV $X$ ($\sim \mathcal{N}(\mu_x, \sigma_x^2)$). Die \textbf{Varianz} $\sigma_x^2$ ist dabei \textbf{bekannt}. 

\subsubsection{Einfacher t-Test}\label{sec:tTest1}
Test des Erwartungswerts $\mu_x$ einer (per Annahme) normalverteilten ZV $X$ ($\sim \mathcal{N}(\mu_x, \sigma_x^2)$). Die \textbf{Varianz} $\sigma_x^2$ ist dabei \textbf{unbekannt} und muss aus der Stichprobe mittels der der Stichprobenvarianz $S^2$ geschätzt werden. 

\subsubsection{Approximativer Einfacher Binomialtest}\label{sec:bintest1}
Test der Erfolgswahrscheinlichkeit $p$ einer (per Annahme) bernoulliverteilten ZV $X$ ($\sim B(1, p)$). Die Teststatistik ist approximativ standardnormalverteilt. 

\subsubsection{Chi-Quadrat-Anpassungstest}\label{sec:chitest1}
Test der unter $H_0$ erwarteten und tatsächlich Beobachteten absoluten Häufigkeiten von verschiedenen Ausprägungen einer ZV. Dadurch kann man testen, ob die Verteilung, von der die Stichprobe stammt, ungleich einer unter $H_0$ angenommenen Verteilung ist.

\subsubsection{F-Test}\label{sec:ftest}
Testet das Verhältnis (Größer/kleiner bzw. Gleichheit gegen Ungleichheit) der Varianzen zweier unabhängiger normalverteiler ZV. Die Teststatistik ist der Quotient der Stichprobenvarianzen und F-verteilt.

\noindent \dangersign[3ex] Die F--Verteilung hat \textbf{zwei Freiheitsgrade}. Denkt immer an beide (z.B. auch bei der F-Statistik eines multiplen Regressionsmodells).

\subsubsection{Doppelter Gauss-Test}\label{sec:Gausstest2}
Testet das Verhältnis (Größer/Kleiner bzw. Gleichheit gegen Ungleichheit) der Erwartungswerte zweier unabhängiger normalverteiler ZVs (X und Y). Die \textbf{Varianzen} $\sigma_x^2$ und $\sigma_y^2$ sind dabei \textbf{bekannt}. 

\subsubsection{Doppelter t-Test}\label{sec:tTest2}
Testet das Verhältnis (Größer/Kleiner bzw. Gleichheit gegen Ungleichheit) der Erwartungswerte zweier unabhängiger normalverteiler ZVs (X und Y). Die \textbf{Varianzen} $\sigma_x^2$ und $\sigma_y^2$ sind dabei \textbf{unbekannt} und müssen aus der Stichprobe mittels der der Stichprobenvarianz $S^2$ geschätzt werden. Hier wird zusätzlich angenommen, dass die zugrundeliegenden Verteilungen \textbf{beider ZVs dieselbe Varianz} haben ($\sigma_x^2 =\sigma_y^2$). 

\subsubsection{Welch-Test}\label{sec:Welch}
Testet das Verhältnis (Größer/Kleiner bzw. Gleichheit gegen Ungleichheit) der Erwartungswerte zweier unabhängiger normalverteiler ZV (X und Y). Die \textbf{Varianzen} $\sigma_x^2$ und $\sigma_y^2$ sind dabei \textbf{unbekannt} und müssen aus der Stichprobe mittels der der Stichprobenvarianzen $S_x^2$ und $S_y^2$ geschätzt werden. Hier wird zusätzlich angenommen, dass die zugrundeliegenden Verteilungen \textbf{beider ZVs ungleiche Varianzen} haben ($\sigma_x^2 \neq \sigma_y^2$). 

\subsubsection{Paired t-Test}\label{sec:paired}
Testet das Verhältnis (Größer/Kleiner bzw. Gleichheit gegen Ungleichheit) der Erwartungswerte zweier \textbf{abhängiger} normalverteiler ZVs (X und Y). Diese Abhängigkeit kann z.B. dadurch entstehen, dass man an denselben Untersuchungseinheiten dasselbe Merkmal zu zwei verschiedenen Zeitpunkten misst. 

\subsubsection{Approximativer Doppelter Binomialtest}\label{sec:bintest2}
Hier werden statt Mittelwerte Wahrscheinlichkeiten miteinander verglichen. Es handelt sich dabei aber immer noch um Erwartungswerte, da $p$ ja genau der Erwartungswert einer ZV mit der Bernoulliverteilung $B(1,p)$ ist.

\subsubsection{Mann-Whitney-U-Test für zwei unabhängige Stichproben}\label{sec:MWUtest}
Vergleich der Lageparameter (das ist bei der Normalverteilung der Erwartungswert $\mu$) zweier stetig verteilter ZVs, die ansonsten der gleichen Verteilung folgen. Hier wird im Vergleich zu den meisten bisherigen Tests die Annahme, dass die ZVs normalverteilt sein müssen, gelockert.

\subsubsection{Kolmogorov-Smirnov-Anpassungstest}\label{sec:KolSmi}
Test von Gleichheit gegen Ungleichheit der Verteilung zweier ZVs (X und Y). Wir berechnen dafür die empirischen Verteilungsfunktionen von X und Y ($\hat{F}$ and $\hat{G}$). Dann schauen wir uns jede einzelne Beobachtung von X und Y an (bzw. wir fassen sie einfach zu einem Pool von Beobachtungen zusammen). Aus diesem Pool nehmen wir jeden Wert ($t$) einmal und betrachten $|\hat{F}(t)-\hat{G}(t)|$. Die größte dieser Differenzen ist die Teststatistik. Dann wird wie üblich gegen einen kritischen Wert eine Entscheidung getroffen. Das ist aber an dieser Stelle zu komplex $\rightarrow$ wir benutzen Computer dafür.

\subsubsection{Chi-Quadrat-Unabhängigkeitstest}\label{sec:chitest2}
Wie beim $\chi^2$--Anpassungstest werden hier beobachtete absolute Häufigkeiten mit unter $H_0$ erwarteten absoluten Häufigkeiten verglichen. Es gibt jedoch zwei Unterschiede. Erstens werden die beobachteten Häufigkeiten aus zwei Stichproben (für zwei ZVs X und Y) mit den unter $H_0$ erwarteten Häufigkeiten verglichen. Zweitens sind die unter $H_0$ erwarteten Häufigkeiten nicht die einer beliebigen Verteilung, sondern genau die, die man unter Unabhängigkeit beider ZVs erwartet. %Die Teststatistik ist übrigens Pearson's $\chi^2$, das wir schon als deskriptives Maß für die (Un--)Abhängigkeit zweier ZVs kennengelernt haben. Hier wird also sehr anschaulich der Übergang von deskriptiver zu induktiver Statistik verdeutlicht.

\subsubsection{Odds-Ratio-Test}\label{sec:ORtest}
Testet ebenfalls die Unabhängigkeit zweier Variablen mittels der Odds-Ratio, kann aber dementsprechend nur für Daten, die sich in einer Vier-Felder-Tafel darstellen lassen, angewendet werden.


\subsection{Unterschied zwischen Gauss-Tests und t-Tests}\label{sec:Gaussvst}
Der Unterschied zwischen Gauss-Tests und t-Tests besteht darin, dass bei Gauss-Tests die Varianz der Verteilung, die der Stichprobe zugrunde liegt, bekannt ist. Bei t-Tests hingegen nicht. Dass führt dazu, dass die Teststatistik bei Gauss-Tests standardnormalverteilt ist. Bei t-Tests hingegen ist sie t-verteilt. \\

\noindent \textbf{Was genau hat das jetzt mit der Testentscheidung zu tun?} Dazu schauen wir uns erst einmal Fig. \ref{Fig:a} an. Wir sehen, dass (vor allem für wenig Freiheitsgrade) die t-Verteilung breitere Enden hat als die Standardnormalverteilung, was wiederum bedeutet, dass alle $\alpha$--Quantile für $\alpha < 0,5$ bei der t-Verteilung kleiner (also weiter im negativen Bereich) sind als die der Standardnormalverteilung. Alle $\alpha$--Quantile für $\alpha > 0,5$ sind bei der t-Verteilung hingegen größer (also weiter im positiven Bereich) als bei der Standardnormalverteilung\footnote{Der Median, also $\tilde{x}_{0,5}$, ist bei beiden Verteilungen der Wert 0, da sie um 0 zentriert sind}. Das gilt dann insbesondere auch für die Quantile, die wir als kritische Werte für die Testentscheidung heranziehen. Somit verkleinert sich der Ablehnbereich bei t-Tests im Vergleich zu Gauss-Tests. \\

\noindent Dafür gibt es auch eine Intuitive Begründung: Bei t-Tests müssen wir nicht nur den interessierenden statistischen Parameter (meistens $\mu$) schätzen, sondern auch $\sigma^2$ mittels der Stichprobenvarianz $S^2$. Da wir mehr Parameter schätzen, wollen wir $H_0$ nicht so bereitwillig ablehnen wie bei Gauss-Tests. Dementsprechend ist der Ablehnbereich bei t-Tests kleiner. \\

\noindent Aber auch dafür gibt es Abhilfe: Je größer der Stichprobenumfang $n$, desto größer die Anzahl der Freiheitsgrade und desto ähnlicher sind sich Standardnormal- und t-Verteilung. D.h. dann, dass wir für einen großen Stichprobenumfang (annähernd) dieselben kritischen Werte für Gauss- und t-Tests benutzen. Die zusätzliche Ungewissheit durch die Schätzung der Varianz (im Vergleich zu bekannter Varianz) wird also durch mehr Beobachtungen gemildert. Das ist natürlich ein intuitives Ergebnis: Je mehr Beobachtungen wir haben, desto "sicherer" sind wir uns mit unserer Schlussfolgerung.

\subsubsection{Unterschied zwischen doppeltem t-Test und Welch-Test}\label{sec:t2vsWelch}
Eine ähnliche Intuition wie für den Unterschied zwischen Gauss-Tests und t-Tests gibt es auch für den doppelten t-Test und den Welch-Test. Die Anzahl der Freiheitsgrade der Teststatistik vom doppelten t-Test ist größer als die beim Welch-Test. D.h. wir haben beim doppelten t-Test einen größeren Ablehnbereich als beim Welch-Test und wir sind uns somit "etwas eher mit der Entscheidung sicher", $H_0$ zu verwerfen, falls es denn dazu kommt. Das liegt daran, dass man beim doppelten t-Test zwei Stichproben hat, mit denen man eine unbekannte Varianz schätzt. Beim Welch-Test hingegen hat man zwei Stichproben, mit Hilfe derer man zwei unbekannte Varianzen schätzen muss. Deshalb ist beim letzteren etwas mehr Ungewissheit im Spiel.

\newpage
\section{Das Multiple Regressionsmodell}\label{sec:Reg}

...folgt bald...

\noindent Eine gute Übersicht zu den Annahmen und den diversen Plots (Residualplot, QQ-Plot, etc.) findet ihr unter folgendem Link:\\
\url{https://wikis.fu-berlin.de/display/fustat/Residuenplots}




\newpage




\section*{References}

\begin{itemize}
    \item[*] Vorlesungsunterlagen "Statistik 1 für Wirtschaftswissenschaflter" (WiSe 2019/20, Prof. Heumann) an der LMU München.
    \item[*] Vorlesungsunterlagen "Statistik 2 für Wirtschaftswissenschaflter" (SoSe 2020, Prof. Heumann) an der LMU München.
    \item[*] Toutenburg, H. and Heumann, C., 2008. Induktive Statistik: eine Einführung mit R und SPSS. Springer-Verlag.
    \item[*] Dr. Alexander Engelhardt: \url{https://www.crashkurs-statistik.de}
    \item[*] \url{https://www.statistik.uni-muenchen.de/formulare/skripte_u_aehnliches/mathehandrechnung_schneider.pdf}
    \item[*] \url{https://www.mathebibel.de}
    \item[*] \url{https://de.wikibooks.org/wiki/Mathe_f\%C3\%BCr_Nicht-Freaks}
    \item[*] \url{https://www.statistik-nachhilfe.de}
    \item[*] \url{https://matheguru.com}
\end{itemize}




\end{document}